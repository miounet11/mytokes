"""AI History Manager API æœåŠ¡

æä¾› OpenAI å…¼å®¹çš„ API æ¥å£ï¼Œé›†æˆå†å²æ¶ˆæ¯ç®¡ç†åŠŸèƒ½ã€‚
å¯æ¥å…¥ NewAPI ä½œä¸ºè‡ªå®šä¹‰æ¸ é“ä½¿ç”¨ã€‚

å¯åŠ¨æ–¹å¼ (æ¨èå¤š worker):
    uvicorn api_server:app --host 0.0.0.0 --port 8100 --workers 4 --loop uvloop --http httptools

NewAPI é…ç½®:
    - ç±»å‹: è‡ªå®šä¹‰æ¸ é“
    - Base URL: http://your-server:8100
    - æ¨¡å‹: æŒ‰éœ€é…ç½®
"""

import json
import time
import uuid
import asyncio
import logging
import os
import re
from typing import Optional, AsyncIterator, Union
from contextlib import asynccontextmanager
from functools import lru_cache

import httpx
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel

from ai_history_manager import HistoryManager, HistoryConfig, TruncateStrategy
from ai_history_manager.utils import is_content_length_error
from hallucination_detection import detect_hallucinated_tool_result

# ==================== é…ç½® ====================

# Kiro ä»£ç†åœ°å€ (tokens ç½‘å…³, ä½¿ç”¨å†…ç½‘åœ°å€)
KIRO_PROXY_BASE = "http://127.0.0.1:8000"
# OpenAI å…¼å®¹ç«¯ç‚¹ (Kiro æ¸ é“)
KIRO_PROXY_URL = f"{KIRO_PROXY_BASE}/kiro/v1/chat/completions"
KIRO_MODELS_URL = f"{KIRO_PROXY_BASE}/kiro/v1/models"
KIRO_API_KEY = "dba22273-65d3-4dc1-8ce9-182f680b2bf5"

# ==================== æ™ºèƒ½æ¥ç»­é…ç½® ====================

# æ¥ç»­æœºåˆ¶é…ç½® - å¤„ç†ä¸Šæ¸¸æˆªæ–­å“åº”
CONTINUATION_CONFIG = {
    # å¯ç”¨æ¥ç»­æœºåˆ¶
    "enabled": os.getenv("CONTINUATION_ENABLED", "true").lower() in ("1", "true", "yes"),

    # æœ€å¤§ç»­ä¼ æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
    # ä¼˜åŒ–ï¼šä» 15 é™ä½åˆ° 5ï¼Œé…åˆç©ºå“åº”éªŒè¯å¯ä»¥æ›´å¿«å¤±è´¥
    # å¦‚æœéœ€è¦å¤„ç†è¶…é•¿è¾“å‡ºï¼Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´
    "max_continuations": int(os.getenv("MAX_CONTINUATIONS", "5")),

    # è§¦å‘ç»­ä¼ çš„æ¡ä»¶
    "triggers": {
        # æµä¸­æ–­ï¼ˆEOF/è¿æ¥æ–­å¼€ï¼‰
        "stream_interrupted": True,
        # max_tokens è¾¾åˆ°ä¸Šé™
        "max_tokens_reached": True,
        # å·¥å…·è°ƒç”¨ JSON ä¸å®Œæ•´
        "incomplete_tool_json": True,
        # è§£æé”™è¯¯
        "parse_error": True,
    },

    # ç»­ä¼ æç¤ºè¯æ¨¡æ¿
    "continuation_prompt": """Your previous response was truncated. Please continue EXACTLY from where you stopped.

IMPORTANT:
- Do NOT repeat any content you already generated
- Do NOT add any preamble or explanation
- Continue the JSON/tool call from the exact character where it was cut off
- If you were in the middle of a tool call, complete it properly

Your truncated response ended with:
```
{truncated_ending}
```

Continue from here:""",

    # æˆªæ–­ç»“å°¾ä¿ç•™å­—ç¬¦æ•°ï¼ˆç”¨äºç»­ä¼ æç¤ºï¼‰
    "truncated_ending_chars": 500,

    # ç»­ä¼ è¯·æ±‚çš„ max_tokensï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´å®Œæˆï¼‰
    "continuation_max_tokens": int(os.getenv("CONTINUATION_MAX_TOKENS", "8192")),

    # æ—¥å¿—çº§åˆ«
    "log_continuations": True,
}

# ==================== ä¸Šä¸‹æ–‡å¢å¼ºé…ç½® ====================

# ä¸Šä¸‹æ–‡å¢å¼ºæœºåˆ¶ - åœ¨ç”¨æˆ·æ–°è¾“å…¥æ—¶æ³¨å…¥é¡¹ç›®èƒŒæ™¯ä¿¡æ¯
CONTEXT_ENHANCEMENT_CONFIG = {
    # å¯ç”¨ä¸Šä¸‹æ–‡å¢å¼º
    "enabled": os.getenv("CONTEXT_ENHANCEMENT_ENABLED", "true").lower() in ("1", "true", "yes"),

    # æå–æ¨¡å‹ï¼ˆä½¿ç”¨ Sonnet å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼‰
    "model": os.getenv("CONTEXT_ENHANCEMENT_MODEL", "claude-sonnet-4-5-20250929"),

    # ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
    "max_tokens": int(os.getenv("CONTEXT_ENHANCEMENT_MAX_TOKENS", "200")),
    "min_tokens": int(os.getenv("CONTEXT_ENHANCEMENT_MIN_TOKENS", "100")),

    # æ›´æ–°ç­–ç•¥ï¼šæ¯ N æ¡ç”¨æˆ·æ¶ˆæ¯æ›´æ–°ä¸€æ¬¡
    "update_interval": int(os.getenv("CONTEXT_ENHANCEMENT_UPDATE_INTERVAL", "10")),

    # æ˜¯å¦ä¸æ™ºèƒ½æ‘˜è¦é›†æˆï¼ˆæ¨èå…³é—­ä»¥æå‡é¦–å­—å“åº”é€Ÿåº¦ï¼‰
    # å¼€å¯æ—¶ï¼šæ¯æ¬¡æ‘˜è¦åä¼šé¢å¤–è°ƒç”¨ AI æå–é¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œå¢åŠ  3-5s å»¶è¿Ÿ
    "integrate_with_summary": os.getenv("CONTEXT_ENHANCEMENT_INTEGRATE_SUMMARY", "false").lower() in ("1", "true", "yes"),

    # ä¸Šä¸‹æ–‡æå–æç¤ºè¯æ¨¡æ¿
    "extraction_prompt": """è¯·åˆ†æä»¥ä¸‹å¯¹è¯å†å²ï¼Œæå–é¡¹ç›®çš„æ ¸å¿ƒä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆ100-200 tokensï¼‰ï¼š

**å¿…é¡»åŒ…å«**ï¼š
1. ç¼–ç¨‹è¯­è¨€å’Œä¸»è¦æ¡†æ¶
2. æ ¸å¿ƒåŠŸèƒ½å’Œä¸šåŠ¡é¢†åŸŸ
3. é‡è¦çš„æŠ€æœ¯çº¦æŸæˆ–æ¶æ„å†³ç­–
4. å½“å‰æ­£åœ¨å¤„ç†çš„ä¸»è¦ä»»åŠ¡

**æ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨ç®€æ´çš„çŸ­è¯­ï¼Œä¸è¦å®Œæ•´å¥å­
- ç”¨ | åˆ†éš”ä¸åŒä¿¡æ¯ç‚¹
- æ€»é•¿åº¦æ§åˆ¶åœ¨ 100-200 tokens

**ç¤ºä¾‹è¾“å‡º**ï¼š
Python + FastAPI | AI API ä»£ç†æœåŠ¡ | Anthropic/OpenAI æ ¼å¼è½¬æ¢ | å†å²æ¶ˆæ¯ç®¡ç†ä¸æ™ºèƒ½æ‘˜è¦ | æ¨¡å‹è·¯ç”±(Opus/Sonnet) | å½“å‰ä»»åŠ¡ï¼šæ·»åŠ ä¸Šä¸‹æ–‡å¢å¼ºåŠŸèƒ½

å¯¹è¯å†å²ï¼š
{conversation_history}

è¯·ç›´æ¥è¾“å‡ºé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€æˆ–è§£é‡Šï¼š""",

    # å¢å¼ºæ¶ˆæ¯æ¨¡æ¿
    "enhancement_template": """<project_context>
{context}
</project_context>

<user_request>
{user_input}
</user_request>""",
}

# å†å²æ¶ˆæ¯ç®¡ç†é…ç½®
# ä¼˜åŒ–é…ç½®ï¼šå¹³è¡¡ä¸Šä¸‹æ–‡ä¿ç•™å’Œç¨³å®šæ€§
HISTORY_CONFIG = HistoryConfig(
    strategies=[
        TruncateStrategy.AUTO_TRUNCATE,     # è‡ªåŠ¨æˆªæ–­ - å‘é€å‰ä¼˜å…ˆä¿ç•™æœ€æ–°ä¸Šä¸‹æ–‡
        TruncateStrategy.SMART_SUMMARY,     # æ™ºèƒ½æ‘˜è¦ - ç”¨ AI ç”Ÿæˆæ—©æœŸå¯¹è¯æ‘˜è¦
        TruncateStrategy.ERROR_RETRY,       # é”™è¯¯é‡è¯• - é‡åˆ°é•¿åº¦é”™è¯¯æ—¶æˆªæ–­åé‡è¯•ï¼ˆæ¨èï¼‰
    ],
    max_messages=30,           # æœ€å¤§æ¶ˆæ¯æ•°
    max_chars=150000,          # æœ€å¤§å­—ç¬¦æ•°
    summary_keep_recent=10,    # ä¿ç•™æœ€è¿‘ 10 æ¡æ¶ˆæ¯å®Œæ•´
    summary_threshold=100000,  # è§¦å‘æ‘˜è¦é˜ˆå€¼ï¼ˆå­—ç¬¦ï¼‰
    retry_max_messages=20,     # é‡è¯•æ—¶ä¿ç•™æ¶ˆæ¯æ•°
    max_retries=2,             # æœ€å¤§é‡è¯•æ¬¡æ•°
    estimate_threshold=150000, # é¢„ä¼°æˆªæ–­é˜ˆå€¼
    summary_cache_enabled=True,
    add_warning_header=True,
)

# ==================== å¼‚æ­¥æ‘˜è¦ä¼˜åŒ–é…ç½® ====================
# æ ¸å¿ƒæ€æƒ³ï¼šé¦–æ¬¡è¯·æ±‚ç”¨ç®€å•æˆªæ–­å¿«é€Ÿå“åº”ï¼Œåå°å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ä¾›åç»­ä½¿ç”¨
ASYNC_SUMMARY_CONFIG = {
    # å¯ç”¨å¼‚æ­¥æ‘˜è¦æ¨¡å¼
    "enabled": os.getenv("ASYNC_SUMMARY_ENABLED", "true").lower() in ("1", "true", "yes"),

    # é¦–æ¬¡è¯·æ±‚ç­–ç•¥ï¼šå½“æ²¡æœ‰ç¼“å­˜æ—¶ï¼Œä½¿ç”¨ç®€å•æˆªæ–­å¿«é€Ÿå“åº”
    # è€ŒéåŒæ­¥ç­‰å¾…æ‘˜è¦ç”Ÿæˆ
    "fast_first_request": os.getenv("ASYNC_SUMMARY_FAST_FIRST", "true").lower() in ("1", "true", "yes"),

    # åå°ä»»åŠ¡é˜Ÿåˆ—å¤§å°ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
    "max_pending_tasks": int(os.getenv("ASYNC_SUMMARY_MAX_TASKS", "100")),

    # æ‘˜è¦æ›´æ–°é—´éš”ï¼šæ¯ N æ¡æ–°ç”¨æˆ·æ¶ˆæ¯åè§¦å‘åå°æ›´æ–°
    "update_interval_messages": int(os.getenv("ASYNC_SUMMARY_UPDATE_INTERVAL", "5")),

    # åå°ä»»åŠ¡è¶…æ—¶ï¼ˆç§’ï¼‰
    "task_timeout": int(os.getenv("ASYNC_SUMMARY_TASK_TIMEOUT", "30")),

    # ==================== ç¼“å­˜è®¡è´¹æ¨¡æ‹Ÿ ====================
    # å½“ç¼“å­˜å‘½ä¸­æ—¶ï¼Œæ¨¡æ‹Ÿ Anthropic prompt caching çš„è®¡è´¹æŠ˜æ‰£
    # è¿™æ · NewAPI ä¼šæ˜¾ç¤ºç±»ä¼¼ "æ¨¡å‹: 2.5 * ç¼“å­˜: 0.1 * ä¸“å±å€ç‡: 1"
    "simulate_cache_billing": os.getenv("SIMULATE_CACHE_BILLING", "true").lower() in ("1", "true", "yes"),

    # ç¼“å­˜è¯»å–æŠ˜æ‰£æ¯”ä¾‹ï¼ˆAnthropic å®˜æ–¹æ˜¯ 0.1ï¼Œå³ 10% ä»·æ ¼ï¼‰
    # æˆ‘ä»¬çš„æ‘˜è¦ç¼“å­˜è™½ç„¶ä¸æ˜¯çœŸæ­£çš„ prompt cachingï¼Œä½†ç¡®å®èŠ‚çœäº†é‡å¤è®¡ç®—
    "cache_read_discount": float(os.getenv("CACHE_READ_DISCOUNT", "0.9")),
}

# æœåŠ¡é…ç½®
SERVICE_PORT = int(os.getenv("SERVICE_PORT", "8100"))
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "300"))
HTTP_CONNECT_TIMEOUT = float(os.getenv("HTTP_CONNECT_TIMEOUT", "10"))
HTTP_READ_TIMEOUT = float(os.getenv("HTTP_READ_TIMEOUT", str(REQUEST_TIMEOUT)))
HTTP_WRITE_TIMEOUT = float(os.getenv("HTTP_WRITE_TIMEOUT", str(REQUEST_TIMEOUT)))
HTTP_POOL_TIMEOUT = float(os.getenv("HTTP_POOL_TIMEOUT", "5"))

# æµå¼è¾“å‡ºåˆ†å—ï¼ˆæå‡ CLI å…¼å®¹æ€§ä¸å¤§æ–‡æœ¬ç¨³å®šæ€§ï¼‰
STREAM_TEXT_CHUNK_SIZE = int(os.getenv("STREAM_TEXT_CHUNK_SIZE", "2000"))
STREAM_TOOL_JSON_CHUNK_SIZE = int(os.getenv("STREAM_TOOL_JSON_CHUNK_SIZE", "2000"))
STREAM_THINKING_CHUNK_SIZE = int(os.getenv("STREAM_THINKING_CHUNK_SIZE", str(STREAM_TEXT_CHUNK_SIZE)))

# Anthropic -> OpenAI è½¬æ¢ä¿çœŸåº¦é…ç½®ï¼ˆé»˜è®¤æœ€å¤§ä¿çœŸï¼‰
ANTHROPIC_TRUNCATE_ENABLED = os.getenv("ANTHROPIC_TRUNCATE_ENABLED", "false").lower() in ("1", "true", "yes")
ANTHROPIC_MAX_MESSAGES = int(os.getenv("ANTHROPIC_MAX_MESSAGES", "200"))
ANTHROPIC_MAX_TOTAL_CHARS = int(os.getenv("ANTHROPIC_MAX_TOTAL_CHARS", "1000000"))
ANTHROPIC_MAX_SINGLE_CONTENT = int(os.getenv("ANTHROPIC_MAX_SINGLE_CONTENT", "300000"))
ANTHROPIC_TOOL_INPUT_MAX_CHARS = int(os.getenv("ANTHROPIC_TOOL_INPUT_MAX_CHARS", "200000"))
ANTHROPIC_TOOL_RESULT_MAX_CHARS = int(os.getenv("ANTHROPIC_TOOL_RESULT_MAX_CHARS", "300000"))
ANTHROPIC_CLEAN_SYSTEM_ENABLED = os.getenv("ANTHROPIC_CLEAN_SYSTEM_ENABLED", "false").lower() in ("1", "true", "yes")
ANTHROPIC_CLEAN_ASSISTANT_ENABLED = os.getenv("ANTHROPIC_CLEAN_ASSISTANT_ENABLED", "false").lower() in ("1", "true", "yes")
ANTHROPIC_MERGE_SAME_ROLE_ENABLED = os.getenv("ANTHROPIC_MERGE_SAME_ROLE_ENABLED", "false").lower() in ("1", "true", "yes")
ANTHROPIC_ENSURE_USER_ENDING = os.getenv("ANTHROPIC_ENSURE_USER_ENDING", "true").lower() in ("1", "true", "yes")
ANTHROPIC_EMPTY_ASSISTANT_PLACEHOLDER = os.getenv("ANTHROPIC_EMPTY_ASSISTANT_PLACEHOLDER", " ")
TOOL_DESC_MAX_CHARS = int(os.getenv("TOOL_DESC_MAX_CHARS", "8000"))
TOOL_PARAM_DESC_MAX_CHARS = int(os.getenv("TOOL_PARAM_DESC_MAX_CHARS", "4000"))

# ==================== åŸç”Ÿ Tools æ”¯æŒé…ç½® ====================
# Kiro ç½‘å…³ç°æ”¯æŒåŸç”Ÿ OpenAI tools æ ¼å¼ï¼Œå¯ç”¨åï¼š
# 1. å‡å°‘ token æ¶ˆè€—ï¼ˆtools ä¸å†æ³¨å…¥ system promptï¼‰
# 2. æé«˜è§£æå‡†ç¡®æ€§ï¼ˆåŸç”Ÿ tool_calls ç»“æ„åŒ–è¿”å›ï¼‰
# 3. æ”¯æŒå¹¶è¡Œå·¥å…·è°ƒç”¨
NATIVE_TOOLS_ENABLED = os.getenv("NATIVE_TOOLS_ENABLED", "true").lower() in ("1", "true", "yes")
# é™çº§å¼€å…³ï¼šå½“åŸç”Ÿ tools å¤±è´¥æ—¶ï¼Œæ˜¯å¦å›é€€åˆ°æ–‡æœ¬æ³¨å…¥æ–¹å¼
NATIVE_TOOLS_FALLBACK_ENABLED = os.getenv("NATIVE_TOOLS_FALLBACK_ENABLED", "true").lower() in ("1", "true", "yes")

# ==================== æ™ºèƒ½æ¨¡å‹è·¯ç”±é…ç½® ====================

# æ¨¡å‹è·¯ç”±é…ç½® - "Opus å¤§è„‘, Sonnet åŒæ‰‹" ç­–ç•¥
# æ ¸å¿ƒç†å¿µï¼š
# 1. Opus æ˜¯å¤§è„‘ - è´Ÿè´£è§„åˆ’ã€å†³ç­–ã€æ·±åº¦åˆ†æã€ç”Ÿæˆé«˜è´¨é‡ä¸Šä¸‹æ–‡
# 2. Sonnet æ˜¯åŒæ‰‹ - è´Ÿè´£æ‰§è¡Œã€å·¥å…·è°ƒç”¨ã€ä»£ç ç¼–å†™
# 3. Sonnet çš„å¼±ç‚¹ä¸æ˜¯èƒ½åŠ›ï¼Œè€Œæ˜¯ç†è§£æ·±åº¦ - é€šè¿‡ Opus ç”Ÿæˆçš„ä¸Šä¸‹æ–‡æ¥å¼¥è¡¥
# 4. ç›®æ ‡ Opus ä½¿ç”¨ç‡ï¼š15-25%ï¼ˆå…³é”®æ—¶åˆ»ç”¨ Opusï¼Œå…¶ä½™ç”¨ Sonnetï¼‰
MODEL_ROUTING_CONFIG = {
    # å¯ç”¨æ™ºèƒ½è·¯ç”±
    "enabled": True,

    # ç›®æ ‡æ¨¡å‹æ˜ å°„
    "opus_model": "claude-opus-4-5-20251101",
    "sonnet_model": "claude-sonnet-4-5-20250929",
    "haiku_model": "claude-haiku-4-5-20251001",

    # ============================================================
    # Opus å¹¶å‘é™åˆ¶å™¨ - è¶…å‡ºç›´æ¥é™çº§ï¼Œæ— ç­‰å¾…
    # ============================================================
    "opus_max_concurrent": int(os.getenv("OPUS_MAX_CONCURRENT", "200")),  # Opus æœ€å¤§å¹¶å‘

    # ============================================================
    # ç¬¬é›¶ä¼˜å…ˆçº§ï¼šå¼ºåˆ¶ Opus çš„åœºæ™¯ï¼ˆä¸å—å…¶ä»–æ¡ä»¶å½±å“ï¼‰
    # ============================================================
    # Extended Thinking è¯·æ±‚ - å¿…é¡»ä½¿ç”¨ Opus
    "force_opus_on_thinking": True,

    # Plan Mode æ£€æµ‹ - è§„åˆ’æ¨¡å¼å¿…é¡»ç”¨ Opus
    "force_opus_on_plan_mode": True,

    # ä¸» Agent é¦–è½®è¯·æ±‚ - éœ€è¦ Opus è¿›è¡Œåˆå§‹åˆ†æå’Œè§„åˆ’
    "main_agent_first_turn_opus_probability": 70,  # ä¸» Agent é¦–è½® 70% ç”¨ Opus

    # ä¸» Agent åç»­è¯·æ±‚ - é™ä½æ¦‚ç‡ï¼Œè®© Sonnet æ‰§è¡Œ
    "main_agent_opus_probability": 15,  # ä¸» Agent åç»­ 15% ç”¨ Opus

    # ============================================================
    # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šå¼ºåˆ¶ Opus çš„å…³é”®è¯ï¼ˆçœŸæ­£éœ€è¦æ·±åº¦æ€è€ƒçš„ä»»åŠ¡ï¼‰
    # è¿™äº›æ˜¯ Opus ä½œä¸º"å¤§è„‘"å¿…é¡»å¤„ç†çš„åœºæ™¯
    # ============================================================
    "force_opus_keywords": [
        # === è§„åˆ’å’Œè®¾è®¡ ===
        "åˆ›å»ºé¡¹ç›®", "æ–°å»ºé¡¹ç›®", "åˆå§‹åŒ–é¡¹ç›®", "é¡¹ç›®è§„åˆ’",
        "create project", "new project", "init project", "project plan",
        "ç³»ç»Ÿè®¾è®¡", "æ¶æ„è®¾è®¡", "æ•´ä½“æ¶æ„", "æŠ€æœ¯æ–¹æ¡ˆ",
        "system design", "architecture design", "technical design",
        "æ•´ä½“è§„åˆ’", "å®æ–½æ–¹æ¡ˆ", "è®¾è®¡æ–¹æ¡ˆ",
        "implementation plan", "design plan",

        # === Plan Mode ç›¸å…³ ===
        "enterplanmode", "exitplanmode", "plan mode",
        "è¿›å…¥è§„åˆ’", "è§„åˆ’æ¨¡å¼", "åˆ¶å®šè®¡åˆ’",

        # === å¤æ‚åˆ†æå’Œè¯Šæ–­ ===
        "å…¨é¢åˆ†æ", "æ ¹å› åˆ†æ", "æ·±åº¦åˆ†æ", "é—®é¢˜è¯Šæ–­",
        "root cause", "deep analysis", "comprehensive analysis",
        "ä¸ºä»€ä¹ˆä¼š", "åŸå› æ˜¯ä»€ä¹ˆ", "åˆ†æä¸€ä¸‹åŸå› ",
        "why does", "what causes", "analyze why",

        # === å¤§è§„æ¨¡é‡æ„ ===
        "æ•´ä½“é‡æ„", "å¤§è§„æ¨¡é‡æ„", "ç³»ç»Ÿé‡æ„", "ä»£ç é‡æ„",
        "major refactor", "complete refactor", "system refactor",

        # === å†³ç­–å’Œè¯„ä¼° ===
        "å¦‚ä½•é€‰æ‹©", "å“ªä¸ªæ›´å¥½", "å¯¹æ¯”åˆ†æ", "ä¼˜ç¼ºç‚¹",
        "which is better", "compare", "pros and cons", "trade-off",
        "æœ€ä½³å®è·µ", "æ¨èæ–¹æ¡ˆ",
        "best practice", "recommended approach",

        # === å¤æ‚ä»»åŠ¡ ===
        "ä»é›¶å¼€å§‹", "å®Œæ•´å®ç°", "å…¨æ–°åŠŸèƒ½",
        "from scratch", "complete implementation", "new feature",
    ],

    # ============================================================
    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šå¼ºåˆ¶ Sonnet çš„å…³é”®è¯ï¼ˆæ‰§è¡Œæ€§ä»»åŠ¡ï¼‰
    # Sonnet ä½œä¸º"åŒæ‰‹"é«˜æ•ˆæ‰§è¡Œè¿™äº›ä»»åŠ¡
    # ============================================================
    "force_sonnet_keywords": [
        # === ç®€å•æŸ¥çœ‹æ“ä½œ ===
        "çœ‹çœ‹", "æ˜¾ç¤º", "åˆ—å‡º", "æ‰“å¼€", "æŸ¥çœ‹", "çœ‹ä¸€ä¸‹",
        "show", "list", "display", "view", "open", "check", "look at",

        # === å°æ”¹åŠ¨ ===
        "ä¿®å¤", "è°ƒæ•´", "æ›´æ–°", "æ”¹ä¸€ä¸‹", "æ”¹æˆ", "ä¿®æ”¹", "æ·»åŠ ", "åˆ é™¤",
        "fix", "adjust", "update", "modify", "add", "delete", "remove",
        "æ”¹ä¸ª", "åŠ ä¸ª", "åˆ æ‰", "å»æ‰",

        # === æ‰§è¡Œå‘½ä»¤ ===
        "è¿è¡Œ", "æ‰§è¡Œ", "å¯åŠ¨", "é‡å¯", "åœæ­¢", "æµ‹è¯•", "éƒ¨ç½²",
        "run", "execute", "start", "restart", "stop", "test", "deploy",

        # === ç®€å•é—®ç­” ===
        "ä»€ä¹ˆæ˜¯", "å“ªé‡Œ", "æ˜¯ä¸æ˜¯", "æœ‰æ²¡æœ‰", "æ€ä¹ˆç”¨",
        "what is", "where is", "is it", "do you", "how to use",

        # === è¯»å–/æœç´¢ç±» ===
        "è¯»å–", "è·å–", "æœç´¢", "æŸ¥æ‰¾", "å®šä½", "æ‰¾åˆ°",
        "read", "get", "search", "find", "locate",

        # === å®‰è£…/é…ç½®ç±» ===
        "å®‰è£…", "ä¸‹è½½", "é…ç½®", "è®¾ç½®", "ç¯å¢ƒ",
        "install", "download", "config", "setup", "environment",

        # === ç»§ç»­æ‰§è¡Œ ===
        "ç»§ç»­", "ä¸‹ä¸€æ­¥", "æ¥ç€", "ç„¶å", "å¥½çš„", "å¯ä»¥",
        "continue", "next", "proceed", "then", "ok", "yes",

        # === å·¥å…·è°ƒç”¨ç›¸å…³ ===
        "è°ƒç”¨", "ä½¿ç”¨å·¥å…·", "æ‰§è¡Œå·¥å…·",
        "call", "use tool", "execute tool",
    ],

    # ============================================================
    # Haiku å…³é”®è¯ï¼ˆæç®€ä»»åŠ¡ï¼Œæ”¶ç´§èŒƒå›´ï¼‰
    # æ³¨æ„ï¼šHaiku ä¼˜å…ˆçº§å·²é™ä½ï¼Œåªåœ¨ Sonnet å…³é”®è¯ä¹‹åæ£€æµ‹
    # ============================================================
    "force_haiku_keywords": [
        # ä»…ä¿ç•™æœ€ç®€å•çš„ä»»åŠ¡
        "ç¿»è¯‘æˆ", "ç¿»è¯‘ä¸º",  # æ˜ç¡®çš„ç¿»è¯‘è¯·æ±‚
        "translate to", "translate into",
    ],

    # ============================================================
    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šåŸºäºå¯¹è¯é˜¶æ®µçš„æ™ºèƒ½åˆ¤æ–­
    # æ ¸å¿ƒç­–ç•¥ï¼šé¦–è½®ç”¨ Opus è§„åˆ’ï¼Œåç»­ç”¨ Sonnet æ‰§è¡Œ
    # ============================================================

    # é¦–è½®å¯¹è¯æ£€æµ‹ - é¦–è½®éœ€è¦ Opus è¿›è¡Œä»»åŠ¡ç†è§£å’Œè§„åˆ’
    "first_turn_opus_probability": 60,    # é¦–è½® 60% æ¦‚ç‡ç”¨ Opus

    # ç”¨æˆ·æ¶ˆæ¯æ•°é˜ˆå€¼ï¼ˆä¸å« systemï¼‰- åˆ¤æ–­æ˜¯å¦ä¸ºé¦–è½®
    "first_turn_max_user_messages": 2,    # â‰¤2 æ¡æ¶ˆæ¯è§†ä¸ºé¦–è½®

    # å·¥å…·æ‰§è¡Œé˜¶æ®µæ£€æµ‹ - æœ‰å·¥å…·è°ƒç”¨è¯´æ˜åœ¨æ‰§è¡Œé˜¶æ®µï¼Œç”¨ Sonnet
    "execution_phase_tool_calls": 3,      # å·¥å…·è°ƒç”¨ >= 3 æ¬¡è§†ä¸ºæ‰§è¡Œé˜¶æ®µ
    "execution_phase_sonnet_probability": 90,  # æ‰§è¡Œé˜¶æ®µ 90% ç”¨ Sonnet

    # ============================================================
    # ç¬¬å››ä¼˜å…ˆçº§ï¼šä¿åº•æ¦‚ç‡
    # ============================================================
    "base_opus_probability": 15,          # åŸºç¡€ 15% æ¦‚ç‡ä½¿ç”¨ Opus

    # ============================================================
    # Sonnet ä¸Šä¸‹æ–‡å¢å¼ºé…ç½®
    # æ ¸å¿ƒæ€æƒ³ï¼šSonnet çš„å¼±ç‚¹æ˜¯ç†è§£æ·±åº¦ï¼Œé€šè¿‡æ³¨å…¥é«˜è´¨é‡ä¸Šä¸‹æ–‡æ¥å¼¥è¡¥
    # ============================================================
    "sonnet_context_enhancement": {
        "enabled": True,
        # å½“è·¯ç”±åˆ° Sonnet æ—¶ï¼Œæ³¨å…¥ Opus ç”Ÿæˆçš„æ‘˜è¦/ä¸Šä¸‹æ–‡
        "inject_opus_summary": True,
        # ä¸Šä¸‹æ–‡æ³¨å…¥çš„æœ€å¤§ token æ•°
        "max_context_tokens": 2000,
        # ä¸Šä¸‹æ–‡æ³¨å…¥æ¨¡æ¿
        "context_template": """<opus_analysis>
ä»¥ä¸‹æ˜¯å¯¹å½“å‰ä»»åŠ¡çš„æ·±åº¦åˆ†æï¼ˆç”± Opus ç”Ÿæˆï¼‰ï¼š
{opus_context}
</opus_analysis>

è¯·åŸºäºä»¥ä¸Šåˆ†æï¼Œé«˜æ•ˆæ‰§è¡Œç”¨æˆ·çš„è¯·æ±‚ã€‚""",
    },

    # ============================================================
    # è°ƒè¯•å’Œç›‘æ§
    # ============================================================
    "log_routing_decision": True,         # è®°å½•è·¯ç”±å†³ç­–åŸå› 
}

# ==================== é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ ====================
# æ€§èƒ½ä¼˜åŒ–ï¼šé¿å…åœ¨çƒ­è·¯å¾„ä¸­é‡å¤ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼

# ç”¨äºæ¸…ç† assistant å†…å®¹
_RE_THINKING_TAG = re.compile(r'<thinking>(.*?)</thinking>', re.IGNORECASE | re.DOTALL)
_RE_THINKING_UNCLOSED = re.compile(r'<thinking>.*$', re.DOTALL)
_RE_THINKING_UNOPEN = re.compile(r'^.*</thinking>', re.DOTALL)
_RE_REDACTED_THINKING = re.compile(r'<redacted_thinking>.*?</redacted_thinking>', re.DOTALL)
_RE_SIGNATURE_TAG = re.compile(r'<signature>.*?</signature>', re.DOTALL)

# ç”¨äºè§£æå·¥å…·è°ƒç”¨
_RE_TOOL_CALL = re.compile(r'\[Calling tool:\s*([^\]]+)\]')
_RE_INPUT_PREFIX = re.compile(r'^[\s]*Input:\s*')
_RE_MARKDOWN_START = re.compile(r'```(?:json)?\s*')
_RE_MARKDOWN_END = re.compile(r'\s*```')

# ç”¨äº JSON ä¿®å¤
_RE_TRAILING_COMMA_OBJ = re.compile(r',\s*}')
_RE_TRAILING_COMMA_ARR = re.compile(r',\s*]')

# ç”¨äºåˆå¹¶å“åº”æ—¶çš„æ¸…ç†
_RE_CONTINUATION_INTRO = [
    re.compile(r"^Continuing from.*?:", re.IGNORECASE | re.DOTALL),
    re.compile(r"^Here is the rest of the response:", re.IGNORECASE),
    re.compile(r"^Continuing the JSON:", re.IGNORECASE),
    re.compile(r"^```json\s*"),
    re.compile(r"^```\s*"),
]

# ç”¨äºæ£€æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°
_RE_NEXT_MARKER = re.compile(r'\[Calling tool:|\[Tool Result\]|\[Tool Error\]')

# ç”¨äºè§£æ XML æ ¼å¼çš„å·¥å…·è°ƒç”¨ (Kiro è¿”å›æ ¼å¼)
# åŒ¹é… <ToolName>...</ToolName> æ ¼å¼ï¼Œå·¥å…·åä»¥å¤§å†™å­—æ¯å¼€å¤´
_RE_XML_TOOL_CALL = re.compile(r'<([A-Z][a-zA-Z0-9_]*)>([\s\S]*?)</\1>')
# åŒ¹é… XML å‚æ•° <param_name>value</param_name>
_RE_XML_PARAM = re.compile(r'<([a-z_][a-z0-9_]*)>([\s\S]*?)</\1>', re.IGNORECASE)

# ç”¨äºæ–‡ä»¶è·¯å¾„åŒ¹é…
_RE_FILE_PATH = re.compile(r'[/\\][\w\-\.]+\.(py|js|ts|jsx|tsx|go|rs|java|cpp|c|h|md|yaml|yml|json|toml)')


class ModelRouter:
    """æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨ - "Opus å¤§è„‘, Sonnet åŒæ‰‹" ç­–ç•¥

    æ ¸å¿ƒç†å¿µï¼š
    1. Opus æ˜¯å¤§è„‘ - è´Ÿè´£è§„åˆ’ã€å†³ç­–ã€æ·±åº¦åˆ†æ (15-25%)
       - Plan Modeï¼ˆè§„åˆ’æ¨¡å¼ï¼‰
       - é¦–è½®ä»»åŠ¡ç†è§£å’Œç­–ç•¥åˆ¶å®š
       - æ¶æ„è®¾è®¡å’Œå¤æ‚å†³ç­–
       - é”™è¯¯è¯Šæ–­å’Œæ ¹å› åˆ†æ

    2. Sonnet æ˜¯åŒæ‰‹ - è´Ÿè´£æ‰§è¡Œã€å·¥å…·è°ƒç”¨ã€ä»£ç ç¼–å†™ (70-80%)
       - å·¥å…·è°ƒç”¨å’Œä»£ç æ‰§è¡Œ
       - è¿­ä»£ä¿®æ”¹å’Œè°ƒè¯•
       - ç®€å•é—®ç­”å’ŒæŸ¥è¯¢

    3. Haiku æ˜¯å¿«æ‰‹ - ç®€å•å¿«é€Ÿä»»åŠ¡ (5-10%)
       - æ ¼å¼è½¬æ¢ã€ç¿»è¯‘ã€ç®€å•æ€»ç»“

    Sonnet å¢å¼ºç­–ç•¥ï¼š
    - Sonnet çš„å¼±ç‚¹ä¸æ˜¯èƒ½åŠ›ï¼Œè€Œæ˜¯ç†è§£æ·±åº¦
    - é€šè¿‡æ³¨å…¥ Opus ç”Ÿæˆçš„é«˜è´¨é‡ä¸Šä¸‹æ–‡æ¥å¼¥è¡¥
    - ç¡®ä¿ Sonnet æœ‰è¶³å¤Ÿçš„èƒŒæ™¯ä¿¡æ¯æ¥é«˜æ•ˆæ‰§è¡Œ

    å¹¶å‘æ§åˆ¶ï¼š
    - Opus æœ‰æœ€å¤§å¹¶å‘é™åˆ¶ï¼Œè¶…å‡ºæ—¶è‡ªåŠ¨é™çº§åˆ° Sonnet
    - ç¡®ä¿ Opus èµ„æºä¸è¢«è€—å°½
    """

    def __init__(self, config: dict = None):
        self.config = config or MODEL_ROUTING_CONFIG
        self.stats = {
            "opus": 0,
            "sonnet": 0,
            "haiku": 0,
            "opus_degraded": 0,
            "opus_plan_mode": 0,      # Plan Mode ä½¿ç”¨ Opus çš„æ¬¡æ•°
            "opus_first_turn": 0,     # é¦–è½®ä½¿ç”¨ Opus çš„æ¬¡æ•°
            "opus_keywords": 0,       # å…³é”®è¯è§¦å‘ Opus çš„æ¬¡æ•°
            "sonnet_enhanced": 0,     # Sonnet ä¸Šä¸‹æ–‡å¢å¼ºçš„æ¬¡æ•°
        }
        self._lock = asyncio.Lock()
        # é¢„å¤„ç†å…³é”®è¯ä¸ºå°å†™ï¼Œé¿å…æ¯æ¬¡åŒ¹é…æ—¶é‡å¤è½¬æ¢
        self._opus_keywords_lower = [kw.lower() for kw in self.config.get("force_opus_keywords", [])]
        self._sonnet_keywords_lower = [kw.lower() for kw in self.config.get("force_sonnet_keywords", [])]
        self._haiku_keywords_lower = [kw.lower() for kw in self.config.get("force_haiku_keywords", [])]

        # Opus å¹¶å‘æ§åˆ¶
        self._opus_semaphore = asyncio.Semaphore(self.config.get("opus_max_concurrent", 15))
        self._opus_current = 0

        # Plan Mode æ£€æµ‹æ ‡è®°
        self._plan_mode_markers = [
            "enterplanmode", "exitplanmode", "plan mode",
            "è¿›å…¥è§„åˆ’", "è§„åˆ’æ¨¡å¼", "åˆ¶å®šè®¡åˆ’",
            "in plan mode", "planning mode",
        ]

    def _count_chars(self, messages: list, system: str = "") -> int:
        """ç»Ÿè®¡æ€»å­—ç¬¦æ•°"""
        total = len(str(system)) if system else 0
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                total += len(content)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        total += len(str(item.get("text", "")))
                        total += len(str(item.get("content", "")))
                    elif isinstance(item, str):
                        total += len(item)
        return total

    def _count_tool_calls(self, messages: list) -> int:
        """ç»Ÿè®¡å†å²ä¸­çš„å·¥å…·è°ƒç”¨æ¬¡æ•°"""
        count = 0
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") in ("tool_use", "tool_result"):
                            count += 1
        return count

    def _count_files_mentioned(self, messages: list) -> int:
        """ç»Ÿè®¡æåŠçš„æ–‡ä»¶æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼‰"""
        files = set()

        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                matches = _RE_FILE_PATH.findall(content)
                files.update(matches)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        text = item.get("text", "") or item.get("content", "")
                        if isinstance(text, str):
                            matches = _RE_FILE_PATH.findall(text)
                            files.update(matches)
        return len(files)

    def _get_last_user_message(self, messages: list) -> str:
        """è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯"""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                content = msg.get("content", "")
                if isinstance(content, str):
                    return content
                elif isinstance(content, list):
                    texts = []
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            texts.append(item.get("text", ""))
                    return " ".join(texts)
        return ""

    def _contains_keywords(self, text: str, keywords: list) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å…³é”®è¯ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        text_lower = text.lower()
        for kw in keywords:
            if kw.lower() in text_lower:
                return True
        return False

    def _contains_keywords_optimized(self, text: str, keywords_lower: list) -> tuple[bool, str]:
        """ä¼˜åŒ–ç‰ˆå…³é”®è¯æ£€æŸ¥ï¼Œä½¿ç”¨é¢„å¤„ç†çš„å°å†™å…³é”®è¯åˆ—è¡¨

        Returns:
            (found, matched_keyword)
        """
        text_lower = text.lower()
        for kw in keywords_lower:
            if kw in text_lower:
                return True, kw
        return False, ""

    def _count_user_messages(self, messages: list) -> int:
        """ç»Ÿè®¡ç”¨æˆ·æ¶ˆæ¯æ•°é‡"""
        return sum(1 for msg in messages if msg.get("role") == "user")

    def _get_hash_probability(self, seed: str, threshold: int) -> bool:
        """åŸºäºå“ˆå¸Œçš„æ¦‚ç‡åˆ¤æ–­ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥å¾—åˆ°ç›¸åŒç»“æœ"""
        import hashlib
        hash_val = int(hashlib.md5(seed.encode()).hexdigest()[:8], 16)
        return (hash_val % 100) < threshold

    def _is_sub_agent_request(self, messages: list) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºå­ Agent è¯·æ±‚"""
        # å­ Agent çš„ system prompt é€šå¸¸åŒ…å«è¿™äº›ç‰¹å¾
        if not messages:
            return False
        first_msg = messages[0]
        if first_msg.get("role") != "system":
            return False
        content = first_msg.get("content", "")
        # å­ Agent ç‰¹å¾
        sub_agent_markers = [
            "command execution specialist",
            "exploring codebase",
            "specialized agent",
            "bash commands efficiently",
            "research task",
        ]
        content_lower = content.lower()
        return any(marker in content_lower for marker in sub_agent_markers)

    def _has_thinking_request(self, request_body: dict) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸º Extended Thinking è¯·æ±‚"""
        # æ£€æŸ¥æ˜¯å¦æœ‰ thinking ç›¸å…³å‚æ•°
        if "thinking" in request_body:
            return True
        if "budget_tokens" in request_body:
            return True
        # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦æœ‰ thinking content
        for msg in request_body.get("messages", []):
            content = msg.get("content", [])
            if isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "thinking":
                        return True
        return False

    def _is_plan_mode(self, messages: list) -> bool:
        """æ£€æµ‹æ˜¯å¦å¤„äº Plan Modeï¼ˆè§„åˆ’æ¨¡å¼ï¼‰

        Plan Mode æ˜¯ Claude Code çš„è§„åˆ’æ¨¡å¼ï¼Œéœ€è¦æ·±åº¦æ€è€ƒå’Œåˆ†æ
        """
        # æ£€æŸ¥ system prompt ä¸­æ˜¯å¦æœ‰ plan mode æ ‡è®°
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                content_lower = content.lower()
                for marker in self._plan_mode_markers:
                    if marker in content_lower:
                        return True
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        text = item.get("text", "") or item.get("content", "")
                        if isinstance(text, str):
                            text_lower = text.lower()
                            for marker in self._plan_mode_markers:
                                if marker in text_lower:
                                    return True
        return False

    def _is_debugging_task(self, text: str) -> tuple[bool, str]:
        """æ£€æµ‹æ˜¯å¦ä¸ºè°ƒè¯•/æ’æŸ¥ä»»åŠ¡ - è¿™ç±»ä»»åŠ¡éœ€è¦ Opus çš„æ·±åº¦åˆ†æèƒ½åŠ›

        è°ƒè¯•ä»»åŠ¡ç‰¹å¾ï¼š
        - æ’æŸ¥é—®é¢˜ã€åˆ†æé”™è¯¯
        - ç†è§£é€»è¾‘ã€è¿½è¸ªæµç¨‹
        - è¯Šæ–­åŸå› ã€å®šä½ bug
        """
        debugging_keywords = [
            # æ’æŸ¥å’Œè¯Šæ–­
            "æ’æŸ¥", "æ’é”™", "è¯Šæ–­", "å®šä½é—®é¢˜", "æ‰¾é—®é¢˜",
            "troubleshoot", "diagnose", "debug",
            # é”™è¯¯ç›¸å…³
            "æŠ¥é”™", "å‡ºé”™", "é”™è¯¯", "å¼‚å¸¸", "å¤±è´¥",
            "error", "exception", "failed", "failure",
            # é—®é¢˜åˆ†æ
            "é—®é¢˜", "bug", "issue", "é—®é¢˜æ˜¯",
            # é€»è¾‘åˆ†æ
            "é€»è¾‘", "æµç¨‹", "åŸç†", "æœºåˆ¶",
            "logic", "flow", "mechanism",
            # æŒç»­æ€§é—®é¢˜
            "è¿˜æ˜¯", "ä¾ç„¶", "ä»ç„¶", "ä¸€ç›´", "æ€»æ˜¯",
            "still", "always", "keeps",
            # åŸå› åˆ†æ
            "ä¸ºä»€ä¹ˆ", "åŸå› ", "æ€ä¹ˆå›äº‹", "æ€ä¹ˆäº†",
            "why", "reason", "what happened",
        ]
        text_lower = text.lower()
        for kw in debugging_keywords:
            if kw in text_lower:
                return True, kw
        return False, ""

    def should_use_opus(self, request_body: dict) -> tuple[bool, str]:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ Opus - "Opus å¤§è„‘" ç­–ç•¥

        å†³ç­–ä¼˜å…ˆçº§ï¼š
        0a. Extended Thinking è¯·æ±‚ â†’ å¼ºåˆ¶ Opus
        0b. Plan Modeï¼ˆè§„åˆ’æ¨¡å¼ï¼‰â†’ å¼ºåˆ¶ Opus
        0c. è°ƒè¯•/æ’æŸ¥ä»»åŠ¡ â†’ å¼ºåˆ¶ Opusï¼ˆéœ€è¦æ·±åº¦åˆ†æï¼‰
        1. å¼ºåˆ¶ Opus å…³é”®è¯ â†’ Opus
        2. ä¸» Agent é¦–è½® â†’ é«˜æ¦‚ç‡ Opusï¼ˆä»»åŠ¡ç†è§£å’Œè§„åˆ’ï¼‰
        3. å¼ºåˆ¶ Sonnet å…³é”®è¯ â†’ Sonnet
        4. æ‰§è¡Œé˜¶æ®µï¼ˆå¤§é‡å·¥å…·è°ƒç”¨ï¼‰â†’ é«˜æ¦‚ç‡ Sonnet
        5. é¦–è½®å¯¹è¯ â†’ æ¦‚ç‡ Opus
        6. ä¿åº•æ¦‚ç‡ â†’ ç¡®ä¿ ~15% Opus

        Returns:
            (should_use_opus, reason)
        """
        if not self.config.get("enabled", True):
            return True, "è·¯ç”±å·²ç¦ç”¨"

        messages = request_body.get("messages", [])
        last_user_msg = self._get_last_user_message(messages)
        user_msg_count = self._count_user_messages(messages)

        # ç”Ÿæˆç¨³å®šçš„å“ˆå¸Œç§å­ï¼ˆç›¸åŒè¯·æ±‚å¾—åˆ°ç›¸åŒç»“æœï¼‰
        hash_seed = f"{len(messages)}:{last_user_msg[:200]}"

        # ============================================================
        # ç¬¬é›¶ä¼˜å…ˆçº§ï¼šç‰¹æ®Šåœºæ™¯å¼ºåˆ¶ Opusï¼ˆå¤§è„‘å¿…é¡»ä»‹å…¥ï¼‰
        # ============================================================

        # 0a. Extended Thinking è¯·æ±‚ - å¿…é¡»ä½¿ç”¨ Opus
        if self.config.get("force_opus_on_thinking", True) and self._has_thinking_request(request_body):
            return True, "ExtendedThinking"

        # 0b. Plan Mode æ£€æµ‹ - è§„åˆ’æ¨¡å¼å¿…é¡»ç”¨ Opus
        if self.config.get("force_opus_on_plan_mode", True) and self._is_plan_mode(messages):
            return True, "PlanModeè§„åˆ’"

        # 0c. è°ƒè¯•/æ’æŸ¥ä»»åŠ¡ - éœ€è¦ Opus çš„æ·±åº¦åˆ†æèƒ½åŠ›
        is_debug, debug_kw = self._is_debugging_task(last_user_msg)
        if is_debug:
            return True, f"è°ƒè¯•ä»»åŠ¡[{debug_kw}]"

        # ============================================================
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šå¼ºåˆ¶ Opus å…³é”®è¯ï¼ˆä½¿ç”¨é¢„å¤„ç†çš„å°å†™å…³é”®è¯ï¼‰
        # ============================================================
        found, matched_kw = self._contains_keywords_optimized(last_user_msg, self._opus_keywords_lower)
        if found:
            return True, f"å…³é”®è¯[{matched_kw}]"

        # ============================================================
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šä¸» Agent é¦–è½®è¯·æ±‚ - éœ€è¦ Opus è¿›è¡Œä»»åŠ¡ç†è§£
        # ============================================================
        is_sub_agent = self._is_sub_agent_request(messages)
        if not is_sub_agent and user_msg_count <= 2:
            # ä¸» Agent é¦–è½®ä½¿ç”¨æ›´é«˜çš„ Opus æ¦‚ç‡
            first_turn_prob = self.config.get("main_agent_first_turn_opus_probability", 70)
            if self._get_hash_probability(hash_seed + ":main_first", first_turn_prob):
                return True, f"ä¸»Agenté¦–è½®({first_turn_prob}%)"

        # ============================================================
        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šå¼ºåˆ¶ Sonnet å…³é”®è¯ï¼ˆæ‰§è¡Œæ€§ä»»åŠ¡ï¼‰
        # ============================================================
        found, matched_kw = self._contains_keywords_optimized(last_user_msg, self._sonnet_keywords_lower)
        if found:
            return False, f"ç®€å•ä»»åŠ¡[{matched_kw}]"

        # ============================================================
        # ç¬¬å››ä¼˜å…ˆçº§ï¼šæ‰§è¡Œé˜¶æ®µæ£€æµ‹ - æœ‰å·¥å…·è°ƒç”¨è¯´æ˜åœ¨æ‰§è¡Œï¼Œç”¨ Sonnet
        # ============================================================
        tool_calls = self._count_tool_calls(messages)
        execution_threshold = self.config.get("execution_phase_tool_calls", 3)
        if tool_calls >= execution_threshold:
            sonnet_prob = self.config.get("execution_phase_sonnet_probability", 90)
            if self._get_hash_probability(hash_seed + ":exec", sonnet_prob):
                return False, f"æ‰§è¡Œé˜¶æ®µ({tool_calls}æ¬¡å·¥å…·,{sonnet_prob}%Sonnet)"
            else:
                return True, f"æ‰§è¡Œé˜¶æ®µéšæœºOpus({tool_calls}æ¬¡å·¥å…·)"

        # ============================================================
        # ç¬¬äº”ä¼˜å…ˆçº§ï¼šé¦–è½®å¯¹è¯æ£€æµ‹
        # ============================================================
        first_turn_max = self.config.get("first_turn_max_user_messages", 2)
        if user_msg_count <= first_turn_max:
            first_turn_prob = self.config.get("first_turn_opus_probability", 60)
            if self._get_hash_probability(hash_seed + ":first", first_turn_prob):
                return True, f"é¦–è½®å¯¹è¯({user_msg_count}æ¡,{first_turn_prob}%)"

        # ============================================================
        # ç¬¬å…­ä¼˜å…ˆçº§ï¼šä¿åº•æ¦‚ç‡
        # ============================================================
        base_opus_prob = self.config.get("base_opus_probability", 15)
        if self._get_hash_probability(hash_seed + ":base", base_opus_prob):
            return True, f"ä¿åº•æ¦‚ç‡({base_opus_prob}%)"
        else:
            return False, f"é»˜è®¤Sonnet(msg={user_msg_count},tools={tool_calls})"

    def should_use_haiku(self, request_body: dict) -> tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ Haikuï¼ˆæœ€å¿«é€Ÿçš„æ¨¡å‹ï¼‰

        é€‚ç”¨åœºæ™¯ï¼š
        - ç®€å•æŸ¥è¯¢ã€æ ¼å¼è½¬æ¢ã€å¿«é€Ÿæ€»ç»“
        - ä¸æ¶‰åŠä»£ç ä¿®æ”¹æˆ–å¤æ‚æ¨ç†

        Returns:
            (should_use_haiku, reason)
        """
        messages = request_body.get("messages", [])
        last_user_msg = self._get_last_user_message(messages)

        # æ£€æŸ¥ Haiku å…³é”®è¯
        found, matched_kw = self._contains_keywords_optimized(last_user_msg, self._haiku_keywords_lower)
        if found:
            return True, f"Haikuä»»åŠ¡[{matched_kw}]"

        return False, ""

    async def acquire_opus_slot(self) -> bool:
        """
        å°è¯•è·å– Opus æ§½ä½

        Returns:
            True å¦‚æœæˆåŠŸè·å–ï¼ŒFalse å¦‚æœ Opus å·²æ»¡
        """
        try:
            # éé˜»å¡å°è¯•è·å–
            acquired = self._opus_semaphore.locked() is False
            if acquired:
                await self._opus_semaphore.acquire()
                async with self._lock:
                    self._opus_current += 1
                return True
            return False
        except Exception:
            return False

    def release_opus_slot(self):
        """é‡Šæ”¾ Opus æ§½ä½"""
        try:
            self._opus_semaphore.release()
            # æ³¨æ„ï¼šè¿™é‡Œä¸ç”¨ async withï¼Œå› ä¸ºå¯èƒ½åœ¨éå¼‚æ­¥ä¸Šä¸‹æ–‡è°ƒç”¨
        except ValueError:
            pass  # å·²ç»é‡Šæ”¾è¿‡äº†

    async def route(self, request_body: dict) -> tuple[str, str]:
        """
        è·¯ç”±åˆ°åˆé€‚çš„æ¨¡å‹ - "Opus å¤§è„‘, Sonnet åŒæ‰‹" ç­–ç•¥

        ç­–ç•¥ä¼˜å…ˆçº§ï¼š
        1. é Opus è¯·æ±‚ç›´æ¥æ”¾è¡Œ
        2. æ£€æŸ¥æ˜¯å¦åº”è¯¥ç”¨ Opusï¼ˆå¤æ‚ä»»åŠ¡ã€è§„åˆ’ã€è°ƒè¯•ï¼‰
        3. æ£€æŸ¥ Opus å¹¶å‘é™åˆ¶ï¼Œè¶…å‡ºæ—¶é™çº§åˆ° Sonnet
        4. æ£€æŸ¥æ˜¯å¦åº”è¯¥ç”¨ Haikuï¼ˆæç®€ä»»åŠ¡ï¼Œä¼˜å…ˆçº§æœ€ä½ï¼‰
        5. é»˜è®¤ä½¿ç”¨ Sonnet

        Returns:
            (routed_model, reason)
        """
        original_model = request_body.get("model", "")

        # åªå¤„ç† Opus è¯·æ±‚ï¼ˆSonnet/Haiku è¯·æ±‚ç›´æ¥æ”¾è¡Œï¼‰
        if "opus" not in original_model.lower():
            async with self._lock:
                if "haiku" in original_model.lower():
                    self.stats["haiku"] += 1
                else:
                    self.stats["sonnet"] += 1
            return original_model, "éOpusè¯·æ±‚"

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç”¨ Opusï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        should_opus, reason = self.should_use_opus(request_body)

        if should_opus:
            # æ£€æŸ¥ Opus å¹¶å‘é™åˆ¶
            max_concurrent = self.config.get("opus_max_concurrent", 15)
            async with self._lock:
                current_opus = self.stats["opus"] - self.stats.get("opus_completed", 0)

            if current_opus >= max_concurrent:
                # Opus å·²æ»¡ï¼Œé™çº§åˆ° Sonnet
                async with self._lock:
                    self.stats["sonnet"] += 1
                    self.stats["opus_degraded"] += 1
                return self.config.get("sonnet_model"), f"Opuså·²æ»¡({current_opus}/{max_concurrent}),é™çº§Sonnet"

            async with self._lock:
                self.stats["opus"] += 1
            return self.config.get("opus_model"), reason

        # Haiku æ£€æµ‹ï¼ˆä¼˜å…ˆçº§æœ€ä½ï¼Œåªå¤„ç†æç®€ä»»åŠ¡ï¼‰
        should_haiku, haiku_reason = self.should_use_haiku(request_body)
        if should_haiku:
            async with self._lock:
                self.stats["haiku"] += 1
            return self.config.get("haiku_model"), haiku_reason

        # é»˜è®¤ä½¿ç”¨ Sonnet
        async with self._lock:
            self.stats["sonnet"] += 1
        return self.config.get("sonnet_model"), reason

    def route_sync(self, request_body: dict) -> tuple[str, str]:
        """
        è·¯ç”±åˆ°åˆé€‚çš„æ¨¡å‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äºéå¼‚æ­¥ä¸Šä¸‹æ–‡ï¼‰
        æ³¨æ„ï¼šç»Ÿè®¡æ•°æ®åœ¨é«˜å¹¶å‘ä¸‹å¯èƒ½ä¸ç²¾ç¡®

        Returns:
            (routed_model, reason)
        """
        original_model = request_body.get("model", "")

        # åªå¤„ç† Opus è¯·æ±‚
        if "opus" not in original_model.lower():
            self.stats["other"] += 1
            return original_model, "éOpusè¯·æ±‚"

        should_opus, reason = self.should_use_opus(request_body)

        if should_opus:
            self.stats["opus"] += 1
            return self.config.get("opus_model", "claude-opus-4-5-20251101"), reason
        else:
            self.stats["sonnet"] += 1
            return self.config.get("sonnet_model", "claude-sonnet-4-5-20250929"), reason

    def get_stats(self) -> dict:
        """è·å–è·¯ç”±ç»Ÿè®¡"""
        total = self.stats["opus"] + self.stats["sonnet"] + self.stats["haiku"]
        if total == 0:
            opus_pct = sonnet_pct = haiku_pct = 0
        else:
            opus_pct = round(self.stats["opus"] / total * 100, 1)
            sonnet_pct = round(self.stats["sonnet"] / total * 100, 1)
            haiku_pct = round(self.stats["haiku"] / total * 100, 1)

        return {
            "opus_requests": self.stats["opus"],
            "sonnet_requests": self.stats["sonnet"],
            "haiku_requests": self.stats["haiku"],
            "opus_degraded": self.stats.get("opus_degraded", 0),
            "total_requests": total,
            "opus_percent": f"{opus_pct}%",
            "sonnet_percent": f"{sonnet_pct}%",
            "haiku_percent": f"{haiku_pct}%",
            "opus_max_concurrent": self.config.get("opus_max_concurrent", 10),
        }


# å…¨å±€æ¨¡å‹è·¯ç”±å™¨å®ä¾‹
model_router = ModelRouter(MODEL_ROUTING_CONFIG)

# ==================== é«˜å¹¶å‘é…ç½® ====================

# HTTP è¿æ¥æ± é…ç½® - é’ˆå¯¹é«˜å¹¶å‘ä¼˜åŒ–
# å…³é”®ï¼šç¦ç”¨ HTTP/2ï¼Œä½¿ç”¨ HTTP/1.1 å¤šè¿æ¥æ¨¡å¼
# åŸå› ï¼šHTTP/2 å¤šè·¯å¤ç”¨ä¼šè®©æ‰€æœ‰è¯·æ±‚èµ°åŒä¸€è¿æ¥ï¼Œtokens å¯èƒ½è¯¯è®¤ä¸ºæ˜¯åŒä¸€ç»ˆç«¯
HTTP_POOL_MAX_CONNECTIONS = int(os.getenv("HTTP_POOL_MAX_CONNECTIONS", "2000"))
HTTP_POOL_MAX_KEEPALIVE = int(os.getenv("HTTP_POOL_MAX_KEEPALIVE", "500"))
HTTP_POOL_KEEPALIVE_EXPIRY = int(os.getenv("HTTP_POOL_KEEPALIVE_EXPIRY", "30"))
HTTP_USE_HTTP2 = os.getenv("HTTP_USE_HTTP2", "false").lower() in ("1", "true", "yes")

# ==================== æ—¥å¿— ====================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("ai_history_manager_api")

# ==================== å…¨å±€ HTTP å®¢æˆ·ç«¯ ====================

# å…¨å±€ HTTP å®¢æˆ·ç«¯ (è¿æ¥æ± å¤ç”¨ï¼Œæè‡´é«˜å¹¶å‘)
http_client: Optional[httpx.AsyncClient] = None


def get_http_client() -> httpx.AsyncClient:
    """è·å–å…¨å±€ HTTP å®¢æˆ·ç«¯"""
    global http_client
    if http_client is None:
        raise RuntimeError("HTTP client not initialized. Server not started properly.")
    return http_client


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† - åˆå§‹åŒ–å’Œæ¸…ç†å…¨å±€èµ„æº"""
    global http_client

    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("åˆå§‹åŒ–å…¨å±€ HTTP å®¢æˆ·ç«¯ (é«˜å¹¶å‘æ¨¡å¼)...")

    # åˆ›å»ºè¿æ¥æ± é™åˆ¶ - å¤§å®¹é‡
    limits = httpx.Limits(
        max_connections=HTTP_POOL_MAX_CONNECTIONS,
        max_keepalive_connections=HTTP_POOL_MAX_KEEPALIVE,
        keepalive_expiry=HTTP_POOL_KEEPALIVE_EXPIRY,
    )

    # åˆ›å»ºå…¨å±€ HTTP å®¢æˆ·ç«¯ - ä¼˜åŒ–é…ç½®
    # å…³é”®ä¿®æ”¹ï¼šç¦ç”¨ HTTP/2ï¼Œä½¿ç”¨ HTTP/1.1
    # åŸå› ï¼šHTTP/2 å¤šè·¯å¤ç”¨è®©æ‰€æœ‰è¯·æ±‚èµ°åŒä¸€è¿æ¥ï¼Œtokens å¯èƒ½è¯¯è®¤ä¸ºæ˜¯åŒä¸€ç»ˆç«¯
    # HTTP/1.1 å…è®¸å¤šä¸ªç‹¬ç«‹çš„ TCP è¿æ¥ï¼Œæ¯ä¸ªè¯·æ±‚å¯ä»¥å¹¶è¡Œå¤„ç†
    timeout = httpx.Timeout(
        timeout=REQUEST_TIMEOUT,
        connect=HTTP_CONNECT_TIMEOUT,
        read=HTTP_READ_TIMEOUT,
        write=HTTP_WRITE_TIMEOUT,
        pool=HTTP_POOL_TIMEOUT,
    )
    http_client = httpx.AsyncClient(
        timeout=timeout,
        limits=limits,
        http2=HTTP_USE_HTTP2,  # ä½¿ç”¨é…ç½®çš„ HTTP ç‰ˆæœ¬
    )

    logger.info(f"HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–: max_connections={HTTP_POOL_MAX_CONNECTIONS}, "
                f"keepalive={HTTP_POOL_MAX_KEEPALIVE}")

    yield  # åº”ç”¨è¿è¡Œä¸­

    # å…³é—­æ—¶æ¸…ç†
    logger.info("å…³é—­å…¨å±€ HTTP å®¢æˆ·ç«¯...")
    if http_client:
        await http_client.aclose()
        http_client = None
    logger.info("èµ„æºæ¸…ç†å®Œæˆ")


# ==================== FastAPI App ====================

app = FastAPI(
    title="AI History Manager API",
    description="OpenAI å…¼å®¹ APIï¼Œé›†æˆæ™ºèƒ½å†å²æ¶ˆæ¯ç®¡ç†",
    version="1.0.0",
    lifespan=lifespan,  # ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
)


# ==================== æ•°æ®æ¨¡å‹ ====================

class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str
    messages: list[dict]
    stream: Optional[bool] = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    stop: Optional[list[str]] = None


# ==================== è¾…åŠ©å‡½æ•° ====================

def generate_session_id(messages: list[dict]) -> str:
    """åŸºäºæ¶ˆæ¯å†…å®¹ç”Ÿæˆä¼šè¯ ID"""
    if not messages:
        return "default"

    content_parts = []
    for msg in messages[:3]:
        content = msg.get("content", "")
        if isinstance(content, str):
            content_parts.append(content[:100])

    if content_parts:
        import hashlib
        return hashlib.md5("".join(content_parts).encode()).hexdigest()[:16]

    return "default"


def extract_user_content(messages: list[dict]) -> str:
    """æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯"""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            content = msg.get("content", "")
            if isinstance(content, str):
                return content
    return ""


# ==================== ä¸Šä¸‹æ–‡å¢å¼ºæœºåˆ¶ ====================

# Session ä¸Šä¸‹æ–‡å­˜å‚¨ï¼ˆå†…å­˜ï¼‰
_session_contexts = {}


def get_session_context(session_id: str) -> dict:
    """è·å– session çš„é¡¹ç›®ä¸Šä¸‹æ–‡"""
    return _session_contexts.get(session_id, {
        "content": "",
        "last_updated_at": 0,
        "message_count_at_update": 0,
        "version": 0,
    })


def update_session_context(session_id: str, context: str, message_count: int):
    """æ›´æ–° session çš„é¡¹ç›®ä¸Šä¸‹æ–‡"""
    _session_contexts[session_id] = {
        "content": context,
        "last_updated_at": time.time(),
        "message_count_at_update": message_count,
        "version": _session_contexts.get(session_id, {}).get("version", 0) + 1,
    }


def count_user_messages(messages: list[dict]) -> int:
    """ç»Ÿè®¡ç”¨æˆ·æ¶ˆæ¯æ•°é‡"""
    return sum(1 for msg in messages if msg.get("role") == "user")


async def extract_project_context(messages: list[dict], session_id: str) -> str:
    """ä»å¯¹è¯å†å²ä¸­æå–é¡¹ç›®ä¸Šä¸‹æ–‡

    Args:
        messages: å¯¹è¯å†å²ï¼ˆå»ºè®®ä¼ å…¥æœ€è¿‘ 20 æ¡ï¼‰
        session_id: ä¼šè¯ ID

    Returns:
        é¡¹ç›®ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ˆ100-200 tokensï¼‰
    """
    if not CONTEXT_ENHANCEMENT_CONFIG["enabled"]:
        return ""

    if not messages:
        return ""

    # æ ¼å¼åŒ–å¯¹è¯å†å²
    conversation_history = []
    for msg in messages[-20:]:  # åªçœ‹æœ€è¿‘ 20 æ¡
        role = msg.get("role", "")
        content = msg.get("content", "")

        # å¤„ç†å¤æ‚ content ç»“æ„
        if isinstance(content, list):
            content_str = ""
            for item in content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        content_str += item.get("text", "")
                    elif item.get("type") == "tool_use":
                        content_str += f"[Tool: {item.get('name', 'unknown')}]"
                    elif item.get("type") == "tool_result":
                        content_str += "[Tool Result]"
            content = content_str

        if isinstance(content, str) and content.strip():
            # æˆªæ–­è¿‡é•¿çš„æ¶ˆæ¯
            if len(content) > 500:
                content = content[:500] + "..."
            conversation_history.append(f"{role}: {content}")

    if not conversation_history:
        return ""

    # æ„å»ºæå–æç¤ºè¯
    prompt = CONTEXT_ENHANCEMENT_CONFIG["extraction_prompt"].format(
        conversation_history="\n".join(conversation_history)
    )

    # è°ƒç”¨ LLM æå–ä¸Šä¸‹æ–‡
    context_id = uuid.uuid4().hex[:8]
    request_body = {
        "model": CONTEXT_ENHANCEMENT_CONFIG["model"],
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "max_tokens": CONTEXT_ENHANCEMENT_CONFIG["max_tokens"] + 50,  # ç•™ä¸€äº›ä½™é‡
    }

    headers = {
        "Authorization": f"Bearer {KIRO_API_KEY}",
        "Content-Type": "application/json",
        "X-Request-ID": f"context_{context_id}",
        "X-Trace-ID": f"trace_{uuid.uuid4().hex}",
    }

    try:
        client = get_http_client()
        response = await client.post(
            KIRO_PROXY_URL,
            json=request_body,
            headers=headers,
            timeout=30.0,
        )

        if response.status_code == 200:
            result = response.json()
            context = result.get("choices", [{}])[0].get("message", {}).get("content", "").strip()

            # éªŒè¯é•¿åº¦
            if len(context) < CONTEXT_ENHANCEMENT_CONFIG["min_tokens"] * 4:  # ç²—ç•¥ä¼°ç®—
                logger.warning(f"[{context_id}] æå–çš„ä¸Šä¸‹æ–‡è¿‡çŸ­: {len(context)} chars")
            elif len(context) > CONTEXT_ENHANCEMENT_CONFIG["max_tokens"] * 4:
                logger.warning(f"[{context_id}] æå–çš„ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œæˆªæ–­: {len(context)} chars")
                context = context[:CONTEXT_ENHANCEMENT_CONFIG["max_tokens"] * 4]

            logger.info(f"[{context_id}] âœ… ä¸Šä¸‹æ–‡æå–æˆåŠŸ: {len(context)} chars")
            return context
        else:
            logger.error(f"[{context_id}] ä¸Šä¸‹æ–‡æå–å¤±è´¥: {response.status_code}")
            return ""

    except Exception as e:
        logger.error(f"[{context_id}] ä¸Šä¸‹æ–‡æå–å¼‚å¸¸: {e}")
        return ""


async def enhance_user_message(messages: list[dict], session_id: str) -> list[dict]:
    """å¢å¼ºç”¨æˆ·æ¶ˆæ¯ï¼ˆåœ¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸­æ³¨å…¥é¡¹ç›®ä¸Šä¸‹æ–‡ï¼‰

    Args:
        messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
        session_id: ä¼šè¯ ID

    Returns:
        å¢å¼ºåçš„æ¶ˆæ¯åˆ—è¡¨
    """
    if not CONTEXT_ENHANCEMENT_CONFIG["enabled"]:
        return messages

    if not messages:
        return messages

    # æ£€æŸ¥æœ€åä¸€æ¡æ˜¯å¦æ˜¯ç”¨æˆ·æ¶ˆæ¯
    if messages[-1].get("role") != "user":
        return messages

    # è·å–å½“å‰ä¸Šä¸‹æ–‡
    session_context = get_session_context(session_id)
    user_message_count = count_user_messages(messages)

    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°ä¸Šä¸‹æ–‡
    should_update = (
        not session_context["content"] or  # é¦–æ¬¡
        user_message_count - session_context["message_count_at_update"] >= CONTEXT_ENHANCEMENT_CONFIG["update_interval"]  # è¶…è¿‡é—´éš”
    )

    if should_update:
        logger.info(f"[{session_id}] ğŸ”„ è§¦å‘ä¸Šä¸‹æ–‡æå–ï¼ˆç”¨æˆ·æ¶ˆæ¯æ•°: {user_message_count}ï¼‰")
        context = await extract_project_context(messages, session_id)
        if context:
            update_session_context(session_id, context, user_message_count)
            session_context = get_session_context(session_id)
    else:
        context = session_context["content"]

    # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œç›´æ¥è¿”å›
    if not context:
        return messages

    # å¢å¼ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    enhanced_messages = messages.copy()
    last_message = enhanced_messages[-1].copy()
    original_content = last_message.get("content", "")

    # å¤„ç†å¤æ‚ content ç»“æ„
    if isinstance(original_content, list):
        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ª text ç±»å‹å¹¶å¢å¼º
        enhanced_content = []
        text_enhanced = False
        for item in original_content:
            if isinstance(item, dict) and item.get("type") == "text" and not text_enhanced:
                enhanced_text = CONTEXT_ENHANCEMENT_CONFIG["enhancement_template"].format(
                    context=context,
                    user_input=item.get("text", "")
                )
                enhanced_content.append({"type": "text", "text": enhanced_text})
                text_enhanced = True
            else:
                enhanced_content.append(item)
        last_message["content"] = enhanced_content
    elif isinstance(original_content, str):
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥å¢å¼º
        enhanced_text = CONTEXT_ENHANCEMENT_CONFIG["enhancement_template"].format(
            context=context,
            user_input=original_content
        )
        last_message["content"] = enhanced_text

    enhanced_messages[-1] = last_message

    logger.info(f"[{session_id}] ğŸ¯ ä¸Šä¸‹æ–‡å¢å¼ºå®Œæˆ: {len(original_content) if isinstance(original_content, str) else 'complex'} -> {len(str(last_message['content']))} chars")

    return enhanced_messages


# æ‘˜è¦ç”Ÿæˆæ¨¡å‹é…ç½®
SUMMARY_MODEL = os.getenv("SUMMARY_MODEL", "claude-haiku-4-5-20251001")


async def call_kiro_for_summary(prompt: str) -> str:
    """è°ƒç”¨ Kiro API ç”Ÿæˆæ‘˜è¦ - ä½¿ç”¨å…¨å±€ HTTP å®¢æˆ·ç«¯"""
    summary_id = uuid.uuid4().hex[:8]
    request_body = {
        "model": SUMMARY_MODEL,  # ä½¿ç”¨ Haiku 4.5 å¿«é€Ÿæ¨¡å‹
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "max_tokens": 2000,
    }

    # æ·»åŠ å”¯ä¸€è¯·æ±‚æ ‡è¯†
    headers = {
        "Authorization": f"Bearer {KIRO_API_KEY}",
        "Content-Type": "application/json",
        "X-Request-ID": f"summary_{summary_id}",
        "X-Trace-ID": f"trace_{uuid.uuid4().hex}",
    }

    try:
        client = get_http_client()
        response = await client.post(
            KIRO_PROXY_URL,
            json=request_body,
            headers=headers,
            timeout=60,  # æ‘˜è¦è¯·æ±‚ä½¿ç”¨è¾ƒçŸ­è¶…æ—¶
        )

        if response.status_code == 200:
            data = response.json()
            return data.get("choices", [{}])[0].get("message", {}).get("content", "")
    except Exception as e:
        logger.warning(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")

    return ""


# ==================== å¼‚æ­¥æ‘˜è¦ç®¡ç†å™¨ ====================

class AsyncSummaryManager:
    """å¼‚æ­¥æ‘˜è¦ç®¡ç†å™¨ - åå°ç”Ÿæˆæ‘˜è¦ï¼Œä¸é˜»å¡ä¸»è¯·æ±‚

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. é¦–æ¬¡è¯·æ±‚ï¼šä½¿ç”¨ç®€å•æˆªæ–­å¿«é€Ÿå“åº”ï¼Œåå°å¯åŠ¨æ‘˜è¦ä»»åŠ¡
    2. åç»­è¯·æ±‚ï¼šä½¿ç”¨ç¼“å­˜çš„æ‘˜è¦ï¼Œ0 å»¶è¿Ÿ
    3. å¢é‡æ›´æ–°ï¼šæ¯ N æ¡æ–°æ¶ˆæ¯åï¼Œåå°æ›´æ–°æ‘˜è¦
    """

    def __init__(self):
        # æ‘˜è¦ç¼“å­˜ï¼šsession_id -> {"summary": str, "message_count": int, "timestamp": float, "original_tokens": int}
        self._summary_cache: dict[str, dict] = {}
        # æ­£åœ¨è¿›è¡Œçš„ä»»åŠ¡ï¼šsession_id -> asyncio.Task
        self._pending_tasks: dict[str, asyncio.Task] = {}
        # é”
        self._lock = asyncio.Lock()
        # ç»Ÿè®¡
        self._stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "async_tasks": 0,
            "tokens_saved": 0,  # é€šè¿‡ç¼“å­˜èŠ‚çœçš„ tokens
        }

    def get_cached_summary(self, session_id: str) -> tuple[str, bool, int]:
        """è·å–ç¼“å­˜çš„æ‘˜è¦

        Returns:
            (summary, is_valid, original_tokens) - æ‘˜è¦å†…å®¹ã€æ˜¯å¦æœ‰æ•ˆã€åŸå§‹ token æ•°
        """
        cache_entry = self._summary_cache.get(session_id)
        if cache_entry and cache_entry.get("summary"):
            self._stats["cache_hits"] += 1
            original_tokens = cache_entry.get("original_tokens", 0)
            return cache_entry["summary"], True, original_tokens
        self._stats["cache_misses"] += 1
        return "", False, 0

    def get_cache_info(self, session_id: str) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯ï¼Œç”¨äºè®¡è´¹æ¨¡æ‹Ÿ

        Returns:
            {
                "hit": bool,
                "original_tokens": int,  # åŸå§‹æ¶ˆæ¯çš„ token æ•°
                "cached_tokens": int,    # ç¼“å­˜çš„æ‘˜è¦ token æ•°
                "saved_tokens": int,     # èŠ‚çœçš„ tokens
            }
        """
        cache_entry = self._summary_cache.get(session_id)
        if cache_entry and cache_entry.get("summary"):
            original_tokens = cache_entry.get("original_tokens", 0)
            cached_tokens = cache_entry.get("cached_tokens", 0)
            return {
                "hit": True,
                "original_tokens": original_tokens,
                "cached_tokens": cached_tokens,
                "saved_tokens": max(0, original_tokens - cached_tokens),
            }
        return {"hit": False, "original_tokens": 0, "cached_tokens": 0, "saved_tokens": 0}

    def should_update_summary(self, session_id: str, current_message_count: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ‘˜è¦

        æ¡ä»¶ï¼š
        1. æ²¡æœ‰ç¼“å­˜
        2. æ¶ˆæ¯æ•°å¢åŠ è¶…è¿‡é˜ˆå€¼
        """
        cache_entry = self._summary_cache.get(session_id)
        if not cache_entry:
            return True

        cached_count = cache_entry.get("message_count", 0)
        update_interval = ASYNC_SUMMARY_CONFIG.get("update_interval_messages", 5)

        return (current_message_count - cached_count) >= update_interval

    def is_task_pending(self, session_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„æ‘˜è¦ä»»åŠ¡"""
        task = self._pending_tasks.get(session_id)
        return task is not None and not task.done()

    async def schedule_summary_task(
        self,
        session_id: str,
        messages: list,
        manager: "HistoryManager",
        user_content: str
    ):
        """è°ƒåº¦åå°æ‘˜è¦ä»»åŠ¡

        ä¸é˜»å¡ä¸»è¯·æ±‚ï¼Œåå°ç”Ÿæˆæ‘˜è¦å¹¶æ›´æ–°ç¼“å­˜
        """
        if not ASYNC_SUMMARY_CONFIG.get("enabled", True):
            return

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œ
        if self.is_task_pending(session_id):
            logger.debug(f"[{session_id[:8]}] å¼‚æ­¥æ‘˜è¦ä»»åŠ¡å·²åœ¨è¿è¡Œï¼Œè·³è¿‡")
            return

        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°
        async with self._lock:
            # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡
            done_sessions = [s for s, t in self._pending_tasks.items() if t.done()]
            for s in done_sessions:
                del self._pending_tasks[s]

            if len(self._pending_tasks) >= ASYNC_SUMMARY_CONFIG.get("max_pending_tasks", 100):
                logger.warning(f"[{session_id[:8]}] å¼‚æ­¥æ‘˜è¦é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡")
                return

            # åˆ›å»ºåå°ä»»åŠ¡
            task = asyncio.create_task(
                self._generate_summary_background(session_id, messages, manager, user_content)
            )
            self._pending_tasks[session_id] = task
            self._stats["async_tasks"] += 1

        logger.info(f"[{session_id[:8]}] ğŸš€ å¯åŠ¨åå°æ‘˜è¦ä»»åŠ¡")

    async def _generate_summary_background(
        self,
        session_id: str,
        messages: list,
        manager: "HistoryManager",
        user_content: str
    ):
        """åå°ç”Ÿæˆæ‘˜è¦ä»»åŠ¡"""
        try:
            timeout = ASYNC_SUMMARY_CONFIG.get("task_timeout", 30)

            # è®¡ç®—åŸå§‹æ¶ˆæ¯çš„ token æ•°ï¼ˆç”¨äºè®¡è´¹æ¨¡æ‹Ÿï¼‰
            original_tokens = 0
            for msg in messages:
                content = msg.get("content", "")
                if isinstance(content, str):
                    original_tokens += estimate_tokens(content)
                elif isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict):
                            original_tokens += estimate_tokens(str(item.get("text", "") or item.get("content", "")))

            # ä½¿ç”¨ asyncio.wait_for æ·»åŠ è¶…æ—¶
            processed_messages = await asyncio.wait_for(
                manager.pre_process_async(messages, user_content, call_kiro_for_summary),
                timeout=timeout
            )

            # ä»å¤„ç†åçš„æ¶ˆæ¯ä¸­æå–æ‘˜è¦ï¼Œå¹¶è®¡ç®—æ‘˜è¦ token æ•°
            summary = ""
            cached_tokens = 0
            for msg in processed_messages:
                content = msg.get("content", "")
                if isinstance(content, str):
                    cached_tokens += estimate_tokens(content)
                    if "[å†å²æ‘˜è¦]" in content:
                        summary = content

            if summary:
                # æ›´æ–°ç¼“å­˜ï¼ŒåŒ…å« token ä¿¡æ¯
                self._summary_cache[session_id] = {
                    "summary": summary,
                    "message_count": len(messages),
                    "timestamp": time.time(),
                    "processed_messages": processed_messages,
                    "original_tokens": original_tokens,
                    "cached_tokens": cached_tokens,
                }
                saved = original_tokens - cached_tokens
                self._stats["tokens_saved"] += max(0, saved)
                logger.info(f"[{session_id[:8]}] âœ… åå°æ‘˜è¦å®Œæˆ: {original_tokens} -> {cached_tokens} tokens (èŠ‚çœ {saved})")
            else:
                logger.debug(f"[{session_id[:8]}] åå°æ‘˜è¦å®Œæˆï¼Œä½†æ— æ‘˜è¦å†…å®¹")

        except asyncio.TimeoutError:
            logger.warning(f"[{session_id[:8]}] âš ï¸ åå°æ‘˜è¦è¶…æ—¶")
        except Exception as e:
            logger.error(f"[{session_id[:8]}] âŒ åå°æ‘˜è¦å¤±è´¥: {e}")

    def get_cached_processed_messages(self, session_id: str) -> list | None:
        """è·å–ç¼“å­˜çš„å·²å¤„ç†æ¶ˆæ¯ï¼ˆåŒ…å«æ‘˜è¦ï¼‰"""
        cache_entry = self._summary_cache.get(session_id)
        if cache_entry:
            return cache_entry.get("processed_messages")
        return None

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self._stats,
            "cache_size": len(self._summary_cache),
            "pending_tasks": len([t for t in self._pending_tasks.values() if not t.done()]),
        }


# å…¨å±€å¼‚æ­¥æ‘˜è¦ç®¡ç†å™¨å®ä¾‹
async_summary_manager = AsyncSummaryManager()


# ==================== Token è®¡æ•° ====================

# Token ä¼°ç®—ç¼“å­˜ - é¿å…å¯¹ç›¸åŒæ–‡æœ¬é‡å¤è®¡ç®—
# ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œä½œä¸ºç¼“å­˜é”®ï¼Œé¿å…å­˜å‚¨å¤§é‡æ–‡æœ¬
@lru_cache(maxsize=2048)
def _estimate_tokens_cached(text_hash: int, text_len: int, chinese_ratio_pct: int) -> int:
    """åŸºäºæ–‡æœ¬ç‰¹å¾çš„ token ä¼°ç®—ï¼ˆå¸¦ç¼“å­˜ï¼‰

    Args:
        text_hash: æ–‡æœ¬çš„å“ˆå¸Œå€¼
        text_len: æ–‡æœ¬é•¿åº¦
        chinese_ratio_pct: ä¸­æ–‡å­—ç¬¦å æ¯”ï¼ˆ0-100ï¼‰

    Returns:
        ä¼°ç®—çš„ token æ•°é‡
    """
    chinese_chars = int(text_len * chinese_ratio_pct / 100)
    other_chars = text_len - chinese_chars

    # ä¸­æ–‡çº¦ 1.5 å­—ç¬¦/tokenï¼Œå…¶ä»–çº¦ 4 å­—ç¬¦/token
    chinese_tokens = chinese_chars / 1.5
    other_tokens = other_chars / 4

    return int(chinese_tokens + other_tokens)


def estimate_tokens(text: str) -> int:
    """ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼ˆä¼˜åŒ–ç‰ˆï¼Œå¸¦ç¼“å­˜ï¼‰

    ç®€å•ä¼°ç®—è§„åˆ™ï¼š
    - è‹±æ–‡/ä»£ç ï¼šçº¦ 4 ä¸ªå­—ç¬¦ = 1 token
    - ä¸­æ–‡ï¼šçº¦ 1.5 ä¸ªå­—ç¬¦ = 1 token
    - æ··åˆè®¡ç®—å–å¹³å‡

    ä¼˜åŒ–ï¼š
    - ä½¿ç”¨ LRU ç¼“å­˜é¿å…é‡å¤è®¡ç®—
    - å¯¹äºçŸ­æ–‡æœ¬ç›´æ¥è®¡ç®—ï¼Œé¿å…ç¼“å­˜å¼€é”€
    """
    if not text:
        return 0

    text_len = len(text)

    # çŸ­æ–‡æœ¬ç›´æ¥è®¡ç®—ï¼Œé¿å…ç¼“å­˜å¼€é”€
    if text_len < 100:
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = text_len - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)

    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°å¹¶è®¡ç®—å æ¯”
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    chinese_ratio_pct = int(chinese_chars * 100 / text_len) if text_len > 0 else 0

    # ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
    text_hash = hash(text)

    return _estimate_tokens_cached(text_hash, text_len, chinese_ratio_pct)


def estimate_messages_tokens(messages: list, system: str = "") -> int:
    """ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€» token æ•°"""
    total = 0

    # system prompt
    if system:
        if isinstance(system, str):
            total += estimate_tokens(system)
        elif isinstance(system, list):
            for item in system:
                if isinstance(item, dict):
                    total += estimate_tokens(item.get("text", ""))
                elif isinstance(item, str):
                    total += estimate_tokens(item)

    # messages
    for msg in messages:
        content = msg.get("content", "")
        if isinstance(content, str):
            total += estimate_tokens(content)
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        total += estimate_tokens(item.get("text", ""))
                    elif item.get("type") == "tool_use":
                        total += estimate_tokens(json.dumps(item.get("input", {})))
                    elif item.get("type") == "tool_result":
                        result = item.get("content", "")
                        if isinstance(result, str):
                            total += estimate_tokens(result)
                        elif isinstance(result, list):
                            for r in result:
                                if isinstance(r, dict):
                                    total += estimate_tokens(r.get("text", ""))
                elif isinstance(item, str):
                    total += estimate_tokens(item)

        # æ¯æ¡æ¶ˆæ¯é¢å¤–å¼€é”€ï¼ˆrole, formattingç­‰ï¼‰
        total += 4

    return total


# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
@app.get("/v1/health")
@app.get("/api/v1/health")
@app.get("/api/v8/health")
async def root():
    """å¥åº·æ£€æŸ¥ - æ”¯æŒå¤šç§è·¯å¾„ä»¥å…¼å®¹ä¸åŒå®¢æˆ·ç«¯"""
    return {
        "status": "ok",
        "service": "AI History Manager",
        "version": "1.0.0",
        "timestamp": time.time()
    }

@app.get("/admin/routing/stats")
async def routing_stats():
    """è·å–æ¨¡å‹è·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
    stats = model_router.get_stats()
    return {
        "status": "ok",
        "routing": {
            "enabled": MODEL_ROUTING_CONFIG.get("enabled", True),
            "stats": stats,
            "config": {
                "opus_model": MODEL_ROUTING_CONFIG.get("opus_model"),
                "sonnet_model": MODEL_ROUTING_CONFIG.get("sonnet_model"),
                "total_chars_threshold": MODEL_ROUTING_CONFIG.get("total_chars_threshold"),
                "message_count_threshold": MODEL_ROUTING_CONFIG.get("message_count_threshold"),
            }
        }
    }


@app.post("/admin/routing/reset")
async def reset_routing_stats():
    """é‡ç½®è·¯ç”±ç»Ÿè®¡"""
    model_router.stats = {"opus": 0, "sonnet": 0, "other": 0}
    return {"status": "ok", "message": "Routing stats reset"}


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹ - Anthropic æ ¼å¼"""
    return {
        "data": [
            {"id": "claude-opus-4-5-20251101", "object": "model", "created": 1699900000, "owned_by": "anthropic"},
            {"id": "claude-sonnet-4-5-20250929", "object": "model", "created": 1699900000, "owned_by": "anthropic"},
            {"id": "claude-haiku-4-5-20251001", "object": "model", "created": 1699900000, "owned_by": "anthropic"},
            {"id": "claude-sonnet-4", "object": "model", "created": 1699900000, "owned_by": "anthropic"},
            {"id": "claude-haiku-4", "object": "model", "created": 1699900000, "owned_by": "anthropic"},
            {"id": "claude-opus-4", "object": "model", "created": 1699900000, "owned_by": "anthropic"},
        ],
        "object": "list"
    }


@app.post("/v1/messages/count_tokens")
async def count_tokens(request: Request):
    """Token è®¡æ•°ç«¯ç‚¹ (ç®€åŒ–å®ç°)"""
    try:
        body = await request.json()
        # ç®€å•ä¼°ç®—: çº¦ 4 å­—ç¬¦ = 1 token
        total_chars = 0

        # è®¡ç®— system
        system = body.get("system", "")
        if isinstance(system, str):
            total_chars += len(system)
        elif isinstance(system, list):
            for item in system:
                if isinstance(item, dict) and "text" in item:
                    total_chars += len(item["text"])

        # è®¡ç®— messages
        for msg in body.get("messages", []):
            content = msg.get("content", "")
            if isinstance(content, str):
                total_chars += len(content)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") == "text":
                            total_chars += len(item.get("text", ""))

        # è®¡ç®— tools
        tools = body.get("tools", [])
        for tool in tools:
            total_chars += len(json.dumps(tool))

        estimated_tokens = total_chars // 4

        return {"input_tokens": estimated_tokens}
    except Exception as e:
        return JSONResponse(
            status_code=400,
            content={"error": {"type": "invalid_request_error", "message": str(e)}}
        )


# ==================== Anthropic -> OpenAI è½¬æ¢ ====================

def extract_content_item(item: dict) -> str:
    """æå–å•ä¸ª content item çš„æ–‡æœ¬è¡¨ç¤º

    æ”¯æŒçš„ç±»å‹ï¼š
    - text: çº¯æ–‡æœ¬
    - image: å›¾åƒï¼ˆbase64/URLï¼‰
    - document: æ–‡æ¡£ï¼ˆPDFç­‰ï¼‰
    - file: æ–‡ä»¶
    - tool_use: å·¥å…·è°ƒç”¨
    - tool_result: å·¥å…·ç»“æœ
    - thinking: æ€è€ƒå†…å®¹
    - code_execution_result: ä»£ç æ‰§è¡Œç»“æœ
    - citation: å¼•ç”¨
    - redacted_thinking: éšè—çš„æ€è€ƒ
    """
    item_type = item.get("type", "")

    if item_type == "text":
        return item.get("text", "")

    elif item_type == "image":
        # å›¾åƒå†…å®¹ - æå–æè¿°æˆ–æ ‡è®°
        source = item.get("source", {})
        if source.get("type") == "base64":
            media_type = source.get("media_type", "image")
            return f"[Image: {media_type}]"
        elif source.get("type") == "url":
            url = source.get("url", "")
            return f"[Image: {url[:50]}...]" if len(url) > 50 else f"[Image: {url}]"
        return "[Image]"

    elif item_type == "document":
        # æ–‡æ¡£å†…å®¹ï¼ˆPDFç­‰ï¼‰
        source = item.get("source", {})
        doc_type = source.get("media_type", "document")
        doc_name = item.get("name", "document")

        # æå–æ–‡æ¡£æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if "text" in item:
            return f"[Document: {doc_name}]\n{item.get('text', '')}"

        # å¦‚æœæœ‰ content å­—æ®µï¼ˆæŸäº› API ç‰ˆæœ¬ï¼‰
        if "content" in item:
            doc_content = item.get("content", "")
            if isinstance(doc_content, str):
                return f"[Document: {doc_name}]\n{doc_content}"

        return f"[Document: {doc_name} ({doc_type})]"

    elif item_type == "file":
        # æ–‡ä»¶å†…å®¹
        file_name = item.get("name", item.get("filename", "file"))
        file_type = item.get("media_type", "")
        file_content = item.get("content", "")

        if file_content:
            if isinstance(file_content, str):
                return f"[File: {file_name}]\n{file_content}"
            elif isinstance(file_content, list):
                content_text = "\n".join(
                    extract_content_item(c) if isinstance(c, dict) else str(c)
                    for c in file_content
                )
                return f"[File: {file_name}]\n{content_text}"

        return f"[File: {file_name}]" + (f" ({file_type})" if file_type else "")

    elif item_type == "tool_result":
        # å·¥å…·ç»“æœ
        tool_id = item.get("tool_use_id", "")
        tool_content = item.get("content", "")
        is_error = item.get("is_error", False)

        # å¤„ç† content å¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
        if isinstance(tool_content, list):
            tool_content = "\n".join(
                extract_content_item(c) if isinstance(c, dict) else str(c)
                for c in tool_content
            )
        elif isinstance(tool_content, dict):
            tool_content = extract_content_item(tool_content)

        prefix = "[Tool Error]" if is_error else "[Tool Result]"
        return f"{prefix}\n{tool_content}" if tool_content else prefix

    elif item_type == "thinking":
        # æ€è€ƒå†…å®¹ - ä¸ä½¿ç”¨ <thinking> æ ‡ç­¾ï¼ˆKiro API ä¸æ”¯æŒï¼‰
        # ç›´æ¥è¿”å›æ€è€ƒå†…å®¹ï¼Œæˆ–è€…å®Œå…¨è·³è¿‡
        thinking_text = item.get("thinking", "")
        # è·³è¿‡æ€è€ƒå†…å®¹ï¼Œé¿å… Kiro API æŠ¥é”™
        return ""

    elif item_type == "redacted_thinking":
        # éšè—çš„æ€è€ƒå†…å®¹ï¼ˆè·³è¿‡ï¼‰
        return ""

    elif item_type == "signature":
        # ç­¾åï¼ˆç”¨äºæ‰©å±•æ€è€ƒï¼Œè·³è¿‡ï¼‰
        return ""

    elif item_type == "code_execution_result":
        # ä»£ç æ‰§è¡Œç»“æœ
        output = item.get("output", "")
        return_code = item.get("return_code", 0)
        if return_code != 0:
            return f"[Code Execution Error (exit={return_code})]\n{output}"
        return f"[Code Execution Result]\n{output}" if output else ""

    elif item_type == "citation":
        # å¼•ç”¨
        cited_text = item.get("cited_text", "")
        source_name = item.get("source", {}).get("name", "source")
        return f"[Citation from {source_name}]: {cited_text}" if cited_text else ""

    elif item_type == "video":
        # è§†é¢‘
        source = item.get("source", {})
        return f"[Video: {source.get('url', 'embedded')}]"

    elif item_type == "audio":
        # éŸ³é¢‘
        source = item.get("source", {})
        return f"[Audio: {source.get('url', 'embedded')}]"

    else:
        # æœªçŸ¥ç±»å‹ - å°è¯•æå–æ–‡æœ¬æˆ–è¿”å›ç±»å‹æ ‡è®°
        if "text" in item:
            return item.get("text", "")
        if "content" in item:
            content = item.get("content", "")
            if isinstance(content, str):
                return content
        # è¿”å›ç±»å‹æ ‡è®°è€Œéç©º
        return f"[{item_type}]" if item_type else ""


def clean_system_content(content: str) -> str:
    """æ¸…ç† system æ¶ˆæ¯å†…å®¹

    ç§»é™¤ä¸åº”è¯¥å‡ºç°åœ¨ system prompt ä¸­çš„å†…å®¹ï¼š
    - HTTP header æ ¼å¼çš„å†…å®¹ï¼ˆå¦‚ x-anthropic-billing-headerï¼‰
    - å…¶ä»–å…ƒæ•°æ®
    """
    if not content:
        return content

    lines = content.split('\n')
    cleaned_lines = []

    for line in lines:
        # è·³è¿‡ HTTP header æ ¼å¼çš„è¡Œ (key: value)
        if ':' in line:
            key = line.split(':')[0].strip().lower()
            # è·³è¿‡å·²çŸ¥çš„ header ç±»å‹
            if key.startswith('x-') or key in [
                'content-type', 'authorization', 'user-agent',
                'accept', 'cache-control', 'cookie'
            ]:
                continue
        cleaned_lines.append(line)

    return '\n'.join(cleaned_lines).strip()


def clean_assistant_content(content: str) -> str:
    """æ¸…ç† assistant æ¶ˆæ¯å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    ç§»é™¤æ ¼å¼åŒ–æ ‡è®°ï¼š
    - (no content)
    - [Calling tool: xxx]
    - <thinking>...</thinking> æ ‡ç­¾ï¼ˆKiro API ä¸æ”¯æŒï¼‰

    ä¼˜åŒ–ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
    """
    if not content:
        return content

    # ç§»é™¤ (no content) æ ‡è®°
    content = content.replace("(no content)", "").strip()

    # ä¸å†ç§»é™¤ [Calling tool: xxx] æ ‡è®°ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ ¼å¼æ¥å†…è”å·¥å…·è°ƒç”¨

    # ç§»é™¤ <thinking>...</thinking> æ ‡ç­¾ï¼ˆKiro API ä¸æ”¯æŒï¼‰
    # ä¿ç•™æ ‡ç­¾å†…çš„å†…å®¹ï¼Œä½†ç§»é™¤æ ‡ç­¾æœ¬èº«ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
    content = _RE_THINKING_TAG.sub(r'\1', content)

    # ç§»é™¤æœªé—­åˆçš„ <thinking> æ ‡ç­¾ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
    content = _RE_THINKING_UNCLOSED.sub('', content)
    content = _RE_THINKING_UNOPEN.sub('', content)

    # ç§»é™¤ <redacted_thinking> ç›¸å…³æ ‡ç­¾ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
    content = _RE_REDACTED_THINKING.sub('', content)

    # ç§»é™¤å…¶ä»–å¯èƒ½çš„ Claude ç‰¹æœ‰æ ‡ç­¾ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
    content = _RE_SIGNATURE_TAG.sub('', content)

    return content.strip() if content.strip() else " "


def convert_anthropic_to_openai(anthropic_body: dict) -> dict:
    """å°† Anthropic è¯·æ±‚è½¬æ¢ä¸º OpenAI æ ¼å¼

    å¤„ç† Claude Code å‘é€çš„å®Œæ•´ Anthropic æ ¼å¼è¯·æ±‚ï¼ŒåŒ…æ‹¬ï¼š
    - system æ¶ˆæ¯ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼Œæ”¯æŒç¼“å­˜æ§åˆ¶ï¼‰
    - messages æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ”¯æŒå¤šæ¨¡æ€å†…å®¹ï¼‰
    - tools å·¥å…·å®šä¹‰
    - tool_choice å·¥å…·é€‰æ‹©
    - thinking/extended thinking ç›¸å…³å­—æ®µ
    - å›¾åƒã€æ–‡æ¡£ã€æ–‡ä»¶ç­‰å¤šåª’ä½“å†…å®¹

    åŒæ—¶åŒ…å«æˆªæ–­ä¿æŠ¤å’Œç©ºæ¶ˆæ¯è¿‡æ»¤
    """
    # æˆªæ–­é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒèŠ‚ï¼‰
    MAX_MESSAGES = ANTHROPIC_MAX_MESSAGES
    MAX_TOTAL_CHARS = ANTHROPIC_MAX_TOTAL_CHARS
    MAX_SINGLE_CONTENT = ANTHROPIC_MAX_SINGLE_CONTENT

    messages = []

    # å¤„ç† system æ¶ˆæ¯
    system = anthropic_body.get("system", "")
    if system:
        if isinstance(system, str):
            system_content = clean_system_content(system) if ANTHROPIC_CLEAN_SYSTEM_ENABLED else system
        elif isinstance(system, list):
            # Anthropic å…è®¸ system ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆæ”¯æŒç¼“å­˜æ§åˆ¶ç­‰ï¼‰
            system_parts = []
            for item in system:
                if isinstance(item, dict):
                    extracted = extract_content_item(item)
                    if extracted:
                        system_parts.append(extracted)
                else:
                    system_parts.append(str(item))
            raw_system = "\n".join(filter(None, system_parts))
            system_content = clean_system_content(raw_system) if ANTHROPIC_CLEAN_SYSTEM_ENABLED else raw_system
        else:
            raw_system = str(system)
            system_content = clean_system_content(raw_system) if ANTHROPIC_CLEAN_SYSTEM_ENABLED else raw_system

        if system_content.strip():
            # æˆªæ–­è¿‡é•¿çš„ system æ¶ˆæ¯
            if len(system_content) > MAX_SINGLE_CONTENT:
                system_content = system_content[:MAX_SINGLE_CONTENT] + "\n...[truncated]"
            messages.append({"role": "system", "content": system_content})

    # è·å–åŸå§‹æ¶ˆæ¯å¹¶æˆªæ–­
    raw_messages = anthropic_body.get("messages", [])
    if ANTHROPIC_TRUNCATE_ENABLED and len(raw_messages) > MAX_MESSAGES:
        raw_messages = raw_messages[-MAX_MESSAGES:]

    # è½¬æ¢ messages
    converted_messages = []

    for msg in raw_messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")

        # å¤„ç† content ä¸ºåˆ—è¡¨çš„æƒ…å†µ (å¤šæ¨¡æ€/å·¥å…·è°ƒç”¨)
        if isinstance(content, list):
            # ä½¿ç”¨å†…è”æ–‡æœ¬æ ¼å¼ï¼ˆç½‘å…³ä¸æ”¯æŒ OpenAI tool_callsï¼‰
            text_parts = []

            for item in content:
                if isinstance(item, dict):
                    item_type = item.get("type", "")

                    if item_type == "tool_use":
                        tool_name = item.get("name", "unknown")
                        tool_input = item.get("input", {})
                        input_str = json.dumps(tool_input, ensure_ascii=False)
                        if ANTHROPIC_TRUNCATE_ENABLED and len(input_str) > ANTHROPIC_TOOL_INPUT_MAX_CHARS:
                            input_str = input_str[:ANTHROPIC_TOOL_INPUT_MAX_CHARS] + "...[truncated]"
                        text_parts.append(f"[Calling tool: {tool_name}]\nInput: {input_str}")
                    elif item_type == "tool_result":
                        tool_content = item.get("content", "")
                        is_error = item.get("is_error", False)

                        if isinstance(tool_content, list):
                            parts = []
                            for c in tool_content:
                                if isinstance(c, dict):
                                    if c.get("type") == "text":
                                        parts.append(c.get("text", ""))
                                    else:
                                        extracted = extract_content_item(c)
                                        if extracted:
                                            # Strip potential double prefix from extract_content_item
                                            if extracted.startswith(("[Tool Result]\n", "[Tool Error]\n")):
                                                extracted = extracted.split("\n", 1)[1]
                                            parts.append(extracted)
                                else:
                                    parts.append(str(c))
                            tool_content = "\n".join(filter(None, parts))
                        elif isinstance(tool_content, dict):
                            tool_content = extract_content_item(tool_content)
                            if isinstance(tool_content, str) and tool_content.startswith(("[Tool Result]\n", "[Tool Error]\n")):
                                tool_content = tool_content.split("\n", 1)[1]

                        if not tool_content:
                            tool_content = "Error" if is_error else "OK"

                        prefix = "[Tool Error]" if is_error else "[Tool Result]"
                        if ANTHROPIC_TRUNCATE_ENABLED and len(tool_content) > ANTHROPIC_TOOL_RESULT_MAX_CHARS:
                            tool_content = tool_content[:ANTHROPIC_TOOL_RESULT_MAX_CHARS] + "\n...[truncated]"
                        text_parts.append(f"{prefix}\n{tool_content}")
                    elif item_type == "thinking":
                        pass  # å¿½ç•¥ thinking blocks
                    else:
                        extracted = extract_content_item(item)
                        if extracted:
                            text_parts.append(extracted)
                else:
                    text_parts.append(str(item))

            content = "\n".join(filter(None, text_parts))

            if role == "assistant" and ANTHROPIC_CLEAN_ASSISTANT_ENABLED:
                content = clean_assistant_content(content)

            if ANTHROPIC_TRUNCATE_ENABLED and len(content) > MAX_SINGLE_CONTENT:
                content = content[:MAX_SINGLE_CONTENT] + "\n...[truncated]"

            if content.strip():
                converted_messages.append({
                    "role": role,
                    "content": content
                })
            elif role == "assistant":
                converted_messages.append({
                    "role": "assistant",
                    "content": ANTHROPIC_EMPTY_ASSISTANT_PLACEHOLDER
                })
        else:
            # content æ˜¯å­—ç¬¦ä¸²
            if role == "assistant" and ANTHROPIC_CLEAN_ASSISTANT_ENABLED:
                content = clean_assistant_content(content)

            # æˆªæ–­è¿‡é•¿å†…å®¹
            if ANTHROPIC_TRUNCATE_ENABLED and len(content) > MAX_SINGLE_CONTENT:
                content = content[:MAX_SINGLE_CONTENT] + "\n...[truncated]"

            # è·³è¿‡ç©ºæ¶ˆæ¯
            if content.strip():
                converted_messages.append({
                    "role": role,
                    "content": content
                })

    # åˆå¹¶è¿ç»­åŒè§’è‰²æ¶ˆæ¯ï¼ˆå¯é…ç½®ï¼‰
    if ANTHROPIC_MERGE_SAME_ROLE_ENABLED:
        merged_messages = []
        for msg in converted_messages:
            role = msg.get("role")
            if merged_messages and merged_messages[-1].get("role") == role:
                # åˆå¹¶å†…å®¹
                merged_messages[-1]["content"] += "\n" + msg.get("content", "")
            else:
                merged_messages.append(msg.copy())
        final_messages = merged_messages
    else:
        final_messages = converted_messages

    # æ·»åŠ åˆ°ä¸»æ¶ˆæ¯åˆ—è¡¨
    messages.extend(final_messages)

    # ç¡®ä¿è‡³å°‘æœ‰ä¸€æ¡æ¶ˆæ¯
    if not messages:
        messages.append({"role": "user", "content": "Hello"})

    # ç¡®ä¿æœ€åä¸€æ¡ä¸æ˜¯ system
    if len(messages) == 1 and messages[0]["role"] == "system":
        messages.append({"role": "user", "content": "Hello"})

    # å…³é”®ä¿®å¤ï¼šç¡®ä¿æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ role="tool"
    # Kiro API éœ€è¦æœ€åä¸€æ¡æ˜¯ user æ¶ˆæ¯
    if ANTHROPIC_ENSURE_USER_ENDING and messages and messages[-1].get("role") == "tool":
        messages.append({"role": "user", "content": "Please continue based on the tool results above."})

    # ç¡®ä¿æ¶ˆæ¯ä¸ä»¥ assistant ç»“å°¾ï¼ˆKiro éœ€è¦ user ç»“å°¾ï¼‰
    if ANTHROPIC_ENSURE_USER_ENDING and messages and messages[-1].get("role") == "assistant":
        messages.append({"role": "user", "content": "Please continue."})

    # æ£€æŸ¥æ€»å­—ç¬¦æ•°ï¼Œå¦‚æœè¶…è¿‡åˆ™è¿›ä¸€æ­¥æˆªæ–­
    if ANTHROPIC_TRUNCATE_ENABLED:
        total_chars = sum(len(m.get("content", "")) for m in messages)
        while total_chars > MAX_TOTAL_CHARS and len(messages) > 2:
            if messages[0].get("role") == "system":
                if len(messages) > 2:
                    messages.pop(1)
            else:
                messages.pop(0)
            total_chars = sum(len(m.get("content", "")) for m in messages)

    # æ„å»º OpenAI è¯·æ±‚
    openai_body = {
        "model": anthropic_body.get("model", "claude-sonnet-4"),
        "messages": messages,
        "stream": anthropic_body.get("stream", False),
    }

    # æµå¼å“åº”æ—¶ï¼Œè¯·æ±‚åŒ…å« usage ä¿¡æ¯
    if anthropic_body.get("stream", False):
        openai_body["stream_options"] = {"include_usage": True}

    # è½¬æ¢å‚æ•°
    if "max_tokens" in anthropic_body:
        openai_body["max_tokens"] = anthropic_body["max_tokens"]
    if "temperature" in anthropic_body:
        openai_body["temperature"] = anthropic_body["temperature"]
    if "top_p" in anthropic_body:
        openai_body["top_p"] = anthropic_body["top_p"]
    if "stop_sequences" in anthropic_body:
        openai_body["stop"] = anthropic_body["stop_sequences"]

    # ==================== å·¥å…·å®šä¹‰å¤„ç† ====================
    anthropic_tools = anthropic_body.get("tools", [])
    if anthropic_tools:
        if NATIVE_TOOLS_ENABLED:
            # åŸç”Ÿ OpenAI tools æ ¼å¼ - Kiro ç½‘å…³ç°å·²æ”¯æŒ
            # ä¼˜åŠ¿ï¼šå‡å°‘ token æ¶ˆè€—ã€ç»“æ„åŒ–è¿”å›ã€æ”¯æŒå¹¶è¡Œè°ƒç”¨
            openai_body["tools"] = convert_anthropic_tools_to_openai(anthropic_tools)

            # è½¬æ¢ tool_choice
            if "tool_choice" in anthropic_body:
                openai_tool_choice = convert_anthropic_tool_choice_to_openai(anthropic_body["tool_choice"])
                if openai_tool_choice:
                    openai_body["tool_choice"] = openai_tool_choice

            logger.debug(f"ä½¿ç”¨åŸç”Ÿ tools æ¨¡å¼ï¼Œå·¥å…·æ•°é‡: {len(anthropic_tools)}")
        else:
            # é™çº§æ¨¡å¼ï¼šå°†å·¥å…·å®šä¹‰æ³¨å…¥ç³»ç»Ÿæç¤º
            # æ¨¡å‹é€šè¿‡ [Calling tool: xxx] æ ¼å¼è°ƒç”¨å·¥å…·ï¼Œå“åº”æ—¶è‡ªåŠ¨è§£æ
            tool_instruction = build_tool_instruction(anthropic_tools)
            # æ‰¾åˆ° system æ¶ˆæ¯å¹¶è¿½åŠ å·¥å…·æŒ‡ä»¤
            for msg in openai_body["messages"]:
                if msg.get("role") == "system":
                    msg["content"] = msg["content"] + "\n\n" + tool_instruction
                    break
            else:
                # æ²¡æœ‰ system æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ª
                openai_body["messages"].insert(0, {
                    "role": "system",
                    "content": tool_instruction
                })
            logger.debug(f"ä½¿ç”¨æ–‡æœ¬æ³¨å…¥ tools æ¨¡å¼ï¼Œå·¥å…·æ•°é‡: {len(anthropic_tools)}")

    return openai_body


def convert_anthropic_tools_to_openai(anthropic_tools: list) -> list:
    """
    å°† Anthropic æ ¼å¼çš„ tools è½¬æ¢ä¸º OpenAI æ ¼å¼

    Anthropic æ ¼å¼:
    {
        "name": "tool_name",
        "description": "...",
        "input_schema": { "type": "object", "properties": {...} }
    }

    OpenAI æ ¼å¼:
    {
        "type": "function",
        "function": {
            "name": "tool_name",
            "description": "...",
            "parameters": { "type": "object", "properties": {...} }
        }
    }
    """
    openai_tools = []
    for tool in anthropic_tools:
        openai_tool = {
            "type": "function",
            "function": {
                "name": tool.get("name", ""),
                "description": tool.get("description", ""),
                "parameters": tool.get("input_schema", {})
            }
        }
        openai_tools.append(openai_tool)
    return openai_tools


def convert_anthropic_tool_choice_to_openai(tool_choice) -> Optional[Union[str, dict]]:
    """
    å°† Anthropic æ ¼å¼çš„ tool_choice è½¬æ¢ä¸º OpenAI æ ¼å¼

    Anthropic æ ¼å¼:
    - {"type": "auto"} -> "auto"
    - {"type": "any"} -> "required"
    - {"type": "tool", "name": "xxx"} -> {"type": "function", "function": {"name": "xxx"}}
    """
    if not tool_choice:
        return None

    tc_type = tool_choice.get("type", "")

    if tc_type == "auto":
        return "auto"
    elif tc_type == "any":
        return "required"
    elif tc_type == "tool":
        return {
            "type": "function",
            "function": {"name": tool_choice.get("name", "")}
        }

    return None


def build_tool_instruction(tools: list) -> str:
    """å°† Anthropic tools è½¬æ¢ä¸ºç³»ç»Ÿæç¤ºä¸­çš„å·¥å…·æŒ‡ä»¤æ–‡æœ¬

    è¿™æ ·æ¨¡å‹å³ä½¿æ²¡æœ‰ OpenAI tools å‚æ•°ä¹ŸçŸ¥é“å¦‚ä½•è°ƒç”¨å·¥å…·ã€‚
    """
    lines = [
        "# Tool Call Format",
        "",
        "You have access to the following tools. To call a tool, output EXACTLY this format:",
        "",
        "[Calling tool: tool_name]",
        "Input: {\"param\": \"value\"}",
        "",
        "IMPORTANT RULES:",
        "- You MUST use the exact format above to call tools",
        "- The Input MUST be valid JSON on a single line",
        "- You can call multiple tools in sequence",
        "- After each tool call, you will receive the result as [Tool Result]",
        "- NEVER show tool calls as code blocks or plain text - ALWAYS use [Calling tool: ...] format",
        "",
        "## Available Tools",
        "",
    ]

    for tool in tools:
        name = tool.get("name", "unknown")
        desc = tool.get("description", "")
        schema = tool.get("input_schema", {})

        lines.append(f"### {name}")
        if desc:
            # æˆªæ–­è¿‡é•¿æè¿°
            if len(desc) > TOOL_DESC_MAX_CHARS:
                desc = desc[:TOOL_DESC_MAX_CHARS] + "..."
            lines.append(desc)

        # æ·»åŠ å‚æ•°ä¿¡æ¯
        props = schema.get("properties", {}) or {}
        required = schema.get("required") or []
        if props:
            lines.append("Parameters:")
            for pname, pschema in props.items():
                ptype = pschema.get("type", "any")
                pdesc = pschema.get("description", "")
                req_mark = " (required)" if pname in required else ""
                if pdesc:
                    # æˆªæ–­å‚æ•°æè¿°
                    if len(pdesc) > TOOL_PARAM_DESC_MAX_CHARS:
                        pdesc = pdesc[:TOOL_PARAM_DESC_MAX_CHARS] + "..."
                    lines.append(f"  - {pname}: {ptype}{req_mark} - {pdesc}")
                else:
                    lines.append(f"  - {pname}: {ptype}{req_mark}")
        lines.append("")

    return "\n".join(lines)


def escape_json_string_newlines(json_str: str) -> str:
    """è½¬ä¹‰ JSON å­—ç¬¦ä¸²å€¼ä¸­çš„åŸå§‹æ¢è¡Œç¬¦å’Œæ§åˆ¶å­—ç¬¦

    å½“æ¨¡å‹è¾“å‡ºçš„ JSON åœ¨å­—ç¬¦ä¸²å€¼ä¸­åŒ…å«æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦æ—¶ï¼Œ
    æ ‡å‡† JSON è§£æä¼šå¤±è´¥ã€‚æ­¤å‡½æ•°å°†è¿™äº›æ§åˆ¶å­—ç¬¦æ­£ç¡®è½¬ä¹‰ã€‚
    """
    result = []
    in_string = False
    escape = False
    i = 0

    while i < len(json_str):
        c = json_str[i]

        if escape:
            # æ­£å¸¸çš„è½¬ä¹‰åºåˆ—ï¼Œä¿æŒåŸæ ·
            result.append(c)
            escape = False
            i += 1
            continue

        if c == '\\':
            result.append(c)
            escape = True
            i += 1
            continue

        if c == '"':
            in_string = not in_string
            result.append(c)
            i += 1
            continue

        if in_string:
            # åœ¨å­—ç¬¦ä¸²å†…éƒ¨ï¼Œè½¬ä¹‰æ§åˆ¶å­—ç¬¦
            if c == '\n':
                result.append('\\n')
            elif c == '\r':
                result.append('\\r')
            elif c == '\t':
                result.append('\\t')
            elif ord(c) < 32:
                # å…¶ä»–æ§åˆ¶å­—ç¬¦
                result.append(f'\\u{ord(c):04x}')
            else:
                result.append(c)
        else:
            result.append(c)

        i += 1

    return ''.join(result)


def _try_parse_json(json_str: str, end_pos: int) -> tuple[dict, int]:
    """å°è¯•å¤šç§æ–¹å¼è§£æ JSON å­—ç¬¦ä¸²ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    Args:
        json_str: JSON å­—ç¬¦ä¸²
        end_pos: æˆåŠŸæ—¶è¿”å›çš„ç»“æŸä½ç½®

    Returns:
        (parsed_json, end_position) æˆ–æŠ›å‡ºå¼‚å¸¸

    ä¼˜åŒ–ï¼š
    - å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥è§£ææˆåŠŸåˆ™ç«‹å³è¿”å›
    - ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
    - å‡å°‘ä¸å¿…è¦çš„å­—ç¬¦ä¸²æ“ä½œ
    """
    # å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥è§£æ
    try:
        return json.loads(json_str), end_pos
    except json.JSONDecodeError:
        pass

    # è¿›å…¥ä¿®å¤è·¯å¾„
    return _try_repair_json(json_str, end_pos)


def _try_repair_json(json_str: str, end_pos: int) -> tuple[dict, int]:
    """å°è¯•ä¿®å¤å¹¶è§£æ JSON å­—ç¬¦ä¸²

    ä»…åœ¨ç›´æ¥è§£æå¤±è´¥æ—¶è°ƒç”¨ï¼Œé¿å…ä¸å¿…è¦çš„ä¿®å¤å°è¯•
    """
    # ä¿®å¤ç­–ç•¥ 1: ç§»é™¤å°¾éšé€—å·ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
    try:
        fixed = _RE_TRAILING_COMMA_OBJ.sub('}', json_str)
        fixed = _RE_TRAILING_COMMA_ARR.sub(']', fixed)
        return json.loads(fixed), end_pos
    except json.JSONDecodeError:
        pass

    # ä¿®å¤ç­–ç•¥ 2: è½¬ä¹‰å­—ç¬¦ä¸²å†…çš„æ§åˆ¶å­—ç¬¦
    try:
        fixed = escape_json_string_newlines(json_str)
        return json.loads(fixed), end_pos
    except json.JSONDecodeError:
        pass

    # ä¿®å¤ç­–ç•¥ 3: ç»„åˆä¿®å¤
    try:
        fixed = escape_json_string_newlines(json_str)
        fixed = _RE_TRAILING_COMMA_OBJ.sub('}', fixed)
        fixed = _RE_TRAILING_COMMA_ARR.sub(']', fixed)
        return json.loads(fixed), end_pos
    except json.JSONDecodeError:
        pass

    # ä¿®å¤ç­–ç•¥ 4: å¤„ç†æˆªæ–­çš„å­—ç¬¦ä¸²å€¼
    try:
        quote_count = json_str.count('"') - json_str.count('\\"')
        if quote_count % 2 == 1:
            fixed = json_str.rstrip()
            if not fixed.endswith('"'):
                fixed = fixed + '"'
            open_braces = fixed.count('{') - fixed.count('}')
            if open_braces > 0:
                fixed = fixed + '}' * open_braces
            return json.loads(fixed), end_pos
    except json.JSONDecodeError:
        pass

    # ä¿®å¤ç­–ç•¥ 5: æå–æœ‰æ•ˆçš„ JSON å­é›†
    try:
        decoder = json.JSONDecoder()
        obj, idx = decoder.raw_decode(json_str)
        return obj, end_pos
    except json.JSONDecodeError:
        pass

    raise json.JSONDecodeError("Failed to parse JSON after all recovery attempts", json_str, 0)


def extract_json_from_position(text: str, start: int) -> tuple[dict, int]:
    """ä»æŒ‡å®šä½ç½®æå– JSON å¯¹è±¡ï¼Œæ”¯æŒä»»æ„åµŒå¥—æ·±åº¦å¹¶å¤„ç† Markdown åŒ…è£…

    Args:
        text: æºæ–‡æœ¬
        start: å¼€å§‹æœç´¢çš„ä½ç½®

    Returns:
        (parsed_json, end_position) æˆ–æŠ›å‡ºå¼‚å¸¸

    ä¼˜åŒ–ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
    """
    # è·³è¿‡ç©ºç™½æ‰¾åˆ° '{' æˆ– Markdown ä»£ç å—æ ‡è®°
    pos = start
    while pos < len(text) and text[pos] in ' \t\n\r':
        pos += 1

    # æ£€æŸ¥æ˜¯å¦ä»¥ ```json æˆ– ``` å¼€å¤´ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
    markdown_match = _RE_MARKDOWN_START.match(text[pos:])
    is_markdown_wrapped = False
    if markdown_match:
        is_markdown_wrapped = True
        pos += markdown_match.end()
        # è·³è¿‡ markdown æ ‡è®°åçš„ç©ºç™½
        while pos < len(text) and text[pos] in ' \t\n\r':
            pos += 1

    if pos >= len(text) or text[pos] != '{':
        raise ValueError(f"No JSON object found at position {start}")

    # ä½¿ç”¨æ‹¬å·è®¡æ•°æ¥æ‰¾åˆ°åŒ¹é…çš„ '}'
    depth = 0
    in_string = False
    escape = False
    json_start = pos

    while pos < len(text):
        c = text[pos]

        if escape:
            escape = False
            pos += 1
            continue

        if c == '\\' and in_string:
            escape = True
            pos += 1
            continue

        if c == '"' and not escape:
            in_string = not in_string
            pos += 1
            continue

        if in_string:
            pos += 1
            continue

        if c == '{':
            depth += 1
        elif c == '}':
            depth -= 1
            if depth == 0:
                json_str = text[json_start:pos + 1]
                parsed_json, _ = _try_parse_json(json_str, pos + 1)

                # å¦‚æœæ˜¯ markdown åŒ…è£…çš„ï¼Œè¿˜éœ€è¦è·³è¿‡ç»“å°¾æ ‡è®°ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
                end_pos = pos + 1
                if is_markdown_wrapped:
                    remaining = text[end_pos:]
                    end_match = _RE_MARKDOWN_END.search(remaining)
                    if end_match:
                        end_pos += end_match.end()

                return parsed_json, end_pos

        pos += 1

    # JSON ä¸å®Œæ•´ - å°è¯•æ™ºèƒ½ä¿®å¤
    incomplete_json = text[json_start:]
    
    # ç­–ç•¥ 1: å°è¯•å¼ºåˆ¶é—­åˆ JSON
    if depth > 0:
        # è¡¥å…¨ç¼ºå¤±çš„å¼•å·å’Œæ‹¬å·
        repaired_json = incomplete_json
        if in_string:
            repaired_json += '"'
        repaired_json += '}' * depth
        
        try:
            parsed_json, _ = _try_parse_json(repaired_json, len(text))
            logger.warning(f"JSON was incomplete (depth={depth}), auto-repaired successfully")
            return parsed_json, len(text)
        except Exception:
            pass

    # ç­–ç•¥ 2: æŸ¥æ‰¾æœ€åä¸€ä¸ªå¯èƒ½çš„æœ‰æ•ˆ JSON
    for i in range(len(text) - 1, json_start, -1):
        if text[i] == '}':
            try:
                candidate = text[json_start:i+1]
                parsed_json, _ = _try_parse_json(candidate, i + 1)
                return parsed_json, i + 1
            except Exception:
                continue

    raise ValueError("Incomplete or malformed JSON object")


def iter_text_chunks(text: str, chunk_size: int):
    """å°†æ–‡æœ¬åˆ†å—ï¼Œç”¨äºæµå¼è¾“å‡º"""
    if chunk_size <= 0:
        yield text
        return
    for i in range(0, len(text), chunk_size):
        yield text[i:i + chunk_size]


def split_thinking_blocks(text: str) -> list[dict]:
    """å°†æ–‡æœ¬æŒ‰ <thinking> æ ‡ç­¾æ‹†åˆ†ä¸º text/thinking blocks"""
    import re

    if not text:
        return []

    lower = text.lower()
    open_pos = lower.rfind("<thinking>")
    close_pos = lower.rfind("</thinking>")
    if open_pos != -1 and (close_pos == -1 or close_pos < open_pos):
        prefix = text[:open_pos]
        thinking = text[open_pos + len("<thinking>"):]
        blocks = []
        if prefix and prefix.strip():
            blocks.append({"type": "text", "text": prefix})
        if thinking and thinking.strip():
            blocks.append({"type": "thinking", "thinking": thinking})
        return blocks

    blocks = []
    pattern = re.compile(r"<thinking>(.*?)</thinking>", re.IGNORECASE | re.DOTALL)
    last_end = 0
    for match in pattern.finditer(text):
        if match.start() > last_end:
            prefix = text[last_end:match.start()]
            if prefix and prefix.strip():
                blocks.append({"type": "text", "text": prefix})
        thinking_text = match.group(1)
        if thinking_text and thinking_text.strip():
            blocks.append({"type": "thinking", "thinking": thinking_text})
        last_end = match.end()

    if last_end < len(text):
        suffix = text[last_end:]
        if suffix and suffix.strip():
            blocks.append({"type": "text", "text": suffix})

    return blocks


def expand_thinking_blocks(blocks: list[dict]) -> list[dict]:
    """å°† text block å†…çš„ thinking æ ‡ç­¾å±•å¼€ä¸ºç‹¬ç«‹ block"""
    expanded = []
    for block in blocks:
        if block.get("type") == "text":
            text_value = block.get("text", "")
            split_blocks = split_thinking_blocks(text_value)
            expanded.extend(split_blocks or [])
        else:
            expanded.append(block)
    return expanded


def tool_calls_to_blocks(tool_calls: list) -> list[dict]:
    """å°† OpenAI tool_calls è½¬æ¢ä¸º Anthropic tool_use blocks"""
    blocks = []
    for tc in tool_calls or []:
        func = tc.get("function", {}) or {}
        name = func.get("name") or tc.get("name") or "unknown"
        args_str = func.get("arguments") or tc.get("arguments") or ""
        tool_id = tc.get("id") or f"toolu_{uuid.uuid4().hex[:12]}"

        if not args_str:
            parsed_input = {}
        else:
            try:
                parsed_input = json.loads(args_str)
            except json.JSONDecodeError:
                try:
                    parsed_input = _try_parse_json(args_str, len(args_str))[0]
                except Exception as e:
                    parsed_input = {"_raw": args_str, "_parse_error": str(e)}

        blocks.append({
            "type": "tool_use",
            "id": tool_id,
            "name": name,
            "input": parsed_input,
        })

    return blocks


def parse_xml_tool_params(xml_content: str) -> dict:
    """è§£æ XML æ ¼å¼çš„å·¥å…·å‚æ•°

    ä¾‹å¦‚: <path>/etc/hostname</path> -> {"path": "/etc/hostname"}
    """
    params = {}
    for match in _RE_XML_PARAM.finditer(xml_content):
        param_name = match.group(1)
        param_value = match.group(2).strip()
        # å°è¯•è§£æ JSON å€¼ï¼ˆæ”¯æŒåµŒå¥—å¯¹è±¡ï¼‰
        try:
            params[param_name] = json.loads(param_value)
        except (json.JSONDecodeError, ValueError):
            params[param_name] = param_value
    return params


def parse_xml_tool_blocks(text: str) -> list[dict]:
    """è§£æ XML æ ¼å¼çš„å·¥å…·è°ƒç”¨ï¼ˆKiro è¿”å›æ ¼å¼ï¼‰

    æ£€æµ‹æ ¼å¼:
    <ToolName>
    <param1>value1</param1>
    <param2>value2</param2>
    </ToolName>

    è¿”å›ä¿æŒé¡ºåºçš„ blocks åˆ—è¡¨ï¼ŒåŒ…å« text å’Œ tool_use ç±»å‹
    """
    blocks = []
    last_end = 0

    for match in _RE_XML_TOOL_CALL.finditer(text):
        # æå–å·¥å…·è°ƒç”¨å‰çš„æ–‡æœ¬
        before_text = text[last_end:match.start()]
        if before_text and before_text.strip():
            blocks.append({"type": "text", "text": before_text})

        tool_name = match.group(1)
        xml_content = match.group(2)

        # è§£æ XML å‚æ•°
        params = parse_xml_tool_params(xml_content)

        blocks.append({
            "type": "tool_use",
            "id": f"toolu_{uuid.uuid4().hex[:12]}",
            "name": tool_name,
            "input": params,
        })

        last_end = match.end()

    # æ·»åŠ å‰©ä½™æ–‡æœ¬
    if last_end < len(text):
        remaining = text[last_end:]
        if remaining and remaining.strip():
            blocks.append({"type": "text", "text": remaining})

    return blocks


def parse_inline_tool_blocks(text: str) -> list[dict]:
    """è§£æå†…è”å·¥å…·è°ƒç”¨ï¼Œä¿ç•™æ–‡æœ¬ä¸å·¥å…·è°ƒç”¨é¡ºåºï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    ä¼˜åŒ–ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
    """
    blocks = []
    last_end = 0
    pos = 0

    while pos < len(text):
        # ä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™åŒ¹é… [Calling tool: name]
        match = _RE_TOOL_CALL.search(text[pos:])
        if not match:
            break

        match_start = pos + match.start()
        match_end = pos + match.end()

        # æå–å·¥å…·è°ƒç”¨å‰çš„æ–‡æœ¬
        before_text = text[last_end:match_start]
        if before_text and before_text.strip():
            blocks.append({"type": "text", "text": before_text})

        tool_name = match.group(1).strip()
        after_match = text[match_end:]

        # æŸ¥æ‰¾ Input: æ ‡è®°ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
        input_match = _RE_INPUT_PREFIX.match(after_match)

        if input_match:
            json_start_pos = match_end + input_match.end()
            try:
                # ä½¿ç”¨æ”¹è¿›çš„ extract_json_from_position è¿›è¡Œè§£æ
                input_json, json_end_pos = extract_json_from_position(text, json_start_pos)
                blocks.append({
                    "type": "tool_use",
                    "id": f"toolu_{uuid.uuid4().hex[:12]}",
                    "name": tool_name,
                    "input": input_json,
                })
                last_end = json_end_pos
                pos = json_end_pos
                continue
            except Exception as e:
                logger.warning(f"JSON parse failed for tool {tool_name} at pos {json_start_pos}: {e}")

                # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœ extract_json_from_position å¤±è´¥ï¼Œå°è¯•å®šä½ä¸‹ä¸€ä¸ªæ ‡è®°å¹¶æå–ä¸­é—´æ–‡æœ¬
                # æ ‡è®°åŒ…æ‹¬ï¼šä¸‹ä¸€ä¸ªå·¥å…·è°ƒç”¨ã€å·¥å…·ç»“æœã€æˆ–è€…æ–‡æœ¬ç»“å°¾ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
                next_marker = _RE_NEXT_MARKER.search(after_match[input_match.end():])
                if next_marker:
                    raw_text = after_match[input_match.end():input_match.end() + next_marker.start()].strip()
                else:
                    raw_text = after_match[input_match.end():].strip()

                # å°è¯•å†æ¬¡è§£æè¿™ä¸ªç‰‡æ®µ
                try:
                    input_json, _ = _try_parse_json(raw_text, 0)
                    blocks.append({
                        "type": "tool_use",
                        "id": f"toolu_{uuid.uuid4().hex[:12]}",
                        "name": tool_name,
                        "input": input_json,
                    })
                    last_end = match_end + input_match.end() + len(raw_text)
                    pos = last_end
                    continue
                except Exception as e:
                    # è®°å½•åŸå§‹æ–‡æœ¬ä»¥ä¾¿è°ƒè¯•
                    blocks.append({
                        "type": "tool_use",
                        "id": f"toolu_{uuid.uuid4().hex[:12]}",
                        "name": tool_name,
                        "input": {"_raw": raw_text[:2000], "_parse_error": str(e)},
                    })
                    last_end = match_end + input_match.end() + len(raw_text)
                    pos = last_end
                    continue

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° Input:ï¼Œæˆ–è€…æ ¼å¼å®Œå…¨ä¸åŒ¹é…ï¼Œå°†æ ‡è®°æœ¬èº«ä½œä¸ºæ–‡æœ¬ä¿ç•™
        marker_text = text[match_start:match_end]
        if marker_text and marker_text.strip():
            blocks.append({"type": "text", "text": marker_text})
        last_end = match_end
        pos = match_end

    # æ·»åŠ å‰©ä½™æ–‡æœ¬
    if last_end < len(text):
        remaining = text[last_end:]
        if remaining and remaining.strip():
            blocks.append({"type": "text", "text": remaining})

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° [Calling tool: ...] æ ¼å¼çš„å·¥å…·è°ƒç”¨ï¼Œ
    # å°è¯•è§£æ XML æ ¼å¼çš„å·¥å…·è°ƒç”¨ï¼ˆKiro è¿”å›æ ¼å¼ï¼‰
    has_tool_use = any(b.get("type") == "tool_use" for b in blocks)
    if not has_tool_use and _RE_XML_TOOL_CALL.search(text):
        logger.debug("No [Calling tool:] format found, trying XML format")
        return parse_xml_tool_blocks(text)

    return blocks


def parse_inline_tool_calls(text: str) -> tuple[list, str]:
    """è§£æå†…è”çš„å·¥å…·è°ƒç”¨æ–‡æœ¬ï¼Œè½¬æ¢ä¸º Anthropic tool_use content blocks

    æ£€æµ‹æ ¼å¼ (æ”¯æŒå¤šç§å˜ä½“):
    [Calling tool: tool_name]
    Input: {"arg": "value"}

    æˆ–è€…å¸¦ç¼©è¿›:
    [Calling tool: tool_name]
      Input: {"arg": "value"}

    Returns:
        (tool_use_blocks, remaining_text)
    """
    import re

    blocks = parse_inline_tool_blocks(text)
    tool_uses = [b for b in blocks if b.get("type") == "tool_use"]
    remaining_parts = []
    for block in blocks:
        if block.get("type") == "text":
            text_part = block.get("text", "").strip()
            if text_part:
                remaining_parts.append(text_part)
    remaining_text = "\n".join(remaining_parts)
    return tool_uses, remaining_text


# ==================== æ™ºèƒ½æ¥ç»­æœºåˆ¶ ====================

class TruncationInfo:
    """æˆªæ–­ä¿¡æ¯å°è£…ç±»"""
    def __init__(self):
        self.is_truncated = False
        self.reason = None
        self.truncated_text = ""
        self.valid_tool_uses = []
        self.failed_tool_uses = []
        self.stream_completed = False
        self.finish_reason = "end_turn"

    def __repr__(self):
        return f"TruncationInfo(truncated={self.is_truncated}, reason={self.reason}, valid_tools={len(self.valid_tool_uses)}, failed_tools={len(self.failed_tool_uses)})"


def detect_truncation(full_text: str, stream_completed: bool, finish_reason: str, request_id: str) -> TruncationInfo:
    """æ£€æµ‹å“åº”æ˜¯å¦è¢«æˆªæ–­ï¼Œè¿”å›è¯¦ç»†çš„æˆªæ–­ä¿¡æ¯

    æ£€æµ‹ç­–ç•¥ï¼š
    1. æµæœªæ­£å¸¸å®Œæˆï¼ˆEOF/è¿æ¥ä¸­æ–­ï¼‰
    2. finish_reason æ˜¯ max_tokens æˆ– length
    3. å·¥å…·è°ƒç”¨è§£æå¤±è´¥
    """
    info = TruncationInfo()
    info.truncated_text = full_text
    info.stream_completed = stream_completed
    info.finish_reason = finish_reason

    # æ£€æµ‹1: æµæœªæ­£å¸¸å®Œæˆ
    if not stream_completed:
        info.is_truncated = True
        info.reason = "stream_interrupted"
        logger.warning(f"[{request_id}] æˆªæ–­æ£€æµ‹: æµæœªæ­£å¸¸å®Œæˆ")

    # æ£€æµ‹2: finish_reason è¡¨ç¤ºè¾¾åˆ°ä¸Šé™
    if finish_reason in ("max_tokens", "length"):
        info.is_truncated = True
        info.reason = "max_tokens_reached"
        logger.warning(f"[{request_id}] æˆªæ–­æ£€æµ‹: finish_reason={finish_reason}")

    # è§£æå·¥å…·è°ƒç”¨
    tool_uses, remaining_text = parse_inline_tool_calls(full_text)

    # æ£€æµ‹4: æ£€æŸ¥è§£æç»“æœä¸­æ˜¯å¦æœ‰é”™è¯¯
    parse_error_should_truncate = (not stream_completed) or (finish_reason in ("max_tokens", "length"))
    for tu in tool_uses:
        inp = tu.get("input", {})
        if isinstance(inp, dict) and ("_parse_error" in inp or "_raw" in inp):
            info.failed_tool_uses.append(tu)
            if parse_error_should_truncate and not info.is_truncated:
                info.is_truncated = True
                info.reason = f"tool_parse_error in {tu.get('name', 'unknown')}"
                logger.warning(f"[{request_id}] æˆªæ–­æ£€æµ‹: å·¥å…·è§£æå¤±è´¥ - {tu.get('name')}")
            elif not parse_error_should_truncate:
                logger.warning(f"[{request_id}] å·¥å…·è§£æå¤±è´¥ä½†æµå·²å®Œæˆï¼Œè·³è¿‡ç»­ä¼ : {tu.get('name')}")
        else:
            info.valid_tool_uses.append(tu)

    return info


# ç»­ä¼ è¯·æ±‚éªŒè¯é…ç½®
CONTINUATION_VALIDATION = {
    # æœ€å°æœ‰æ•ˆæ–‡æœ¬é•¿åº¦ï¼ˆä½äºæ­¤å€¼ä¸è¿›è¡Œç»­ä¼ ï¼‰
    "min_text_length": 10,
    # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°ï¼ˆè¶…è¿‡ååœæ­¢ç»­ä¼ ï¼‰
    "max_consecutive_failures": 3,
    # ç©ºå“åº”æ—¶çš„é™çº§ç­–ç•¥
    "empty_response_action": "skip",  # skip | retry_with_lower_tokens | error
}


def validate_continuation_text(truncated_text: str, request_id: str) -> tuple[bool, str]:
    """éªŒè¯æˆªæ–­æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆï¼Œå†³å®šæ˜¯å¦åº”è¯¥ç»­ä¼ 

    Returns:
        (is_valid, reason)
    """
    config = CONTINUATION_VALIDATION
    min_length = config.get("min_text_length", 10)

    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæˆ–è¿‡çŸ­
    if not truncated_text:
        return False, "æˆªæ–­æ–‡æœ¬ä¸ºç©º"

    stripped_text = truncated_text.strip()
    if len(stripped_text) < min_length:
        return False, f"æˆªæ–­æ–‡æœ¬è¿‡çŸ­ ({len(stripped_text)} < {min_length})"

    # æ£€æŸ¥æ˜¯å¦åªåŒ…å«é”™è¯¯ä¿¡æ¯
    error_markers = ["[ä¸Šæ¸¸æœåŠ¡é”™è¯¯]", "[Tool Error]", "Error:", "error:"]
    for marker in error_markers:
        if stripped_text.startswith(marker):
            return False, f"æˆªæ–­æ–‡æœ¬æ˜¯é”™è¯¯ä¿¡æ¯: {marker}"

    return True, "æœ‰æ•ˆ"


def build_continuation_request(
    original_messages: list,
    truncated_text: str,
    original_body: dict,
    continuation_count: int,
    request_id: str
) -> tuple[dict | None, bool, str]:
    """æ„å»ºç»­ä¼ è¯·æ±‚ï¼ˆå¢å¼ºç‰ˆï¼Œå¸¦éªŒè¯ï¼‰

    ç­–ç•¥ï¼š
    1. éªŒè¯æˆªæ–­æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
    2. ä¿ç•™åŸå§‹æ¶ˆæ¯å†å²
    3. æ·»åŠ æˆªæ–­çš„ assistant å“åº”
    4. æ·»åŠ ç»­ä¼ æç¤ºä½œä¸ºæ–°çš„ user æ¶ˆæ¯

    Returns:
        (request_body, should_continue, reason)
        - request_body: ç»­ä¼ è¯·æ±‚ä½“ï¼Œå¦‚æœä¸åº”ç»­ä¼ åˆ™ä¸º None
        - should_continue: æ˜¯å¦åº”è¯¥ç»§ç»­ç»­ä¼ 
        - reason: å†³ç­–åŸå› 
    """
    config = CONTINUATION_CONFIG

    # ==================== å…³é”®ä¿®å¤ï¼šéªŒè¯æˆªæ–­æ–‡æœ¬ ====================
    is_valid, validation_reason = validate_continuation_text(truncated_text, request_id)
    if not is_valid:
        logger.warning(f"[{request_id}] ç»­ä¼ éªŒè¯å¤±è´¥: {validation_reason}ï¼Œåœæ­¢ç»­ä¼ ")
        return None, False, validation_reason

    # è·å–æˆªæ–­ç»“å°¾ï¼ˆç”¨äºç»­ä¼ æç¤ºï¼‰
    ending_chars = config.get("truncated_ending_chars", 500)
    truncated_ending = truncated_text[-ending_chars:] if len(truncated_text) > ending_chars else truncated_text

    # æ„å»ºç»­ä¼ æç¤º
    continuation_prompt = config.get("continuation_prompt", "").format(
        truncated_ending=truncated_ending
    )

    # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
    new_messages = list(original_messages)  # å¤åˆ¶åŸå§‹æ¶ˆæ¯

    # æ·»åŠ æˆªæ–­çš„ assistant å“åº”
    new_messages.append({
        "role": "assistant",
        "content": truncated_text
    })

    # æ·»åŠ ç»­ä¼ æç¤º
    new_messages.append({
        "role": "user",
        "content": continuation_prompt
    })

    # æ„å»ºæ–°çš„è¯·æ±‚ä½“
    new_body = dict(original_body)
    new_body["messages"] = new_messages

    # ä½¿ç”¨ç»­ä¼ ä¸“ç”¨çš„ max_tokens
    new_body["max_tokens"] = config.get("continuation_max_tokens", 8192)

    logger.info(f"[{request_id}] æ„å»ºç»­ä¼ è¯·æ±‚ #{continuation_count + 1}: "
                f"åŸå§‹æ¶ˆæ¯={len(original_messages)}, æ–°æ¶ˆæ¯={len(new_messages)}, "
                f"æˆªæ–­æ–‡æœ¬é•¿åº¦={len(truncated_text)}, æˆªæ–­ç»“å°¾é¢„è§ˆ={truncated_ending[:100]}...")

    return new_body, True, "éªŒè¯é€šè¿‡"


def merge_responses(original_text: str, continuation_text: str, request_id: str) -> str:
    """åˆå¹¶åŸå§‹å“åº”å’Œç»­ä¼ å“åº”ï¼Œå¢å¼º JSON è¾¹ç•Œå¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    ç­–ç•¥ï¼š
    1. æ£€æµ‹ç»­ä¼ å“åº”æ˜¯å¦æœ‰é‡å¤å†…å®¹
    2. æ™ºèƒ½æ‹¼æ¥ï¼Œç‰¹åˆ«å¤„ç† JSON æˆªæ–­ç‚¹
    3. ä¿®å¤å¯èƒ½å‡ºç°çš„è½¬ä¹‰å†²çª

    ä¼˜åŒ–ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
    """
    if not continuation_text:
        return original_text

    # æ¸…ç†ç»­ä¼ å“åº”å¼€å¤´å¯èƒ½çš„é‡å¤å†…å®¹æˆ–æç¤º
    continuation_clean = continuation_text.lstrip()

    # ç§»é™¤æ¨¡å‹å¯èƒ½æ·»åŠ çš„ç»­ä¼ å¼•å¯¼è¯ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
    for pattern in _RE_CONTINUATION_INTRO:
        match = pattern.match(continuation_clean)
        if match:
            continuation_clean = continuation_clean[match.end():].lstrip()

    # æ£€æŸ¥é‡å 
    overlap_check_len = min(2000, len(original_text), len(continuation_clean))
    if overlap_check_len > 0:
        original_ending = original_text[-overlap_check_len:]
        for i in range(overlap_check_len, 0, -1):
            if continuation_clean.startswith(original_ending[-i:]):
                # å‘ç°é‡å ï¼Œå‰¥ç¦»é‡å¤éƒ¨åˆ†
                continuation_clean = continuation_clean[i:]
                logger.info(f"[{request_id}] Merge: stripped {i} chars overlap")
                break

    # æ™ºèƒ½æ‹¼æ¥
    # å¦‚æœåŸå§‹æ–‡æœ¬ä»¥åæ–œæ ç»“å°¾ï¼Œå¯èƒ½æ­£åœ¨è½¬ä¹‰å­—ç¬¦
    if original_text.endswith('\\') and not original_text.endswith('\\\\'):
        # è¿™æ˜¯ä¸€ä¸ªæœªå®Œæˆçš„è½¬ä¹‰åºåˆ—
        merged = original_text + continuation_clean
    elif original_text.rstrip().endswith(('"', '{', '[', ',', ':', ' ')):
        # å¤„äº JSON ç»“æ„æˆ–å€¼ä¸­é—´ï¼Œç›´æ¥æ‹¼æ¥
        merged = original_text + continuation_clean
    else:
        # å…¶ä»–æƒ…å†µï¼Œå°è¯•å¹³æ»‘è¿‡æ¸¡
        merged = original_text + continuation_clean

    logger.info(f"[{request_id}] Combined response: orig={len(original_text)}, cont={len(continuation_text)} -> final={len(merged)}")
    return merged


async def fetch_with_continuation(
    openai_body: dict,
    headers: dict,
    request_id: str,
    model: str,
) -> tuple[str, str, bool, dict, list]:
    """å¸¦æ¥ç»­æœºåˆ¶çš„è¯·æ±‚è·å–

    Returns:
        (full_text, finish_reason, stream_completed, usage_info, tool_calls)
    """
    config = CONTINUATION_CONFIG
    max_continuations = config.get("max_continuations", 3)

    accumulated_text = ""
    continuation_count = 0
    consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°ï¼ˆç”¨äºæ™ºèƒ½åœæ­¢ï¼‰
    final_finish_reason = "end_turn"
    final_stream_completed = False
    total_input_tokens = 0
    total_output_tokens = 0
    aggregated_tool_calls = []

    current_body = dict(openai_body)
    original_messages = list(openai_body.get("messages", []))

    while continuation_count <= max_continuations:
        # å‘èµ·è¯·æ±‚
        text, finish_reason, stream_completed, usage, tool_calls = await _fetch_single_stream(
            current_body, headers, request_id, continuation_count
        )

        # ç´¯ç§¯ token è®¡æ•°
        total_input_tokens += usage.get("input_tokens", 0)
        total_output_tokens += usage.get("output_tokens", 0)

        # åˆå¹¶å“åº”
        if continuation_count == 0:
            accumulated_text = text
        else:
            accumulated_text = merge_responses(accumulated_text, text, request_id)

        # ==================== å¹»è§‰æ£€æµ‹ ====================
        # æ£€æµ‹ AI æ˜¯å¦ç”Ÿæˆäº†è™šå‡çš„å·¥å…·ç»“æœ
        has_hallucination, cleaned_text, hallucination_reason = detect_hallucinated_tool_result(
            accumulated_text, request_id
        )
        if has_hallucination:
            logger.warning(f"[{request_id}] æ£€æµ‹åˆ°å¹»è§‰ï¼Œæ¸…ç†åç»§ç»­: {hallucination_reason}")
            accumulated_text = cleaned_text
            # å¹»è§‰é€šå¸¸æ„å‘³ç€æ¨¡å‹åœ¨ç­‰å¾…å·¥å…·ç»“æœæ—¶äº§ç”Ÿäº†é”™è¯¯è¾“å‡º
            # æ¸…ç†ååº”è¯¥åœæ­¢ç»­ä¼ ï¼Œè®©ç³»ç»Ÿæ­£å¸¸å¤„ç†å·¥å…·è°ƒç”¨
            final_finish_reason = "end_turn"
            final_stream_completed = True
            break

        if tool_calls:
            aggregated_tool_calls.extend(tool_calls)

        # ==================== å¢å¼ºé”™è¯¯å¤„ç† ====================
        # å…³é”®ï¼šå¦‚æœä¸Šæ¸¸è¿”å›é”™è¯¯ï¼Œä¸è¦ç»­ä¼ 
        if finish_reason in ("error", "timeout"):
            logger.warning(f"[{request_id}] ä¸Šæ¸¸è¿”å›é”™è¯¯ ({finish_reason})ï¼Œåœæ­¢ç»­ä¼ ")
            final_finish_reason = "end_turn"  # è¿”å› end_turn é¿å…è§¦å‘ CLI é”™è¯¯
            final_stream_completed = True
            break

        # æ£€æµ‹æœ¬æ¬¡è¯·æ±‚æ˜¯å¦è·å¾—äº†æœ‰æ•ˆå†…å®¹
        current_text_len = len(text.strip()) if text else 0
        if current_text_len == 0 and continuation_count > 0:
            # ç»­ä¼ è¯·æ±‚è¿”å›ç©ºå†…å®¹ï¼Œå¢åŠ å¤±è´¥è®¡æ•°
            consecutive_failures += 1
            logger.warning(f"[{request_id}] ç»­ä¼ è¯·æ±‚ #{continuation_count} è¿”å›ç©ºå†…å®¹ï¼Œè¿ç»­å¤±è´¥={consecutive_failures}")

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°
            max_failures = CONTINUATION_VALIDATION.get("max_consecutive_failures", 3)
            if consecutive_failures >= max_failures:
                logger.error(f"[{request_id}] è¿ç»­ {consecutive_failures} æ¬¡ç»­ä¼ å¤±è´¥ï¼Œåœæ­¢ç»­ä¼ ")
                final_finish_reason = "end_turn"
                final_stream_completed = True
                break
        else:
            # è·å¾—äº†æœ‰æ•ˆå†…å®¹ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
            consecutive_failures = 0

        # æ£€æµ‹æ˜¯å¦éœ€è¦ç»­ä¼ 
        truncation_info = detect_truncation(accumulated_text, stream_completed, finish_reason, request_id)

        if not truncation_info.is_truncated:
            # æ²¡æœ‰æˆªæ–­ï¼Œæ­£å¸¸å®Œæˆ
            final_finish_reason = finish_reason
            final_stream_completed = True
            logger.info(f"[{request_id}] è¯·æ±‚å®Œæˆ: æ— æˆªæ–­, æ€»ç»­ä¼ æ¬¡æ•°={continuation_count}")
            break

        # ==================== æ™ºèƒ½ç»­ä¼ å†³ç­– ====================
        should_continue = False
        triggers = config.get("triggers", {})

        # åŸºäºè§¦å‘æ¡ä»¶åˆ¤æ–­
        if truncation_info.reason == "stream_interrupted" and triggers.get("stream_interrupted", True):
            should_continue = True
        elif truncation_info.reason == "max_tokens_reached" and triggers.get("max_tokens_reached", True):
            should_continue = True
        elif "incomplete_json" in str(truncation_info.reason) and triggers.get("incomplete_tool_json", True):
            should_continue = True
        elif "tool_parse_error" in str(truncation_info.reason) and triggers.get("parse_error", True):
            should_continue = True

        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœç´¯ç§¯æ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œä¸åº”ç»­ä¼ 
        accumulated_len = len(accumulated_text.strip()) if accumulated_text else 0
        min_text_for_continuation = CONTINUATION_VALIDATION.get("min_text_length", 10)
        if accumulated_len < min_text_for_continuation:
            logger.warning(f"[{request_id}] ç´¯ç§¯æ–‡æœ¬è¿‡çŸ­ ({accumulated_len} < {min_text_for_continuation})ï¼Œåœæ­¢ç»­ä¼ ")
            should_continue = False

        if not should_continue:
            logger.info(f"[{request_id}] æˆªæ–­ä½†ä¸ç»­ä¼ : reason={truncation_info.reason}, accumulated_len={accumulated_len}")
            final_finish_reason = finish_reason
            final_stream_completed = stream_completed
            break

        if continuation_count >= max_continuations:
            logger.warning(f"[{request_id}] è¾¾åˆ°æœ€å¤§ç»­ä¼ æ¬¡æ•° {max_continuations}ï¼Œåœæ­¢ç»­ä¼ ")
            final_finish_reason = "end_turn"  # ä¸è¿”å› max_tokensï¼Œé¿å…è§¦å‘ CLI é”™è¯¯
            final_stream_completed = False
            break

        # ==================== å…³é”®ä¿®å¤ï¼šæ„å»ºç»­ä¼ è¯·æ±‚ï¼ˆå¸¦éªŒè¯ï¼‰ ====================
        logger.info(f"[{request_id}] è§¦å‘ç»­ä¼  #{continuation_count + 1}: reason={truncation_info.reason}")

        # ä½¿ç”¨æ–°çš„éªŒè¯ç‰ˆæœ¬æ„å»ºç»­ä¼ è¯·æ±‚
        continuation_result = build_continuation_request(
            original_messages,
            accumulated_text,
            openai_body,
            continuation_count,
            request_id
        )

        # æ£€æŸ¥è¿”å›å€¼ç±»å‹ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰
        if isinstance(continuation_result, tuple):
            # æ–°ç‰ˆæœ¬ï¼šè¿”å› (body, should_continue, reason)
            new_body, should_build, build_reason = continuation_result
            if not should_build or new_body is None:
                logger.warning(f"[{request_id}] ç»­ä¼ è¯·æ±‚æ„å»ºå¤±è´¥: {build_reason}ï¼Œåœæ­¢ç»­ä¼ ")
                final_finish_reason = "end_turn"
                final_stream_completed = True
                break
            current_body = new_body
        else:
            # æ—§ç‰ˆæœ¬å…¼å®¹ï¼šç›´æ¥è¿”å› body
            current_body = continuation_result

        continuation_count += 1

    # ==================== å®Œæˆæ—¥å¿—å’Œé™çº§å¤„ç† ====================
    final_text_len = len(accumulated_text.strip()) if accumulated_text else 0
    final_tool_count = len(aggregated_tool_calls)

    # åˆ¤æ–­æ˜¯å¦éœ€è¦é™çº§å¤„ç†
    if final_text_len == 0 and final_tool_count == 0 and continuation_count > 0:
        # å¤šæ¬¡ç»­ä¼ åä»ç„¶æ²¡æœ‰æœ‰æ•ˆå†…å®¹ï¼Œè®°å½•è¯¦ç»†è­¦å‘Š
        logger.error(f"[{request_id}] âš ï¸ ç»­ä¼ å¤±è´¥: {continuation_count} æ¬¡ç»­ä¼ åæ— æœ‰æ•ˆå†…å®¹")
        # é™çº§ç­–ç•¥ï¼šè¿”å›å‹å¥½çš„é”™è¯¯æç¤ºè€Œä¸æ˜¯ç©ºå“åº”
        accumulated_text = "[ç³»ç»Ÿæç¤º] è¯·æ±‚å¤„ç†é‡åˆ°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•æˆ–ç®€åŒ–æ‚¨çš„è¯·æ±‚ã€‚"
        final_finish_reason = "end_turn"
        final_stream_completed = True
    elif continuation_count > 0:
        logger.info(f"[{request_id}] ğŸ”„ æ¥ç»­å®Œæˆ: {continuation_count} æ¬¡ç»­ä¼ , "
                    f"æœ€ç»ˆæ–‡æœ¬é•¿åº¦={final_text_len}, å·¥å…·è°ƒç”¨={final_tool_count}")
    else:
        logger.info(f"[{request_id}] âœ… è¯·æ±‚å®Œæˆ: æ— éœ€ç»­ä¼ , "
                    f"æ–‡æœ¬é•¿åº¦={final_text_len}, å·¥å…·è°ƒç”¨={final_tool_count}")

    return accumulated_text, final_finish_reason, final_stream_completed, {
        "input_tokens": total_input_tokens,
        "output_tokens": total_output_tokens,
        "continuation_count": continuation_count,
        "consecutive_failures": consecutive_failures,
        "final_text_length": final_text_len,
    }, aggregated_tool_calls


async def _fetch_single_stream(
    openai_body: dict,
    headers: dict,
    request_id: str,
    continuation_count: int
) -> tuple[str, str, bool, dict, list]:
    """æ‰§è¡Œå•æ¬¡æµå¼è¯·æ±‚

    Returns:
        (text, finish_reason, stream_completed, usage, tool_calls)
    """
    full_text = ""
    finish_reason = "end_turn"
    stream_completed = False
    input_tokens = 0
    output_tokens = 0
    tool_call_acc = {}

    try:
        client = get_http_client()
        async with client.stream(
            "POST",
            KIRO_PROXY_URL,
            json=openai_body,
            headers=headers,
        ) as response:
            if response.status_code != 200:
                error_text = await response.aread()
                error_str = error_text.decode()

                # ==================== å¢å¼ºé”™è¯¯åˆ†ç±»å’Œæ—¥å¿— ====================
                error_msg = error_str[:500]
                error_type = "unknown"
                is_retryable = False

                try:
                    error_json = json.loads(error_str)
                    error_msg = error_json.get("error", {}).get("message", error_str[:500])
                    # error_code å’Œ error_param å¯ç”¨äºæœªæ¥æ‰©å±•
                    # error_code = error_json.get("error", {}).get("code")
                    # error_param = error_json.get("error", {}).get("param")

                    # åˆ†ç±»é”™è¯¯ç±»å‹
                    if "Improperly formed request" in error_msg:
                        error_type = "malformed_request"
                        is_retryable = False
                        logger.error(f"[{request_id}] âŒ è¯·æ±‚æ ¼å¼é”™è¯¯ (ç»­ä¼  #{continuation_count}): {error_msg[:200]}")
                    elif "token" in error_msg.lower() or "æ²¡æœ‰å¯ç”¨" in error_msg:
                        error_type = "token_exhausted"
                        is_retryable = False
                        logger.error(f"[{request_id}] âŒ Token è€—å°½ (ç»­ä¼  #{continuation_count}): {error_msg[:200]}")
                    elif "rate limit" in error_msg.lower() or "too many" in error_msg.lower():
                        error_type = "rate_limit"
                        is_retryable = True
                        logger.warning(f"[{request_id}] âš ï¸ é€Ÿç‡é™åˆ¶ (ç»­ä¼  #{continuation_count}): {error_msg[:200]}")
                    elif "timeout" in error_msg.lower():
                        error_type = "timeout"
                        is_retryable = True
                        logger.warning(f"[{request_id}] âš ï¸ è¶…æ—¶ (ç»­ä¼  #{continuation_count}): {error_msg[:200]}")
                    elif response.status_code == 400:
                        error_type = "bad_request"
                        is_retryable = False
                        logger.error(f"[{request_id}] âŒ é”™è¯¯è¯·æ±‚ (ç»­ä¼  #{continuation_count}): {error_msg[:200]}")
                    elif response.status_code >= 500:
                        error_type = "server_error"
                        is_retryable = True
                        logger.warning(f"[{request_id}] âš ï¸ æœåŠ¡å™¨é”™è¯¯ (ç»­ä¼  #{continuation_count}): {error_msg[:200]}")
                    else:
                        logger.error(f"[{request_id}] âŒ æœªçŸ¥é”™è¯¯ (ç»­ä¼  #{continuation_count}): status={response.status_code}, msg={error_msg[:200]}")

                except json.JSONDecodeError:
                    logger.error(f"[{request_id}] âŒ æ— æ³•è§£æé”™è¯¯å“åº” (ç»­ä¼  #{continuation_count}): {error_str[:200]}")

                # è¿”å›é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å«é”™è¯¯ç±»å‹ä»¥ä¾¿ä¸Šå±‚å†³ç­–
                return f"[ä¸Šæ¸¸æœåŠ¡é”™è¯¯] {error_msg}", "error", True, {
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "error_type": error_type,
                    "is_retryable": is_retryable,
                    "status_code": response.status_code,
                }, []

            buffer = ""
            try:
                async for chunk in response.aiter_text():
                    buffer += chunk
                    while "\n" in buffer:
                        line, buffer = buffer.split("\n", 1)
                        line = line.strip()
                        if not line or not line.startswith("data:"):
                            continue

                        data_str = line[5:].strip()
                        if data_str == "[DONE]":
                            stream_completed = True
                            continue

                        try:
                            data = json.loads(data_str)

                            # è·å– usage
                            usage = data.get("usage")
                            if usage:
                                input_tokens = usage.get("prompt_tokens", input_tokens)
                                output_tokens = usage.get("completion_tokens", output_tokens)

                            choice = data.get("choices", [{}])[0]
                            delta = choice.get("delta", {})
                            fr = choice.get("finish_reason")

                            if fr:
                                stream_completed = True
                                if fr == "tool_calls":
                                    finish_reason = "tool_use"
                                elif fr == "length":
                                    finish_reason = "end_turn"  # ä¸è¿”å› max_tokensï¼Œç»­ä¼ æœºåˆ¶ä¼šå¤„ç†
                                elif fr == "stop":
                                    finish_reason = "end_turn"

                            content = delta.get("content", "")
                            if content:
                                full_text += content

                            delta_tool_calls = delta.get("tool_calls", []) or []
                            for tc in delta_tool_calls:
                                index = tc.get("index")
                                call_id = tc.get("id")
                                key = call_id or f"index_{index}" if index is not None else None
                                if not key:
                                    key = f"idx_{len(tool_call_acc)}"
                                entry = tool_call_acc.setdefault(
                                    key,
                                    {"id": call_id or f"toolu_{uuid.uuid4().hex[:12]}", "name": None, "arguments": ""}
                                )
                                if call_id:
                                    entry["id"] = call_id
                                func = tc.get("function", {}) or {}
                                if func.get("name"):
                                    entry["name"] = func.get("name")
                                if func.get("arguments"):
                                    entry["arguments"] += func.get("arguments")

                        except json.JSONDecodeError:
                            pass

            except (httpx.RemoteProtocolError, httpx.ReadError) as e:
                logger.error(f"[{request_id}] ç»­ä¼ è¯·æ±‚ #{continuation_count} æµä¸­æ–­: {type(e).__name__}")
                stream_completed = False

    except httpx.TimeoutException:
        logger.error(f"[{request_id}] ç»­ä¼ è¯·æ±‚ #{continuation_count} è¶…æ—¶")
        return full_text, "timeout", False, {"input_tokens": input_tokens, "output_tokens": output_tokens}, []
    except Exception as e:
        logger.error(f"[{request_id}] ç»­ä¼ è¯·æ±‚ #{continuation_count} å¼‚å¸¸: {type(e).__name__}: {e}")
        return full_text, "error", False, {"input_tokens": input_tokens, "output_tokens": output_tokens}, []

    # ä¼°ç®— tokenï¼ˆå¦‚æœ API æ²¡è¿”å›ï¼‰
    if output_tokens == 0:
        output_tokens = estimate_tokens(full_text)

    logger.info(f"[{request_id}] ç»­ä¼ è¯·æ±‚ #{continuation_count} å®Œæˆ: "
                f"text_len={len(full_text)}, finish={finish_reason}, completed={stream_completed}")

    return full_text, finish_reason, stream_completed, {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens
    }, list(tool_call_acc.values())


def convert_openai_to_anthropic(openai_response: dict, model: str, request_id: str) -> dict:
    """å°† OpenAI å“åº”è½¬æ¢ä¸º Anthropic æ ¼å¼

    æ”¯æŒä¸¤ç§å·¥å…·è°ƒç”¨æ ¼å¼ï¼š
    1. åŸç”Ÿ tool_callsï¼ˆä¼˜å…ˆï¼‰- Kiro ç½‘å…³åŸç”Ÿæ”¯æŒ
    2. å†…è”æ–‡æœ¬æ ¼å¼ï¼ˆé™çº§ï¼‰- [Calling tool: xxx] æ ¼å¼
    """
    choice = openai_response.get("choices", [{}])[0]
    message = choice.get("message", {})
    content = message.get("content", "")
    finish_reason = choice.get("finish_reason", "stop")

    # æ„å»º content blocks
    content_blocks = []
    stop_reason = "end_turn"

    # ä¼˜å…ˆå¤„ç†åŸç”Ÿ tool_callsï¼ˆKiro ç½‘å…³åŸç”Ÿæ”¯æŒï¼‰
    tool_calls = message.get("tool_calls", [])
    has_native_tool_calls = bool(tool_calls)

    if has_native_tool_calls:
        # åŸç”Ÿ tool_calls æ¨¡å¼
        logger.debug(f"[{request_id}] æ£€æµ‹åˆ°åŸç”Ÿ tool_calls: {len(tool_calls)} ä¸ª")

        # å…ˆæ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if content:
            # æ£€æŸ¥æ˜¯å¦æœ‰ thinking æ ‡ç­¾
            blocks = expand_thinking_blocks([{"type": "text", "text": content}])
            for block in blocks:
                if block.get("type") == "text":
                    text_value = block.get("text", "")
                    if text_value and text_value.strip():
                        content_blocks.append({"type": "text", "text": text_value})
                elif block.get("type") == "thinking":
                    content_blocks.append({"type": "thinking", "thinking": block.get("thinking", "")})

        # æ·»åŠ åŸç”Ÿ tool_calls
        content_blocks.extend(tool_calls_to_blocks(tool_calls))
        stop_reason = "tool_use"

    elif content:
        # é™çº§æ¨¡å¼ï¼šè§£æå†…è”çš„å·¥å…·è°ƒç”¨ï¼ˆ[Calling tool: xxx] æ ¼å¼ï¼‰
        blocks = parse_inline_tool_blocks(content)
        blocks = expand_thinking_blocks(blocks)
        for block in blocks:
            if block.get("type") == "text":
                text_value = block.get("text", "")
                if text_value:
                    content_blocks.append({"type": "text", "text": text_value})
            elif block.get("type") == "thinking":
                content_blocks.append({"type": "thinking", "thinking": block.get("thinking", "")})
            elif block.get("type") == "tool_use":
                content_blocks.append(block)
                stop_reason = "tool_use"

    # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œæ·»åŠ ç©ºæ–‡æœ¬
    if not content_blocks:
        content_blocks = [{"type": "text", "text": ""}]

    # æ ¹æ® finish_reason è°ƒæ•´ stop_reason
    if finish_reason == "tool_calls":
        stop_reason = "tool_use"
    elif finish_reason == "length":
        stop_reason = "end_turn"  # ä¸è¿”å› max_tokensï¼Œé¿å…è§¦å‘ Claude Code CLI é”™è¯¯
    elif finish_reason == "stop" and stop_reason != "tool_use":
        stop_reason = "end_turn"

    return {
        "id": f"msg_{request_id}",
        "type": "message",
        "role": "assistant",
        "content": content_blocks,
        "model": model,
        "stop_reason": stop_reason,
        "stop_sequence": None,
        "usage": {
            "input_tokens": openai_response.get("usage", {}).get("prompt_tokens", 0),
            "output_tokens": openai_response.get("usage", {}).get("completion_tokens", 0),
            "cache_creation_input_tokens": 0,
            "cache_read_input_tokens": 0,
        }
    }


@app.post("/v1/messages")
async def anthropic_messages(request: Request):
    """Anthropic /v1/messages ç«¯ç‚¹ - é€šè¿‡ OpenAI æ ¼å¼å‘é€åˆ° tokens ç½‘å…³"""
    request_id = uuid.uuid4().hex[:8]

    try:
        body = await request.json()
    except json.JSONDecodeError:
        raise HTTPException(400, "Invalid JSON")

    original_model = body.get("model", "claude-sonnet-4")
    stream = body.get("stream", False)
    orig_msg_count = len(body.get("messages", []))

    # ==================== max_tokens å¤„ç† ====================
    # ç¡®ä¿æœ‰åˆç†çš„ max_tokensï¼Œé˜²æ­¢å“åº”è¢«æ„å¤–æˆªæ–­
    DEFAULT_MAX_TOKENS = 16384  # 16K tokens ä½œä¸ºé»˜è®¤å€¼
    MAX_ALLOWED_TOKENS = 64000  # 64K tokens ä¸Šé™

    original_max_tokens = body.get("max_tokens")
    if original_max_tokens is None:
        body["max_tokens"] = DEFAULT_MAX_TOKENS
        logger.info(f"[{request_id}] è®¾ç½®é»˜è®¤ max_tokens: {DEFAULT_MAX_TOKENS}")
    elif original_max_tokens < 1000:
        # å¦‚æœè®¾ç½®å¾—å¤ªå°ï¼Œå¯èƒ½å¯¼è‡´æˆªæ–­
        logger.warning(f"[{request_id}] max_tokens è¾ƒå° ({original_max_tokens})ï¼Œå¯èƒ½å¯¼è‡´å“åº”æˆªæ–­")

    # è®°å½• max_tokens ä»¥ä¾¿è°ƒè¯•
    final_max_tokens = body.get("max_tokens")

    # ==================== æ™ºèƒ½æ¨¡å‹è·¯ç”± ====================
    # å¯¹ Opus è¯·æ±‚è¿›è¡Œæ™ºèƒ½é™çº§åˆ¤æ–­
    routed_model, route_reason = await model_router.route(body)

    if routed_model != original_model:
        logger.info(f"[{request_id}] ğŸ”€ æ¨¡å‹è·¯ç”±: {original_model} -> {routed_model} ({route_reason})")
        # æ›´æ–°è¯·æ±‚ä¸­çš„æ¨¡å‹
        body["model"] = routed_model
        model = routed_model
    else:
        model = original_model
        if "opus" in original_model.lower():
            logger.info(f"[{request_id}] âœ… ä¿ç•™ Opus: {route_reason}")

    # ==================== ä¸Šä¸‹æ–‡å¢å¼º ====================
    # åœ¨å†å²ç®¡ç†å‰å¢å¼ºç”¨æˆ·æ¶ˆæ¯
    messages = body.get("messages", [])
    session_id = generate_session_id(messages)

    # å¢å¼ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ³¨å…¥é¡¹ç›®ä¸Šä¸‹æ–‡ï¼‰
    messages = await enhance_user_message(messages, session_id)
    body["messages"] = messages

    # ==================== å†å²æ¶ˆæ¯ç®¡ç† ====================
    # åˆ›å»ºå†å²ç®¡ç†å™¨ï¼ˆä¸ /v1/chat/completions ä¿æŒä¸€è‡´ï¼‰
    manager = HistoryManager(HISTORY_CONFIG, cache_key=session_id)

    # é¢„å¤„ç†æ¶ˆæ¯ï¼ˆæˆªæ–­/æ‘˜è¦ï¼‰
    user_content = extract_user_content(messages)

    # è®¡ç®—åŸå§‹æ¶ˆæ¯å¤§å°
    original_chars = len(json.dumps(messages, ensure_ascii=False))
    logger.info(f"[{request_id}] åŸå§‹æ¶ˆæ¯: {len(messages)} æ¡, {original_chars} å­—ç¬¦")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æˆªæ–­/æ‘˜è¦
    should_summarize = manager.should_summarize(messages)
    logger.info(f"[{request_id}] éœ€è¦æ‘˜è¦: {should_summarize}, é˜ˆå€¼: {HISTORY_CONFIG.summary_threshold}")

    # ==================== å¼‚æ­¥æ‘˜è¦ä¼˜åŒ– ====================
    # æ ¸å¿ƒæ€æƒ³ï¼šé¦–æ¬¡è¯·æ±‚ç”¨ç®€å•æˆªæ–­å¿«é€Ÿå“åº”ï¼Œåå°å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ä¾›åç»­ä½¿ç”¨
    cache_info = {"hit": False, "original_tokens": 0, "cached_tokens": 0, "saved_tokens": 0}

    if should_summarize and ASYNC_SUMMARY_CONFIG.get("enabled", True):
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ‘˜è¦
        cached_summary, has_cache, original_tokens = async_summary_manager.get_cached_summary(session_id)

        if has_cache:
            # è·å–ç¼“å­˜ä¿¡æ¯ç”¨äºè®¡è´¹æ¨¡æ‹Ÿ
            cache_info = async_summary_manager.get_cache_info(session_id)

            # ä½¿ç”¨ç¼“å­˜çš„å·²å¤„ç†æ¶ˆæ¯
            cached_processed = async_summary_manager.get_cached_processed_messages(session_id)
            if cached_processed:
                logger.info(f"[{request_id}] âš¡ ä½¿ç”¨ç¼“å­˜æ‘˜è¦ï¼Œè·³è¿‡åŒæ­¥æ‘˜è¦ (èŠ‚çœ {cache_info['saved_tokens']} tokens)")
                processed_messages = cached_processed

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åå°æ›´æ–°æ‘˜è¦
                if async_summary_manager.should_update_summary(session_id, len(messages)):
                    await async_summary_manager.schedule_summary_task(
                        session_id, messages, manager, user_content
                    )
            else:
                # ç¼“å­˜ä¸å®Œæ•´ï¼Œä½¿ç”¨ç®€å•æˆªæ–­
                logger.info(f"[{request_id}] âš¡ ç¼“å­˜ä¸å®Œæ•´ï¼Œä½¿ç”¨ç®€å•æˆªæ–­")
                processed_messages = manager.pre_process(messages, user_content)
                cache_info = {"hit": False, "original_tokens": 0, "cached_tokens": 0, "saved_tokens": 0}
        elif ASYNC_SUMMARY_CONFIG.get("fast_first_request", True):
            # é¦–æ¬¡è¯·æ±‚ï¼šä½¿ç”¨ç®€å•æˆªæ–­å¿«é€Ÿå“åº”
            logger.info(f"[{request_id}] âš¡ é¦–æ¬¡è¯·æ±‚ï¼Œä½¿ç”¨ç®€å•æˆªæ–­ï¼ˆåå°ç”Ÿæˆæ‘˜è¦ï¼‰")
            processed_messages = manager.pre_process(messages, user_content)

            # å¯åŠ¨åå°æ‘˜è¦ä»»åŠ¡
            await async_summary_manager.schedule_summary_task(
                session_id, messages, manager, user_content
            )
        else:
            # ç¦ç”¨å¿«é€Ÿé¦–æ¬¡è¯·æ±‚ï¼ŒåŒæ­¥ç”Ÿæˆæ‘˜è¦ï¼ˆæ—§è¡Œä¸ºï¼‰
            logger.info(f"[{request_id}] è§¦å‘åŒæ­¥æ™ºèƒ½æ‘˜è¦...")
            processed_messages = await manager.pre_process_async(
                messages, user_content, call_kiro_for_summary
            )
    elif should_summarize:
        # å¼‚æ­¥æ‘˜è¦ç¦ç”¨ï¼Œä½¿ç”¨åŒæ­¥æ‘˜è¦ï¼ˆæ—§è¡Œä¸ºï¼‰
        logger.info(f"[{request_id}] è§¦å‘åŒæ­¥æ™ºèƒ½æ‘˜è¦...")
        processed_messages = await manager.pre_process_async(
            messages, user_content, call_kiro_for_summary
        )

        # ä¸æ™ºèƒ½æ‘˜è¦é›†æˆï¼šæ‘˜è¦æ—¶åŒæ­¥æ›´æ–°ä¸Šä¸‹æ–‡
        if CONTEXT_ENHANCEMENT_CONFIG["integrate_with_summary"]:
            logger.info(f"[{request_id}] ğŸ”„ æ‘˜è¦è§¦å‘ï¼ŒåŒæ­¥æ›´æ–°é¡¹ç›®ä¸Šä¸‹æ–‡...")
            context = await extract_project_context(messages, session_id)
            if context:
                user_message_count = count_user_messages(messages)
                update_session_context(session_id, context, user_message_count)
                logger.info(f"[{request_id}] âœ… é¡¹ç›®ä¸Šä¸‹æ–‡å·²æ›´æ–°")
    else:
        processed_messages = manager.pre_process(messages, user_content)

    if manager.was_truncated:
        logger.info(f"[{request_id}] âœ‚ï¸ {manager.truncate_info}")
    else:
        logger.info(f"[{request_id}] æ— éœ€æˆªæ–­")

    # æ›´æ–° body ä¸­çš„ messages
    body["messages"] = processed_messages

    # ä½¿ç”¨å®Œæ•´è½¬æ¢ï¼ˆåŒ…å«æˆªæ–­å’Œç©ºæ¶ˆæ¯è¿‡æ»¤ï¼‰
    openai_body = convert_anthropic_to_openai(body)

    final_msg_count = len(openai_body.get("messages", []))
    total_chars = sum(len(str(m.get("content", ""))) for m in openai_body.get("messages", []))

    # è®°å½•å·¥å…·æ¨¡å¼
    tools_count = len(openai_body.get("tools", []))
    tools_mode = "åŸç”Ÿ" if tools_count > 0 and NATIVE_TOOLS_ENABLED else ("æ–‡æœ¬æ³¨å…¥" if body.get("tools") else "æ— ")

    logger.info(f"[{request_id}] Anthropic -> OpenAI: model={model}, stream={stream}, "
                f"msgs={orig_msg_count}->{final_msg_count}, chars={total_chars}, max_tokens={final_max_tokens}, "
                f"tools={tools_count}({tools_mode})")

    # ä¿å­˜è°ƒè¯•æ–‡ä»¶ï¼ˆä»…ä¿ç•™æœ€è¿‘å‡ ä¸ªï¼‰
    debug_dir = "/tmp/ai-history-debug"
    os.makedirs(debug_dir, exist_ok=True)
    try:
        with open(f"{debug_dir}/{request_id}_converted.json", "w") as f:
            json.dump(openai_body, f, indent=2, ensure_ascii=False)
        # æ¸…ç†æ—§æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘ 10 ä¸ªï¼‰- å¤„ç†å¹¶å‘åˆ é™¤çš„ç«æ€æ¡ä»¶
        try:
            debug_files = sorted(
                [f for f in os.listdir(debug_dir) if f.endswith('.json')],
                key=lambda x: os.path.getmtime(os.path.join(debug_dir, x)),
                reverse=True
            )
            for old_file in debug_files[10:]:
                try:
                    os.remove(os.path.join(debug_dir, old_file))
                except FileNotFoundError:
                    pass  # å·²è¢«å…¶ä»–è¯·æ±‚åˆ é™¤
                except OSError:
                    pass  # å…¶ä»–æ–‡ä»¶ç³»ç»Ÿé”™è¯¯
        except OSError:
            pass  # ç›®å½•åˆ—è¡¨å¤±è´¥
    except Exception:
        pass  # éå…³é”®æ“ä½œï¼Œå¿½ç•¥æ‰€æœ‰é”™è¯¯

    # æ„å»ºè¯·æ±‚å¤´ - æ·»åŠ å”¯ä¸€æ ‡è¯†è®© tokens åŒºåˆ†ä¸åŒè¯·æ±‚
    # å…³é”®ï¼šæ¯ä¸ªè¯·æ±‚ä½¿ç”¨ä¸åŒçš„ X-Request-ID å’Œ X-Trace-ID
    # è¿™æ · tokens ä¸ä¼šæŠŠå¤šä¸ªè¯·æ±‚å½“ä½œåŒä¸€ç»ˆç«¯å¤„ç†
    headers = {
        "Authorization": f"Bearer {KIRO_API_KEY}",
        "Content-Type": "application/json",
        "X-Request-ID": f"req_{request_id}_{uuid.uuid4().hex[:8]}",
        "X-Trace-ID": f"trace_{uuid.uuid4().hex}",
        "X-Client-ID": f"client_{uuid.uuid4().hex[:12]}",  # æ¨¡æ‹Ÿä¸åŒå®¢æˆ·ç«¯
    }

    if stream:
        return await handle_anthropic_stream_via_openai(openai_body, headers, request_id, model, cache_info)
    else:
        return await handle_anthropic_non_stream_via_openai(openai_body, headers, request_id, model, cache_info)


async def handle_anthropic_stream_via_openai(
    openai_body: dict,
    headers: dict,
    request_id: str,
    model: str,
    cache_info: dict = None,
) -> StreamingResponse:
    """å¤„ç† Anthropic æµå¼è¯·æ±‚ - é€šè¿‡ OpenAI æ ¼å¼

    å…³é”®å¢å¼ºï¼š
    1. æ£€æµ‹å†…è”å·¥å…·è°ƒç”¨å¹¶è½¬æ¢ä¸ºæ ‡å‡† tool_use content blocks
    2. æ™ºèƒ½æ¥ç»­æœºåˆ¶ - å½“æ£€æµ‹åˆ°æˆªæ–­æ—¶è‡ªåŠ¨å‘èµ·ç»­ä¼ è¯·æ±‚
    3. é«˜å¹¶å‘ä¼˜åŒ– - ä½¿ç”¨å…¨å±€ HTTP å®¢æˆ·ç«¯è¿æ¥æ± 
    4. Token è®¡æ•° - æ”¯æŒä» OpenAI API è·å–æˆ–ä¼°ç®— token æ•°é‡
    5. ç¼“å­˜è®¡è´¹æ¨¡æ‹Ÿ - å½“ç¼“å­˜å‘½ä¸­æ—¶ï¼Œæ¨¡æ‹Ÿ prompt caching æŠ˜æ‰£

    ç­–ç•¥ï¼šç´¯ç§¯å®Œæ•´å“åº”åè§£æï¼Œæ£€æµ‹æˆªæ–­å¹¶è‡ªåŠ¨ç»­ä¼ ï¼Œç„¶åæ­£ç¡®å‘é€ content blocks
    """
    cache_info = cache_info or {"hit": False, "original_tokens": 0, "cached_tokens": 0, "saved_tokens": 0}

    # é¢„ä¼°è¾“å…¥ token æ•°
    estimated_input_tokens = 0
    for msg in openai_body.get("messages", []):
        content = msg.get("content", "")
        if isinstance(content, str):
            estimated_input_tokens += estimate_tokens(content)
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict):
                    estimated_input_tokens += estimate_tokens(item.get("text", ""))
                elif isinstance(item, str):
                    estimated_input_tokens += estimate_tokens(item)
        estimated_input_tokens += 4  # æ¯æ¡æ¶ˆæ¯é¢å¤–å¼€é”€

    # ==================== ç¼“å­˜è®¡è´¹æ¨¡æ‹Ÿ ====================
    # å½“ç¼“å­˜å‘½ä¸­æ—¶ï¼Œå°†èŠ‚çœçš„ tokens æŠ¥å‘Šä¸º cache_read_input_tokens
    # è¿™æ · NewAPI ä¼šæ˜¾ç¤ºç±»ä¼¼ "æ¨¡å‹: 2.5 * ç¼“å­˜: 0.1"
    cache_read_tokens = 0
    if cache_info.get("hit") and ASYNC_SUMMARY_CONFIG.get("simulate_cache_billing", True):
        # è®¡ç®—ç¼“å­˜è¯»å–çš„ tokensï¼ˆèŠ‚çœçš„éƒ¨åˆ†ï¼‰
        saved_tokens = cache_info.get("saved_tokens", 0)
        if saved_tokens > 0:
            # å°†èŠ‚çœçš„ tokens æŠ¥å‘Šä¸º cache_read
            # Anthropic å®˜æ–¹ cache_read æŠ˜æ‰£æ˜¯ 0.1x
            cache_read_tokens = saved_tokens
            # å®é™…è®¡è´¹çš„ input_tokens å‡å°‘
            estimated_input_tokens = max(0, estimated_input_tokens - saved_tokens) + cache_read_tokens
            logger.info(f"[{request_id}] ğŸ’° ç¼“å­˜è®¡è´¹æ¨¡æ‹Ÿ: cache_read={cache_read_tokens}, å®é™…input={estimated_input_tokens - cache_read_tokens}")

    async def generate() -> AsyncIterator[bytes]:
        try:
            # å‘é€ Anthropic æµå¼å¤´
            # è®¡ç®—å®é™…çš„ input_tokensï¼ˆæ‰£é™¤ç¼“å­˜è¯»å–éƒ¨åˆ†ï¼‰
            actual_input_tokens = max(0, estimated_input_tokens - cache_read_tokens)
            msg_start = {
                "type": "message_start",
                "message": {
                    "id": f"msg_{request_id}",
                    "type": "message",
                    "role": "assistant",
                    "content": [],
                    "model": model,
                    "stop_reason": None,
                    "stop_sequence": None,
                    "usage": {
                        "input_tokens": actual_input_tokens,
                        "output_tokens": 0,
                        "cache_creation_input_tokens": 0,
                        "cache_read_input_tokens": cache_read_tokens,
                    }
                }
            }
            yield f"data: {json.dumps(msg_start)}\n\n".encode()

            # ========== æ™ºèƒ½æ¥ç»­æœºåˆ¶ ==========
            # ä½¿ç”¨ fetch_with_continuation è·å–å®Œæ•´å“åº”ï¼ˆè‡ªåŠ¨å¤„ç†æˆªæ–­å’Œç»­ä¼ ï¼‰
            if CONTINUATION_CONFIG.get("enabled", True):
                full_text, finish_reason, stream_completed, usage_info, tool_calls = await fetch_with_continuation(
                    openai_body, headers, request_id, model
                )
                input_tokens = usage_info.get("input_tokens", estimated_input_tokens)
                output_tokens = usage_info.get("output_tokens", 0)
                continuation_count = usage_info.get("continuation_count", 0)

                if continuation_count > 0:
                    logger.info(f"[{request_id}] ğŸ”„ æ¥ç»­å®Œæˆ: {continuation_count} æ¬¡ç»­ä¼ , "
                                f"æœ€ç»ˆæ–‡æœ¬é•¿åº¦={len(full_text)}")
            else:
                # æ¥ç»­æœºåˆ¶ç¦ç”¨ï¼Œä½¿ç”¨å•æ¬¡è¯·æ±‚
                full_text, finish_reason, stream_completed, usage_info, tool_calls = await _fetch_single_stream(
                    openai_body, headers, request_id, 0
                )
                input_tokens = usage_info.get("input_tokens", estimated_input_tokens)
                output_tokens = usage_info.get("output_tokens", 0)

            # æ£€æµ‹æœ€ç»ˆå“åº”æ˜¯å¦ä»æœ‰æˆªæ–­ï¼ˆæ¥ç»­åä»å¯èƒ½æœ‰é—®é¢˜ï¼‰
            truncation_info = detect_truncation(full_text, stream_completed, finish_reason, request_id)

            # è§£æå†…è”å·¥å…·è°ƒç”¨ï¼ˆä¿åºï¼‰
            blocks = parse_inline_tool_blocks(full_text)
            tool_call_blocks = tool_calls_to_blocks(tool_calls or [])
            if tool_call_blocks:
                blocks.extend(tool_call_blocks)
            blocks = expand_thinking_blocks(blocks)

            # å¤„ç†æˆªæ–­æƒ…å†µ
            if truncation_info.is_truncated:
                # è¿‡æ»¤æ‰è§£æå¤±è´¥çš„å·¥å…·è°ƒç”¨
                valid_tools = []
                tool_call_ids = {b.get("id") for b in tool_call_blocks if b.get("id")}
                for tu in (b for b in blocks if b.get("type") == "tool_use"):
                    inp = tu.get("input", {})
                    if tu.get("id") in tool_call_ids:
                        valid_tools.append(tu)
                    elif isinstance(inp, dict) and ("_parse_error" not in inp and "_raw" not in inp):
                        valid_tools.append(tu)
                    else:
                        logger.warning(f"[{request_id}] ä¸¢å¼ƒæ— æ•ˆå·¥å…·è°ƒç”¨: {tu.get('name')} - "
                                       f"{inp.get('_parse_error', 'unknown error')[:100]}")

                if valid_tools:
                    blocks = [b for b in blocks if b.get("type") != "tool_use"] + valid_tools
                    logger.info(f"[{request_id}] æ¢å¤ {len(valid_tools)} ä¸ªæœ‰æ•ˆå·¥å…·è°ƒç”¨")
                else:
                    # æ‰€æœ‰å·¥å…·è°ƒç”¨éƒ½å¤±è´¥ï¼Œä¸”ç¡®å®å‘ç”Ÿäº†æˆªæ–­ï¼Œæ‰æ·»åŠ è­¦å‘Š
                    blocks = [{"type": "text", "text": full_text}]
                    logger.warning(f"[{request_id}] æ‰€æœ‰å·¥å…·è°ƒç”¨è§£æå¤±è´¥ï¼Œå›é€€ä¸ºçº¯æ–‡æœ¬å“åº”")
                    # ä¸æ·»åŠ  [âš ï¸ Response truncated: ...] æ ‡è®°
                    # åŸå› ï¼šClaude Code CLI ä¼šè§£æè¿™ä¸ªæ ¼å¼å¹¶æ˜¾ç¤ºä¸º API é”™è¯¯
                    # å³ä½¿å“åº”è¢«æˆªæ–­ï¼Œä¹Ÿè®©ç»­ä¼ æœºåˆ¶å¤„ç†ï¼Œä¸è¦è§¦å‘ CLI é”™è¯¯æç¤º
                    pass

            # å‘é€ content blocksï¼ˆä¿åºï¼‰
            block_index = 0
            emitted_block = False

            for block in blocks:
                if block.get("type") == "text":
                    text_value = block.get("text", "")
                    if not text_value:
                        continue
                    emitted_block = True
                    yield (
                        f'data: {{"type":"content_block_start","index":{block_index},"content_block":'
                        f'{{"type":"text","text":""}}}}\n\n'
                    ).encode()
                    for chunk in iter_text_chunks(text_value, STREAM_TEXT_CHUNK_SIZE):
                        delta_event = {
                            "type": "content_block_delta",
                            "index": block_index,
                            "delta": {"type": "text_delta", "text": chunk}
                        }
                        yield f"data: {json.dumps(delta_event)}\n\n".encode()
                    yield f'data: {{"type":"content_block_stop","index":{block_index}}}\n\n'.encode()
                    block_index += 1
                elif block.get("type") == "thinking":
                    thinking_value = block.get("thinking", "")
                    if not thinking_value:
                        continue
                    emitted_block = True
                    yield (
                        f'data: {{"type":"content_block_start","index":{block_index},"content_block":'
                        f'{{"type":"thinking","thinking":""}}}}\n\n'
                    ).encode()
                    for chunk in iter_text_chunks(thinking_value, STREAM_THINKING_CHUNK_SIZE):
                        delta_event = {
                            "type": "content_block_delta",
                            "index": block_index,
                            "delta": {"type": "thinking_delta", "thinking": chunk}
                        }
                        yield f"data: {json.dumps(delta_event)}\n\n".encode()
                    yield f'data: {{"type":"content_block_stop","index":{block_index}}}\n\n'.encode()
                    block_index += 1
                elif block.get("type") == "tool_use":
                    emitted_block = True
                    finish_reason = "tool_use"
                    tool_start = {
                        "type": "content_block_start",
                        "index": block_index,
                        "content_block": {
                            "type": "tool_use",
                            "id": block["id"],
                            "name": block["name"],
                            "input": {}
                        }
                    }
                    yield f"data: {json.dumps(tool_start)}\n\n".encode()

                    tool_json = json.dumps(block.get("input", {}))
                    for chunk in iter_text_chunks(tool_json, STREAM_TOOL_JSON_CHUNK_SIZE):
                        delta_event = {
                            "type": "content_block_delta",
                            "index": block_index,
                            "delta": {"type": "input_json_delta", "partial_json": chunk}
                        }
                        yield f"data: {json.dumps(delta_event)}\n\n".encode()

                    yield f'data: {{"type":"content_block_stop","index":{block_index}}}\n\n'.encode()
                    block_index += 1

            if not emitted_block:
                yield f'data: {{"type":"content_block_start","index":0,"content_block":{{"type":"text","text":""}}}}\n\n'.encode()
                yield f'data: {{"type":"content_block_stop","index":0}}\n\n'.encode()

            # å¦‚æœ OpenAI æ²¡æœ‰è¿”å› usageï¼Œä½¿ç”¨ä¼°ç®—å€¼
            if output_tokens == 0:
                output_tokens = estimate_tokens(full_text)

            # å¦‚æœæ£€æµ‹åˆ°æˆªæ–­ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            if truncation_info.is_truncated:
                tool_count = len([b for b in blocks if b.get("type") == "tool_use"])
                logger.warning(f"[{request_id}] âš ï¸ å“åº”æˆªæ–­å®Œæˆ: reason={truncation_info.reason}, "
                               f"text_len={len(full_text)}, tools={tool_count}, "
                               f"finish_reason={finish_reason}")

            # message delta with token usage (åŒ…å«ç¼“å­˜ä¿¡æ¯)
            yield f'data: {{"type":"message_delta","delta":{{"stop_reason":"{finish_reason}","stop_sequence":null}},"usage":{{"output_tokens":{output_tokens},"cache_creation_input_tokens":0,"cache_read_input_tokens":{cache_read_tokens}}}}}\n\n'.encode()

            # message stop
            yield f'data: {{"type":"message_stop"}}\n\n'.encode()

        except httpx.TimeoutException:
            logger.error(f"[{request_id}] è¯·æ±‚è¶…æ—¶")
            error_response = {
                "type": "error",
                "error": {"type": "timeout_error", "message": "Request timeout"}
            }
            yield f"data: {json.dumps(error_response)}\n\n".encode()
        except (httpx.RemoteProtocolError, httpx.ReadError) as e:
            # EOF / è¿æ¥ä¸­æ–­ - è¿™æ˜¯å¸¸è§çš„ä¸Šæ¸¸é”™è¯¯
            logger.error(f"[{request_id}] è¿æ¥ä¸­æ–­ (EOF): {type(e).__name__}: {e}")
            error_response = {
                "type": "error",
                "error": {"type": "stream_error", "message": f"Connection interrupted: {type(e).__name__}. Please retry."}
            }
            yield f"data: {json.dumps(error_response)}\n\n".encode()
        except Exception as e:
            logger.error(f"[{request_id}] è¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {e}")
            error_response = {
                "type": "error",
                "error": {"type": "api_error", "message": str(e)}
            }
            yield f"data: {json.dumps(error_response)}\n\n".encode()

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


async def handle_anthropic_non_stream_via_openai(
    openai_body: dict,
    headers: dict,
    request_id: str,
    model: str,
    cache_info: dict = None,
) -> JSONResponse:
    """å¤„ç† Anthropic éæµå¼è¯·æ±‚ - é€šè¿‡ OpenAI æ ¼å¼

    é«˜å¹¶å‘ä¼˜åŒ–ï¼šä½¿ç”¨å…¨å±€ HTTP å®¢æˆ·ç«¯è¿æ¥æ± 
    æ”¯æŒç¼“å­˜è®¡è´¹æ¨¡æ‹Ÿ
    """
    cache_info = cache_info or {"hit": False, "original_tokens": 0, "cached_tokens": 0, "saved_tokens": 0}

    try:
        client = get_http_client()
        response = await client.post(
            KIRO_PROXY_URL,
            json=openai_body,
            headers=headers,
        )

        if response.status_code != 200:
            error_str = response.text
            logger.error(f"[{request_id}] OpenAI API Error {response.status_code}: {error_str[:200]}")

            return JSONResponse(
                status_code=response.status_code,
                content={
                    "type": "error",
                    "error": {
                        "type": "api_error",
                        "message": error_str[:500],
                    }
                }
            )

        # è½¬æ¢ OpenAI å“åº”ä¸º Anthropic æ ¼å¼
        openai_response = response.json()
        anthropic_response = convert_openai_to_anthropic(openai_response, model, request_id)

        # æ·»åŠ ç¼“å­˜è®¡è´¹ä¿¡æ¯
        if cache_info.get("hit") and ASYNC_SUMMARY_CONFIG.get("simulate_cache_billing", True):
            saved_tokens = cache_info.get("saved_tokens", 0)
            if saved_tokens > 0 and "usage" in anthropic_response:
                original_input = anthropic_response["usage"].get("input_tokens", 0)
                # å°†èŠ‚çœçš„ tokens ä½œä¸º cache_read
                anthropic_response["usage"]["cache_read_input_tokens"] = saved_tokens
                # è°ƒæ•´å®é™… input_tokens
                anthropic_response["usage"]["input_tokens"] = max(0, original_input - saved_tokens)
                logger.info(f"[{request_id}] ğŸ’° ç¼“å­˜è®¡è´¹æ¨¡æ‹Ÿ: cache_read={saved_tokens}")

        return JSONResponse(content=anthropic_response)

    except httpx.TimeoutException:
        logger.error(f"[{request_id}] è¯·æ±‚è¶…æ—¶")
        return JSONResponse(
            status_code=408,
            content={
                "type": "error",
                "error": {"type": "timeout_error", "message": "Request timeout"}
            }
        )
    except Exception as e:
        logger.error(f"[{request_id}] è¯·æ±‚å¼‚å¸¸: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "type": "error",
                "error": {"type": "api_error", "message": str(e)}
            }
        )


# ==================== OpenAI API ====================

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """èŠå¤©å®Œæˆæ¥å£ - OpenAI å…¼å®¹"""
    start_time = time.time()
    request_id = uuid.uuid4().hex[:8]

    try:
        body = await request.json()
    except json.JSONDecodeError:
        raise HTTPException(400, "Invalid JSON")

    model = body.get("model", "claude-sonnet-4")
    messages = body.get("messages", [])
    stream = body.get("stream", False)

    if not messages:
        raise HTTPException(400, "messages is required")

    logger.info(f"[{request_id}] Request: model={model}, messages={len(messages)}, stream={stream}")

    # ==================== ä¸Šä¸‹æ–‡å¢å¼º ====================
    # åœ¨å†å²ç®¡ç†å‰å¢å¼ºç”¨æˆ·æ¶ˆæ¯
    session_id = generate_session_id(messages)
    messages = await enhance_user_message(messages, session_id)
    body["messages"] = messages

    # åˆ›å»ºå†å²ç®¡ç†å™¨
    manager = HistoryManager(HISTORY_CONFIG, cache_key=session_id)

    # é¢„å¤„ç†æ¶ˆæ¯
    user_content = extract_user_content(messages)

    if manager.should_summarize(messages):
        processed_messages = await manager.pre_process_async(
            messages, user_content, call_kiro_for_summary
        )
    else:
        processed_messages = manager.pre_process(messages, user_content)

    if manager.was_truncated:
        logger.info(f"[{request_id}] {manager.truncate_info}")

    # æ„å»ºè¯·æ±‚
    kiro_request = {
        "model": model,
        "messages": processed_messages,
        "stream": stream,
    }

    # ä¼ é€’å…¶ä»–å‚æ•°
    for key in ["temperature", "max_tokens", "top_p", "frequency_penalty", "presence_penalty", "stop"]:
        if key in body and body[key] is not None:
            kiro_request[key] = body[key]

    # ==================== åŸç”Ÿ Tools æ”¯æŒ ====================
    # é€ä¼  tools å’Œ tool_choice å‚æ•°ç»™ Kiro ç½‘å…³
    if NATIVE_TOOLS_ENABLED:
        if "tools" in body and body["tools"]:
            kiro_request["tools"] = body["tools"]
            logger.debug(f"[{request_id}] é€ä¼  tools å‚æ•°ï¼Œå·¥å…·æ•°é‡: {len(body['tools'])}")
        if "tool_choice" in body and body["tool_choice"]:
            kiro_request["tool_choice"] = body["tool_choice"]

    # æ·»åŠ å”¯ä¸€è¯·æ±‚æ ‡è¯†
    headers = {
        "Authorization": f"Bearer {KIRO_API_KEY}",
        "Content-Type": "application/json",
        "X-Request-ID": f"chat_{request_id}_{uuid.uuid4().hex[:8]}",
        "X-Trace-ID": f"trace_{uuid.uuid4().hex}",
        "X-Client-ID": f"client_{uuid.uuid4().hex[:12]}",  # æ¨¡æ‹Ÿä¸åŒå®¢æˆ·ç«¯
    }

    if stream:
        return await handle_stream(kiro_request, headers, manager, request_id, messages)
    else:
        return await handle_non_stream(kiro_request, headers, manager, request_id, messages)


async def handle_stream(
    kiro_request: dict,
    headers: dict,
    manager: HistoryManager,
    request_id: str,
    original_messages: list,
) -> StreamingResponse:
    """å¤„ç†æµå¼å“åº” - ä½¿ç”¨å…¨å±€ HTTP å®¢æˆ·ç«¯ï¼Œæ— å¹¶å‘é™åˆ¶"""

    async def generate() -> AsyncIterator[bytes]:
        nonlocal kiro_request
        retry_count = 0
        max_retries = HISTORY_CONFIG.max_retries

        while retry_count <= max_retries:
            try:
                client = get_http_client()
                async with client.stream(
                    "POST",
                    KIRO_PROXY_URL,
                    json=kiro_request,
                    headers=headers,
                ) as response:

                    if response.status_code != 200:
                        error_text = await response.aread()
                        error_str = error_text.decode()

                        logger.error(f"[{request_id}] Kiro API Error {response.status_code}: {error_str[:200]}")

                        # æ£€æŸ¥æ˜¯å¦ä¸ºé•¿åº¦é”™è¯¯
                        if is_content_length_error(response.status_code, error_str):
                            logger.info(f"[{request_id}] æ£€æµ‹åˆ°é•¿åº¦é”™è¯¯ï¼Œå°è¯•æˆªæ–­é‡è¯•")

                            truncated, should_retry = await manager.handle_length_error_async(
                                kiro_request["messages"],
                                retry_count,
                                call_kiro_for_summary,
                            )

                            if should_retry:
                                kiro_request["messages"] = truncated
                                retry_count += 1
                                logger.info(f"[{request_id}] {manager.truncate_info}")
                                continue

                        # è¿”å›é”™è¯¯
                        error_response = {
                            "error": {
                                "message": error_str[:500],
                                "type": "api_error",
                                "code": response.status_code,
                            }
                        }
                        yield f"data: {json.dumps(error_response)}\n\n".encode()
                        yield b"data: [DONE]\n\n"
                        return

                    # æ­£å¸¸æµå¼å“åº”
                    async for chunk in response.aiter_bytes():
                        yield chunk

                    return

            except httpx.TimeoutException:
                logger.error(f"[{request_id}] è¯·æ±‚è¶…æ—¶")
                if retry_count < max_retries:
                    retry_count += 1
                    await asyncio.sleep(1)
                    continue

                error_response = {"error": {"message": "Request timeout", "type": "timeout_error"}}
                yield f"data: {json.dumps(error_response)}\n\n".encode()
                yield b"data: [DONE]\n\n"
                return

            except Exception as e:
                logger.error(f"[{request_id}] è¯·æ±‚å¼‚å¸¸: {e}")
                error_response = {"error": {"message": str(e), "type": "api_error"}}
                yield f"data: {json.dumps(error_response)}\n\n".encode()
                yield b"data: [DONE]\n\n"
                return

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


async def handle_non_stream(
    kiro_request: dict,
    headers: dict,
    manager: HistoryManager,
    request_id: str,
    original_messages: list,
) -> JSONResponse:
    """å¤„ç†éæµå¼å“åº” - ä½¿ç”¨å…¨å±€ HTTP å®¢æˆ·ç«¯ï¼Œæ— å¹¶å‘é™åˆ¶"""
    retry_count = 0
    max_retries = HISTORY_CONFIG.max_retries

    while retry_count <= max_retries:
        try:
            client = get_http_client()
            response = await client.post(
                KIRO_PROXY_URL,
                json=kiro_request,
                headers=headers,
            )

            if response.status_code != 200:
                error_str = response.text
                logger.error(f"[{request_id}] Kiro API Error {response.status_code}: {error_str[:200]}")

                # æ£€æŸ¥æ˜¯å¦ä¸ºé•¿åº¦é”™è¯¯
                if is_content_length_error(response.status_code, error_str):
                    logger.info(f"[{request_id}] æ£€æµ‹åˆ°é•¿åº¦é”™è¯¯ï¼Œå°è¯•æˆªæ–­é‡è¯•")

                    truncated, should_retry = await manager.handle_length_error_async(
                        kiro_request["messages"],
                        retry_count,
                        call_kiro_for_summary,
                    )

                    if should_retry:
                        kiro_request["messages"] = truncated
                        retry_count += 1
                        logger.info(f"[{request_id}] {manager.truncate_info}")
                        continue

                raise HTTPException(response.status_code, error_str[:500])

            return JSONResponse(content=response.json())

        except HTTPException:
            raise
        except httpx.TimeoutException:
            logger.error(f"[{request_id}] è¯·æ±‚è¶…æ—¶")
            if retry_count < max_retries:
                retry_count += 1
                await asyncio.sleep(1)
                continue
            raise HTTPException(408, "Request timeout")
        except Exception as e:
            logger.error(f"[{request_id}] è¯·æ±‚å¼‚å¸¸: {e}")
            raise HTTPException(500, str(e))

    raise HTTPException(503, "All retries exhausted")


# ==================== é…ç½®æ¥å£ ====================

@app.get("/admin/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    return {
        "kiro_proxy_url": KIRO_PROXY_URL,
        "history_config": HISTORY_CONFIG.to_dict(),
        "async_summary_config": ASYNC_SUMMARY_CONFIG,
        "native_tools_enabled": NATIVE_TOOLS_ENABLED,
    }


@app.get("/admin/async-summary/stats")
async def get_async_summary_stats():
    """è·å–å¼‚æ­¥æ‘˜è¦ç»Ÿè®¡"""
    return {
        "config": ASYNC_SUMMARY_CONFIG,
        "stats": async_summary_manager.get_stats(),
    }


@app.get("/admin/routing/stats")
async def get_routing_stats():
    """è·å–æ¨¡å‹è·¯ç”±ç»Ÿè®¡"""
    return model_router.get_stats()


@app.post("/admin/routing/reset")
async def reset_routing_stats():
    """é‡ç½®è·¯ç”±ç»Ÿè®¡"""
    model_router.stats = {"opus": 0, "sonnet": 0, "haiku": 0, "opus_degraded": 0}
    return {"status": "ok", "message": "è·¯ç”±ç»Ÿè®¡å·²é‡ç½®"}


@app.post("/admin/config/history")
async def update_history_config(request: Request):
    """æ›´æ–°å†å²ç®¡ç†é…ç½®"""
    global HISTORY_CONFIG

    try:
        data = await request.json()
        HISTORY_CONFIG = HistoryConfig.from_dict(data)
        return {"status": "ok", "config": HISTORY_CONFIG.to_dict()}
    except Exception as e:
        raise HTTPException(400, str(e))


# ==================== å¯åŠ¨å…¥å£ ====================

if __name__ == "__main__":
    import uvicorn

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           AI History Manager API Server                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æœåŠ¡åœ°å€: http://0.0.0.0:{SERVICE_PORT}                          â•‘
â•‘  API ç«¯ç‚¹: /v1/chat/completions                              â•‘
â•‘  å¥åº·æ£€æŸ¥: /                                                 â•‘
â•‘  æ¨¡å‹åˆ—è¡¨: /v1/models                                        â•‘
â•‘  é…ç½®æŸ¥çœ‹: /admin/config                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  NewAPI é…ç½®:                                                â•‘
â•‘  - ç±»å‹: è‡ªå®šä¹‰æ¸ é“ (OpenAI)                                 â•‘
â•‘  - Base URL: http://your-server:{SERVICE_PORT}/v1                 â•‘
â•‘  - API Key: ä»»æ„å€¼                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
